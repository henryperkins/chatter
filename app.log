2024-12-29 19:49:02,493 - WARNING -  * Debugger is active!
2024-12-29 19:49:02,502 - INFO -  * Debugger PIN: 995-560-430
2024-12-29 19:49:29,480 - INFO - 127.0.0.1 - - [29/Dec/2024 19:49:29] "[32mGET /chat_interface HTTP/1.1[0m" 302 -
2024-12-29 19:49:29,490 - INFO - 127.0.0.1 - - [29/Dec/2024 19:49:29] "GET /login?next=/chat_interface HTTP/1.1" 200 -
2024-12-29 19:49:36,046 - INFO - 127.0.0.1 - - [29/Dec/2024 19:49:36] "[32mPOST /login HTTP/1.1[0m" 302 -
2024-12-29 19:49:36,055 - INFO - 127.0.0.1 - - [29/Dec/2024 19:49:36] "GET /chat_interface HTTP/1.1" 200 -
2024-12-29 19:49:36,170 - INFO - 127.0.0.1 - - [29/Dec/2024 19:49:36] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-29 19:49:38,962 - DEBUG - Received chat message
2024-12-29 19:49:38,963 - DEBUG - Chat ID: ccfa3ee6-8bde-409a-8c6d-48be6bdf1191
2024-12-29 19:49:38,963 - INFO - User message: Test
2024-12-29 19:49:38,963 - DEBUG - Sending request with 2 messages
2024-12-29 19:49:38,963 - DEBUG - Using deployment: o1-preview
2024-12-29 19:49:38,964 - DEBUG - Payload: [{'role': 'user', 'content': 'Please format your responses in Markdown.'}, {'role': 'user', 'content': 'Test'}]
2024-12-29 19:49:38,968 - DEBUG - Request options: {'method': 'post', 'url': '/deployments/o1-preview/chat/completions', 'headers': {'api-key': '<redacted>'}, 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': 'Please format your responses in Markdown.'}, {'role': 'user', 'content': 'Test'}], 'model': 'o1-preview', 'max_completion_tokens': 500, 'response_format': {'type': 'text'}, 'temperature': 1.0}}
2024-12-29 19:49:39,054 - DEBUG - Sending HTTP Request: POST https://openai-hp.openai.azure.com/openai/deployments/o1-preview/chat/completions?api-version=2024-12-01-preview
2024-12-29 19:49:39,054 - DEBUG - connect_tcp.started host='openai-hp.openai.azure.com' port=443 local_address=None timeout=5.0 socket_options=None
2024-12-29 19:49:39,121 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000023C522C5940>
2024-12-29 19:49:39,121 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x0000023C522024E0> server_hostname='openai-hp.openai.azure.com' timeout=5.0
2024-12-29 19:49:39,194 - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000023C521D3C50>
2024-12-29 19:49:39,195 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2024-12-29 19:49:39,195 - DEBUG - send_request_headers.complete
2024-12-29 19:49:39,196 - DEBUG - send_request_body.started request=<Request [b'POST']>
2024-12-29 19:49:39,196 - DEBUG - send_request_body.complete
2024-12-29 19:49:39,196 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2024-12-29 19:49:47,006 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Length', b'528'), (b'Content-Type', b'application/json'), (b'apim-request-id', b'3ca2a5c9-730a-4086-ac50-0f06906a4415'), (b'Strict-Transport-Security', b'max-age=31536000; includeSubDomains; preload'), (b'x-content-type-options', b'nosniff'), (b'x-ratelimit-remaining-tokens', b'2399500'), (b'x-accel-buffering', b'no'), (b'x-ms-rai-invoked', b'true'), (b'x-request-id', b'cd0742b4-74aa-4dbd-9c5c-1b13fbb2396f'), (b'x-ms-region', b'East US 2'), (b'azureml-model-session', b'd091-20241220021933'), (b'x-ratelimit-remaining-requests', b'399'), (b'x-envoy-upstream-service-time', b'7649'), (b'x-ms-client-request-id', b'3ca2a5c9-730a-4086-ac50-0f06906a4415'), (b'Date', b'Mon, 30 Dec 2024 01:49:45 GMT')])
2024-12-29 19:49:47,007 - INFO - HTTP Request: POST https://openai-hp.openai.azure.com/openai/deployments/o1-preview/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2024-12-29 19:49:47,007 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2024-12-29 19:49:47,007 - DEBUG - receive_response_body.complete
2024-12-29 19:49:47,008 - DEBUG - response_closed.started
2024-12-29 19:49:47,008 - DEBUG - response_closed.complete
2024-12-29 19:49:47,008 - DEBUG - HTTP Response: POST https://openai-hp.openai.azure.com/openai/deployments/o1-preview/chat/completions?api-version=2024-12-01-preview "200 OK" Headers({'content-length': '528', 'content-type': 'application/json', 'apim-request-id': '3ca2a5c9-730a-4086-ac50-0f06906a4415', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'x-content-type-options': 'nosniff', 'x-ratelimit-remaining-tokens': '2399500', 'x-accel-buffering': 'no', 'x-ms-rai-invoked': 'true', 'x-request-id': 'cd0742b4-74aa-4dbd-9c5c-1b13fbb2396f', 'x-ms-region': 'East US 2', 'azureml-model-session': 'd091-20241220021933', 'x-ratelimit-remaining-requests': '399', 'x-envoy-upstream-service-time': '7649', 'x-ms-client-request-id': '3ca2a5c9-730a-4086-ac50-0f06906a4415', 'date': 'Mon, 30 Dec 2024 01:49:45 GMT'})
2024-12-29 19:49:47,009 - DEBUG - request_id: cd0742b4-74aa-4dbd-9c5c-1b13fbb2396f
2024-12-29 19:49:47,012 - ERROR - Error during API call: Empty response content
2024-12-29 19:49:47,013 - DEBUG - API endpoint: https://openai-hp.openai.azure.com/
2024-12-29 19:49:47,013 - DEBUG - Model deployment name: o1-preview
2024-12-29 19:49:47,014 - DEBUG - Traceback: Traceback (most recent call last):
  File "C:\Users\htper\OneDrive\Folder\chats\chatapp\app.py", line 421, in chat
    raise ValueError("Empty response content")
ValueError: Empty response content

2024-12-29 19:49:47,015 - INFO - 127.0.0.1 - - [29/Dec/2024 19:49:47] "POST /chat HTTP/1.1" 200 -
2024-12-29 19:50:32,436 - INFO -  * Detected change in 'C:\\Users\\htper\\OneDrive\\Folder\\chats\\chatapp\\app.py', reloading
2024-12-29 19:50:35,171 - WARNING -  * Debugger is active!
2024-12-29 19:50:35,183 - INFO -  * Debugger PIN: 995-560-430
2024-12-29 19:50:43,548 - INFO - 127.0.0.1 - - [29/Dec/2024 19:50:43] "[32mGET /chat_interface HTTP/1.1[0m" 302 -
2024-12-29 19:50:43,567 - INFO - 127.0.0.1 - - [29/Dec/2024 19:50:43] "GET /login?next=/chat_interface HTTP/1.1" 200 -
2024-12-29 19:50:52,614 - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-12-29 19:50:52,615 - INFO - [33mPress CTRL+C to quit[0m
2024-12-29 19:50:52,616 - INFO -  * Restarting with stat
2024-12-29 19:50:55,407 - WARNING -  * Debugger is active!
2024-12-29 19:50:55,415 - INFO -  * Debugger PIN: 995-560-430
2024-12-29 19:50:59,859 - INFO - 127.0.0.1 - - [29/Dec/2024 19:50:59] "[32mPOST /login HTTP/1.1[0m" 302 -
2024-12-29 19:50:59,875 - INFO - 127.0.0.1 - - [29/Dec/2024 19:50:59] "GET /chat_interface HTTP/1.1" 200 -
2024-12-29 19:51:00,083 - INFO - 127.0.0.1 - - [29/Dec/2024 19:51:00] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-29 19:51:03,407 - DEBUG - Received chat message
2024-12-29 19:51:03,407 - DEBUG - Chat ID: bbd31c1a-3072-4633-b908-37ee1db4830f
2024-12-29 19:51:03,407 - INFO - User message: Test
2024-12-29 19:51:03,408 - DEBUG - Sending request with 2 messages
2024-12-29 19:51:03,408 - DEBUG - Using deployment: o1-preview
2024-12-29 19:51:03,408 - DEBUG - Payload: [{'role': 'user', 'content': 'Please format your responses in Markdown.'}, {'role': 'user', 'content': 'Test'}]
2024-12-29 19:51:03,413 - DEBUG - Request options: {'method': 'post', 'url': '/deployments/o1-preview/chat/completions', 'headers': {'api-key': '<redacted>'}, 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': 'Please format your responses in Markdown.'}, {'role': 'user', 'content': 'Test'}], 'model': 'o1-preview', 'max_completion_tokens': 500, 'response_format': {'type': 'text'}, 'temperature': 1.0}}
2024-12-29 19:51:03,503 - DEBUG - Sending HTTP Request: POST https://openai-hp.openai.azure.com/openai/deployments/o1-preview/chat/completions?api-version=2024-12-01-preview
2024-12-29 19:51:03,503 - DEBUG - connect_tcp.started host='openai-hp.openai.azure.com' port=443 local_address=None timeout=5.0 socket_options=None
2024-12-29 19:51:03,576 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000282FB185160>
2024-12-29 19:51:03,577 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x00000282FB0C24E0> server_hostname='openai-hp.openai.azure.com' timeout=5.0
2024-12-29 19:51:03,664 - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000282FB093B10>
2024-12-29 19:51:03,664 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2024-12-29 19:51:03,664 - DEBUG - send_request_headers.complete
2024-12-29 19:51:03,665 - DEBUG - send_request_body.started request=<Request [b'POST']>
2024-12-29 19:51:03,665 - DEBUG - send_request_body.complete
2024-12-29 19:51:03,665 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2024-12-29 19:51:15,092 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Length', b'528'), (b'Content-Type', b'application/json'), (b'apim-request-id', b'0ef050e3-0953-46ec-8394-e95e5c8cadb1'), (b'Strict-Transport-Security', b'max-age=31536000; includeSubDomains; preload'), (b'x-content-type-options', b'nosniff'), (b'x-ratelimit-remaining-tokens', b'2399500'), (b'x-accel-buffering', b'no'), (b'x-ms-rai-invoked', b'true'), (b'x-request-id', b'6b56418c-971b-495a-9822-df8ecc01923a'), (b'x-ms-region', b'East US 2'), (b'azureml-model-session', b'd085-20241219205556'), (b'x-ratelimit-remaining-requests', b'399'), (b'x-envoy-upstream-service-time', b'11354'), (b'x-ms-client-request-id', b'0ef050e3-0953-46ec-8394-e95e5c8cadb1'), (b'Date', b'Mon, 30 Dec 2024 01:51:12 GMT')])
2024-12-29 19:51:15,093 - INFO - HTTP Request: POST https://openai-hp.openai.azure.com/openai/deployments/o1-preview/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2024-12-29 19:51:15,093 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2024-12-29 19:51:15,093 - DEBUG - receive_response_body.complete
2024-12-29 19:51:15,094 - DEBUG - response_closed.started
2024-12-29 19:51:15,094 - DEBUG - response_closed.complete
2024-12-29 19:51:15,094 - DEBUG - HTTP Response: POST https://openai-hp.openai.azure.com/openai/deployments/o1-preview/chat/completions?api-version=2024-12-01-preview "200 OK" Headers({'content-length': '528', 'content-type': 'application/json', 'apim-request-id': '0ef050e3-0953-46ec-8394-e95e5c8cadb1', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'x-content-type-options': 'nosniff', 'x-ratelimit-remaining-tokens': '2399500', 'x-accel-buffering': 'no', 'x-ms-rai-invoked': 'true', 'x-request-id': '6b56418c-971b-495a-9822-df8ecc01923a', 'x-ms-region': 'East US 2', 'azureml-model-session': 'd085-20241219205556', 'x-ratelimit-remaining-requests': '399', 'x-envoy-upstream-service-time': '11354', 'x-ms-client-request-id': '0ef050e3-0953-46ec-8394-e95e5c8cadb1', 'date': 'Mon, 30 Dec 2024 01:51:12 GMT'})
2024-12-29 19:51:15,094 - DEBUG - request_id: 6b56418c-971b-495a-9822-df8ecc01923a
2024-12-29 19:51:15,099 - ERROR - API response contains empty content: ChatCompletion(id='chatcmpl-AjzDqeWyLi981wUKfEvfZcZZMCeti', choices=[Choice(finish_reason='length', index=0, logprobs=None, message=ChatCompletionMessage(content='', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None), content_filter_results={})], created=1735523462, model='o1-preview-2024-09-12', object='chat.completion', service_tier=None, system_fingerprint='fp_772cb6e46d', usage=CompletionUsage(completion_tokens=500, prompt_tokens=27, total_tokens=527, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=500, rejected_prediction_tokens=None), prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=0)), prompt_filter_results=[{'prompt_index': 0, 'content_filter_results': {}}])
2024-12-29 19:51:15,099 - ERROR - Error during API call: Empty response content
2024-12-29 19:51:15,099 - DEBUG - API endpoint: https://openai-hp.openai.azure.com/
2024-12-29 19:51:15,100 - DEBUG - Model deployment name: o1-preview
2024-12-29 19:51:15,101 - DEBUG - Traceback: Traceback (most recent call last):
  File "C:\Users\htper\OneDrive\Folder\chats\chatapp\app.py", line 423, in chat
    raise ValueError("Empty response content")
ValueError: Empty response content

2024-12-29 19:51:15,102 - INFO - 127.0.0.1 - - [29/Dec/2024 19:51:15] "POST /chat HTTP/1.1" 200 -
2024-12-29 19:51:19,385 - DEBUG - Received chat message
2024-12-29 19:51:19,385 - DEBUG - Chat ID: bbd31c1a-3072-4633-b908-37ee1db4830f
2024-12-29 19:51:19,385 - INFO - User message: Hello
2024-12-29 19:51:19,385 - DEBUG - Sending request with 4 messages
2024-12-29 19:51:19,386 - DEBUG - Using deployment: o1-preview
2024-12-29 19:51:19,386 - DEBUG - Payload: [{'role': 'user', 'content': 'Please format your responses in Markdown.'}, {'role': 'user', 'content': 'Test'}, {'role': 'user', 'content': 'Sorry, I encountered an error while processing your request. Please try again.'}, {'role': 'user', 'content': 'Hello'}]
2024-12-29 19:51:19,392 - DEBUG - Request options: {'method': 'post', 'url': '/deployments/o1-preview/chat/completions', 'headers': {'api-key': '<redacted>'}, 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': 'Please format your responses in Markdown.'}, {'role': 'user', 'content': 'Test'}, {'role': 'user', 'content': 'Sorry, I encountered an error while processing your request. Please try again.'}, {'role': 'user', 'content': 'Hello'}], 'model': 'o1-preview', 'max_completion_tokens': 500, 'response_format': {'type': 'text'}, 'temperature': 1.0}}
2024-12-29 19:51:19,393 - DEBUG - Sending HTTP Request: POST https://openai-hp.openai.azure.com/openai/deployments/o1-preview/chat/completions?api-version=2024-12-01-preview
2024-12-29 19:51:19,393 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2024-12-29 19:51:19,394 - DEBUG - send_request_headers.complete
2024-12-29 19:51:19,394 - DEBUG - send_request_body.started request=<Request [b'POST']>
2024-12-29 19:51:19,395 - DEBUG - send_request_body.complete
2024-12-29 19:51:19,395 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2024-12-29 19:51:22,382 - INFO - 127.0.0.1 - - [29/Dec/2024 19:51:22] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-29 19:51:29,484 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Length', b'528'), (b'Content-Type', b'application/json'), (b'apim-request-id', b'e13ba864-c2e6-48b4-b106-89f32a1176c9'), (b'Strict-Transport-Security', b'max-age=31536000; includeSubDomains; preload'), (b'x-content-type-options', b'nosniff'), (b'x-ratelimit-remaining-tokens', b'2399000'), (b'x-accel-buffering', b'no'), (b'x-ms-rai-invoked', b'true'), (b'x-request-id', b'9db16a3d-8daf-4024-8df1-0e3e3f0b18da'), (b'x-ms-region', b'East US 2'), (b'azureml-model-session', b'd085-20241219205556'), (b'x-ratelimit-remaining-requests', b'398'), (b'x-envoy-upstream-service-time', b'10015'), (b'x-ms-client-request-id', b'e13ba864-c2e6-48b4-b106-89f32a1176c9'), (b'Date', b'Mon, 30 Dec 2024 01:51:27 GMT')])
2024-12-29 19:51:29,485 - INFO - HTTP Request: POST https://openai-hp.openai.azure.com/openai/deployments/o1-preview/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2024-12-29 19:51:29,485 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2024-12-29 19:51:29,485 - DEBUG - receive_response_body.complete
2024-12-29 19:51:29,486 - DEBUG - response_closed.started
2024-12-29 19:51:29,486 - DEBUG - response_closed.complete
2024-12-29 19:51:29,486 - DEBUG - HTTP Response: POST https://openai-hp.openai.azure.com/openai/deployments/o1-preview/chat/completions?api-version=2024-12-01-preview "200 OK" Headers({'content-length': '528', 'content-type': 'application/json', 'apim-request-id': 'e13ba864-c2e6-48b4-b106-89f32a1176c9', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'x-content-type-options': 'nosniff', 'x-ratelimit-remaining-tokens': '2399000', 'x-accel-buffering': 'no', 'x-ms-rai-invoked': 'true', 'x-request-id': '9db16a3d-8daf-4024-8df1-0e3e3f0b18da', 'x-ms-region': 'East US 2', 'azureml-model-session': 'd085-20241219205556', 'x-ratelimit-remaining-requests': '398', 'x-envoy-upstream-service-time': '10015', 'x-ms-client-request-id': 'e13ba864-c2e6-48b4-b106-89f32a1176c9', 'date': 'Mon, 30 Dec 2024 01:51:27 GMT'})
2024-12-29 19:51:29,486 - DEBUG - request_id: 9db16a3d-8daf-4024-8df1-0e3e3f0b18da
2024-12-29 19:51:29,487 - ERROR - API response contains empty content: ChatCompletion(id='chatcmpl-AjzE63mVxTEPwyHHPZug60Z7PJyY3', choices=[Choice(finish_reason='length', index=0, logprobs=None, message=ChatCompletionMessage(content='', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None), content_filter_results={})], created=1735523478, model='o1-preview-2024-09-12', object='chat.completion', service_tier=None, system_fingerprint='fp_772cb6e46d', usage=CompletionUsage(completion_tokens=500, prompt_tokens=66, total_tokens=566, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=500, rejected_prediction_tokens=None), prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=0)), prompt_filter_results=[{'prompt_index': 0, 'content_filter_results': {}}])
2024-12-29 19:51:29,487 - ERROR - Error during API call: Empty response content
2024-12-29 19:51:29,487 - DEBUG - API endpoint: https://openai-hp.openai.azure.com/
2024-12-29 19:51:29,488 - DEBUG - Model deployment name: o1-preview
2024-12-29 19:51:29,488 - DEBUG - Traceback: Traceback (most recent call last):
  File "C:\Users\htper\OneDrive\Folder\chats\chatapp\app.py", line 423, in chat
    raise ValueError("Empty response content")
ValueError: Empty response content

2024-12-29 19:51:29,489 - INFO - 127.0.0.1 - - [29/Dec/2024 19:51:29] "POST /chat HTTP/1.1" 200 -
2024-12-29 19:55:59,893 - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-12-29 19:55:59,893 - INFO - [33mPress CTRL+C to quit[0m
2024-12-29 19:55:59,896 - INFO -  * Restarting with stat
2024-12-29 19:56:01,990 - WARNING -  * Debugger is active!
2024-12-29 19:56:01,997 - INFO -  * Debugger PIN: 995-560-430
2024-12-29 19:56:02,851 - INFO - 127.0.0.1 - - [29/Dec/2024 19:56:02] "[32mGET /chat_interface HTTP/1.1[0m" 302 -
2024-12-29 19:56:02,864 - INFO - 127.0.0.1 - - [29/Dec/2024 19:56:02] "GET /login?next=/chat_interface HTTP/1.1" 200 -
2024-12-29 19:56:07,494 - INFO - 127.0.0.1 - - [29/Dec/2024 19:56:07] "POST /login HTTP/1.1" 200 -
2024-12-29 19:56:11,538 - INFO - 127.0.0.1 - - [29/Dec/2024 19:56:11] "[32mPOST /login HTTP/1.1[0m" 302 -
2024-12-29 19:56:11,547 - INFO - 127.0.0.1 - - [29/Dec/2024 19:56:11] "GET /chat_interface HTTP/1.1" 200 -
2024-12-29 19:56:11,709 - INFO - 127.0.0.1 - - [29/Dec/2024 19:56:11] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-29 19:56:14,385 - DEBUG - Received chat message
2024-12-29 19:56:14,385 - DEBUG - Chat ID: 8310d5d9-03fd-4f09-beb9-e3133211365a
2024-12-29 19:56:14,386 - INFO - User message: Test
2024-12-29 19:56:14,386 - DEBUG - Sending request with 2 messages
2024-12-29 19:56:14,386 - DEBUG - Using deployment: o1-preview
2024-12-29 19:56:14,387 - DEBUG - Payload: [{'role': 'user', 'content': 'Please format your responses in Markdown.'}, {'role': 'user', 'content': 'Test'}]
2024-12-29 19:56:14,391 - DEBUG - Request options: {'method': 'post', 'url': '/deployments/o1-preview/chat/completions', 'headers': {'api-key': '<redacted>'}, 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': 'Please format your responses in Markdown.'}, {'role': 'user', 'content': 'Test'}], 'model': 'o1-preview', 'max_completion_tokens': 500, 'response_format': {'type': 'text'}, 'temperature': 1.0}}
2024-12-29 19:56:14,483 - DEBUG - Sending HTTP Request: POST https://openai-hp.openai.azure.com/openai/deployments/o1-preview/chat/completions?api-version=2024-12-01-preview
2024-12-29 19:56:14,483 - DEBUG - connect_tcp.started host='openai-hp.openai.azure.com' port=443 local_address=None timeout=5.0 socket_options=None
2024-12-29 19:56:14,566 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000019382465BE0>
2024-12-29 19:56:14,566 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x00000193823A2570> server_hostname='openai-hp.openai.azure.com' timeout=5.0
2024-12-29 19:56:14,642 - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000019382373B10>
2024-12-29 19:56:14,642 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2024-12-29 19:56:14,643 - DEBUG - send_request_headers.complete
2024-12-29 19:56:14,643 - DEBUG - send_request_body.started request=<Request [b'POST']>
2024-12-29 19:56:14,643 - DEBUG - send_request_body.complete
2024-12-29 19:56:14,643 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2024-12-29 19:56:24,493 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Length', b'528'), (b'Content-Type', b'application/json'), (b'apim-request-id', b'd091445f-c459-449e-98e3-0b84e05552cd'), (b'Strict-Transport-Security', b'max-age=31536000; includeSubDomains; preload'), (b'x-content-type-options', b'nosniff'), (b'x-ratelimit-remaining-tokens', b'2399500'), (b'x-accel-buffering', b'no'), (b'x-ms-rai-invoked', b'true'), (b'x-request-id', b'9522927c-917f-4aa3-9817-6be229883206'), (b'x-ms-region', b'East US 2'), (b'azureml-model-session', b'd091-20241220021933'), (b'x-ratelimit-remaining-requests', b'399'), (b'x-envoy-upstream-service-time', b'9691'), (b'x-ms-client-request-id', b'd091445f-c459-449e-98e3-0b84e05552cd'), (b'Date', b'Mon, 30 Dec 2024 01:56:22 GMT')])
2024-12-29 19:56:24,494 - INFO - HTTP Request: POST https://openai-hp.openai.azure.com/openai/deployments/o1-preview/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2024-12-29 19:56:24,494 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2024-12-29 19:56:24,495 - DEBUG - receive_response_body.complete
2024-12-29 19:56:24,495 - DEBUG - response_closed.started
2024-12-29 19:56:24,495 - DEBUG - response_closed.complete
2024-12-29 19:56:24,496 - DEBUG - HTTP Response: POST https://openai-hp.openai.azure.com/openai/deployments/o1-preview/chat/completions?api-version=2024-12-01-preview "200 OK" Headers({'content-length': '528', 'content-type': 'application/json', 'apim-request-id': 'd091445f-c459-449e-98e3-0b84e05552cd', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'x-content-type-options': 'nosniff', 'x-ratelimit-remaining-tokens': '2399500', 'x-accel-buffering': 'no', 'x-ms-rai-invoked': 'true', 'x-request-id': '9522927c-917f-4aa3-9817-6be229883206', 'x-ms-region': 'East US 2', 'azureml-model-session': 'd091-20241220021933', 'x-ratelimit-remaining-requests': '399', 'x-envoy-upstream-service-time': '9691', 'x-ms-client-request-id': 'd091445f-c459-449e-98e3-0b84e05552cd', 'date': 'Mon, 30 Dec 2024 01:56:22 GMT'})
2024-12-29 19:56:24,496 - DEBUG - request_id: 9522927c-917f-4aa3-9817-6be229883206
2024-12-29 19:56:24,500 - ERROR - API response contains empty content: ChatCompletion(id='chatcmpl-AjzIra7ZzGTrQKY42PhvxvkxJv94d', choices=[Choice(finish_reason='length', index=0, logprobs=None, message=ChatCompletionMessage(content='', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None), content_filter_results={})], created=1735523773, model='o1-preview-2024-09-12', object='chat.completion', service_tier=None, system_fingerprint='fp_772cb6e46d', usage=CompletionUsage(completion_tokens=500, prompt_tokens=27, total_tokens=527, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=500, rejected_prediction_tokens=None), prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=0)), prompt_filter_results=[{'prompt_index': 0, 'content_filter_results': {}}])
2024-12-29 19:56:24,500 - INFO - Fallback response provided to the user.
2024-12-29 19:56:24,502 - INFO - Received response: <p>The assistant was unable to generate a response. Please try again or rephrase your input.</p>
<div class="shortcuts"><button onclick="copyToClipboard()">Copy</button><button onclick="retryMessage()">Retry</button></div>
2024-12-29 19:56:24,503 - DEBUG - Raw API response: ChatCompletion(id='chatcmpl-AjzIra7ZzGTrQKY42PhvxvkxJv94d', choices=[Choice(finish_reason='length', index=0, logprobs=None, message=ChatCompletionMessage(content='', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None), content_filter_results={})], created=1735523773, model='o1-preview-2024-09-12', object='chat.completion', service_tier=None, system_fingerprint='fp_772cb6e46d', usage=CompletionUsage(completion_tokens=500, prompt_tokens=27, total_tokens=527, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=500, rejected_prediction_tokens=None), prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=0)), prompt_filter_results=[{'prompt_index': 0, 'content_filter_results': {}}])
2024-12-29 19:56:24,503 - DEBUG - Usage: CompletionUsage(completion_tokens=500, prompt_tokens=27, total_tokens=527, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=500, rejected_prediction_tokens=None), prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=0))
2024-12-29 19:56:24,504 - INFO - 127.0.0.1 - - [29/Dec/2024 19:56:24] "POST /chat HTTP/1.1" 200 -
2024-12-29 19:56:28,583 - DEBUG - Received chat message
2024-12-29 19:56:28,583 - DEBUG - Chat ID: 8310d5d9-03fd-4f09-beb9-e3133211365a
2024-12-29 19:56:28,583 - INFO - User message: Test
2024-12-29 19:56:28,584 - DEBUG - Sending request with 4 messages
2024-12-29 19:56:28,584 - DEBUG - Using deployment: o1-preview
2024-12-29 19:56:28,584 - DEBUG - Payload: [{'role': 'user', 'content': 'Please format your responses in Markdown.'}, {'role': 'user', 'content': 'Test'}, {'role': 'user', 'content': Markup('<p>The assistant was unable to generate a response. Please try again or rephrase your input.</p>\n<div class="shortcuts"><button onclick="copyToClipboard()">Copy</button><button onclick="retryMessage()">Retry</button></div>')}, {'role': 'user', 'content': 'Test'}]
2024-12-29 19:56:28,590 - DEBUG - Request options: {'method': 'post', 'url': '/deployments/o1-preview/chat/completions', 'headers': {'api-key': '<redacted>'}, 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': 'Please format your responses in Markdown.'}, {'role': 'user', 'content': 'Test'}, {'role': 'user', 'content': Markup('<p>The assistant was unable to generate a response. Please try again or rephrase your input.</p>\n<div class="shortcuts"><button onclick="copyToClipboard()">Copy</button><button onclick="retryMessage()">Retry</button></div>')}, {'role': 'user', 'content': 'Test'}], 'model': 'o1-preview', 'max_completion_tokens': 500, 'response_format': {'type': 'text'}, 'temperature': 1.0}}
2024-12-29 19:56:28,591 - DEBUG - Sending HTTP Request: POST https://openai-hp.openai.azure.com/openai/deployments/o1-preview/chat/completions?api-version=2024-12-01-preview
2024-12-29 19:56:28,592 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2024-12-29 19:56:28,593 - DEBUG - send_request_headers.complete
2024-12-29 19:56:28,593 - DEBUG - send_request_body.started request=<Request [b'POST']>
2024-12-29 19:56:28,594 - DEBUG - send_request_body.complete
2024-12-29 19:56:28,594 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2024-12-29 19:56:34,292 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Length', b'529'), (b'Content-Type', b'application/json'), (b'apim-request-id', b'4f56976c-c6b1-416c-9937-d098532ca42a'), (b'Strict-Transport-Security', b'max-age=31536000; includeSubDomains; preload'), (b'x-content-type-options', b'nosniff'), (b'x-ratelimit-remaining-tokens', b'2399000'), (b'x-accel-buffering', b'no'), (b'x-ms-rai-invoked', b'true'), (b'x-request-id', b'4d4620e0-68ca-4e9f-b113-5231f6788592'), (b'x-ms-region', b'East US 2'), (b'azureml-model-session', b'd091-20241220021933'), (b'x-ratelimit-remaining-requests', b'398'), (b'x-envoy-upstream-service-time', b'5627'), (b'x-ms-client-request-id', b'4f56976c-c6b1-416c-9937-d098532ca42a'), (b'Date', b'Mon, 30 Dec 2024 01:56:32 GMT')])
2024-12-29 19:56:34,293 - INFO - HTTP Request: POST https://openai-hp.openai.azure.com/openai/deployments/o1-preview/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2024-12-29 19:56:34,293 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2024-12-29 19:56:34,293 - DEBUG - receive_response_body.complete
2024-12-29 19:56:34,294 - DEBUG - response_closed.started
2024-12-29 19:56:34,294 - DEBUG - response_closed.complete
2024-12-29 19:56:34,294 - DEBUG - HTTP Response: POST https://openai-hp.openai.azure.com/openai/deployments/o1-preview/chat/completions?api-version=2024-12-01-preview "200 OK" Headers({'content-length': '529', 'content-type': 'application/json', 'apim-request-id': '4f56976c-c6b1-416c-9937-d098532ca42a', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'x-content-type-options': 'nosniff', 'x-ratelimit-remaining-tokens': '2399000', 'x-accel-buffering': 'no', 'x-ms-rai-invoked': 'true', 'x-request-id': '4d4620e0-68ca-4e9f-b113-5231f6788592', 'x-ms-region': 'East US 2', 'azureml-model-session': 'd091-20241220021933', 'x-ratelimit-remaining-requests': '398', 'x-envoy-upstream-service-time': '5627', 'x-ms-client-request-id': '4f56976c-c6b1-416c-9937-d098532ca42a', 'date': 'Mon, 30 Dec 2024 01:56:32 GMT'})
2024-12-29 19:56:34,295 - DEBUG - request_id: 4d4620e0-68ca-4e9f-b113-5231f6788592
2024-12-29 19:56:34,296 - ERROR - API response contains empty content: ChatCompletion(id='chatcmpl-AjzJ5yxXhpf7hU3AOmuEUxfgQc38L', choices=[Choice(finish_reason='length', index=0, logprobs=None, message=ChatCompletionMessage(content='', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None), content_filter_results={})], created=1735523787, model='o1-preview-2024-09-12', object='chat.completion', service_tier=None, system_fingerprint='fp_772cb6e46d', usage=CompletionUsage(completion_tokens=500, prompt_tokens=104, total_tokens=604, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=500, rejected_prediction_tokens=None), prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=0)), prompt_filter_results=[{'prompt_index': 0, 'content_filter_results': {}}])
2024-12-29 19:56:34,296 - INFO - Fallback response provided to the user.
2024-12-29 19:56:34,297 - INFO - Received response: <p>The assistant was unable to generate a response. Please try again or rephrase your input.</p>
<div class="shortcuts"><button onclick="copyToClipboard()">Copy</button><button onclick="retryMessage()">Retry</button></div>
2024-12-29 19:56:34,297 - DEBUG - Raw API response: ChatCompletion(id='chatcmpl-AjzJ5yxXhpf7hU3AOmuEUxfgQc38L', choices=[Choice(finish_reason='length', index=0, logprobs=None, message=ChatCompletionMessage(content='', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None), content_filter_results={})], created=1735523787, model='o1-preview-2024-09-12', object='chat.completion', service_tier=None, system_fingerprint='fp_772cb6e46d', usage=CompletionUsage(completion_tokens=500, prompt_tokens=104, total_tokens=604, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=500, rejected_prediction_tokens=None), prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=0)), prompt_filter_results=[{'prompt_index': 0, 'content_filter_results': {}}])
2024-12-29 19:56:34,297 - DEBUG - Usage: CompletionUsage(completion_tokens=500, prompt_tokens=104, total_tokens=604, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=500, rejected_prediction_tokens=None), prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=0))
2024-12-29 19:56:34,298 - INFO - 127.0.0.1 - - [29/Dec/2024 19:56:34] "POST /chat HTTP/1.1" 200 -
2024-12-29 19:57:54,102 - INFO -  * Detected change in 'C:\\Users\\htper\\OneDrive\\Folder\\chats\\chatapp\\app.py', reloading
2024-12-29 19:57:54,238 - INFO -  * Restarting with stat
2024-12-29 19:57:56,756 - WARNING -  * Debugger is active!
2024-12-29 19:57:56,763 - INFO -  * Debugger PIN: 995-560-430
2024-12-29 19:58:12,397 - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-12-29 19:58:12,397 - INFO - [33mPress CTRL+C to quit[0m
2024-12-29 19:58:12,399 - INFO -  * Restarting with stat
2024-12-29 19:58:14,778 - WARNING -  * Debugger is active!
2024-12-29 19:58:14,787 - INFO -  * Debugger PIN: 995-560-430
2024-12-29 19:58:14,856 - INFO - 127.0.0.1 - - [29/Dec/2024 19:58:14] "[32mGET /chat_interface HTTP/1.1[0m" 302 -
2024-12-29 19:58:14,868 - INFO - 127.0.0.1 - - [29/Dec/2024 19:58:14] "GET /login?next=/chat_interface HTTP/1.1" 200 -
2024-12-29 19:58:19,353 - INFO - 127.0.0.1 - - [29/Dec/2024 19:58:19] "[32mPOST /login HTTP/1.1[0m" 302 -
2024-12-29 19:58:19,361 - INFO - 127.0.0.1 - - [29/Dec/2024 19:58:19] "GET /chat_interface HTTP/1.1" 200 -
2024-12-29 19:58:19,515 - INFO - 127.0.0.1 - - [29/Dec/2024 19:58:19] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-29 19:58:21,722 - DEBUG - Received chat message
2024-12-29 19:58:21,722 - DEBUG - Chat ID: a580e21b-41ed-40e5-9518-6f840eb7c582
2024-12-29 19:58:21,722 - INFO - User message: Test
2024-12-29 19:58:21,723 - DEBUG - Sending request with 2 messages
2024-12-29 19:58:21,723 - DEBUG - Using deployment: o1-preview
2024-12-29 19:58:21,723 - DEBUG - Payload: [{'role': 'user', 'content': 'Please format your responses in Markdown.'}, {'role': 'user', 'content': 'Test'}]
2024-12-29 19:58:21,727 - DEBUG - Request options: {'method': 'post', 'url': '/deployments/o1-preview/chat/completions', 'headers': {'api-key': '<redacted>'}, 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': 'Please format your responses in Markdown.'}, {'role': 'user', 'content': 'Test'}], 'model': 'o1-preview', 'max_completion_tokens': 500, 'response_format': {'type': 'text'}, 'temperature': 1.0}}
2024-12-29 19:58:21,825 - DEBUG - Sending HTTP Request: POST https://openai-hp.openai.azure.com/openai/deployments/o1-preview/chat/completions?api-version=2024-12-01-preview
2024-12-29 19:58:21,825 - DEBUG - connect_tcp.started host='openai-hp.openai.azure.com' port=443 local_address=None timeout=5.0 socket_options=None
2024-12-29 19:58:21,893 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000180FEFB1D30>
2024-12-29 19:58:21,893 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x00000180FEEF2570> server_hostname='openai-hp.openai.azure.com' timeout=5.0
2024-12-29 19:58:21,974 - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000180FEECF9D0>
2024-12-29 19:58:21,975 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2024-12-29 19:58:21,975 - DEBUG - send_request_headers.complete
2024-12-29 19:58:21,976 - DEBUG - send_request_body.started request=<Request [b'POST']>
2024-12-29 19:58:21,976 - DEBUG - send_request_body.complete
2024-12-29 19:58:21,976 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2024-12-29 19:58:29,914 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Length', b'528'), (b'Content-Type', b'application/json'), (b'apim-request-id', b'4d593057-caa6-4357-bfe6-8ad3782ecfad'), (b'Strict-Transport-Security', b'max-age=31536000; includeSubDomains; preload'), (b'x-content-type-options', b'nosniff'), (b'x-ratelimit-remaining-tokens', b'2399500'), (b'x-accel-buffering', b'no'), (b'x-ms-rai-invoked', b'true'), (b'x-request-id', b'c1daf080-c72c-4afe-8a4d-a442b81f9324'), (b'x-ms-region', b'East US 2'), (b'azureml-model-session', b'd091-20241220021933'), (b'x-ratelimit-remaining-requests', b'399'), (b'x-envoy-upstream-service-time', b'7862'), (b'x-ms-client-request-id', b'4d593057-caa6-4357-bfe6-8ad3782ecfad'), (b'Date', b'Mon, 30 Dec 2024 01:58:28 GMT')])
2024-12-29 19:58:29,915 - INFO - HTTP Request: POST https://openai-hp.openai.azure.com/openai/deployments/o1-preview/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2024-12-29 19:58:29,915 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2024-12-29 19:58:29,916 - DEBUG - receive_response_body.complete
2024-12-29 19:58:29,916 - DEBUG - response_closed.started
2024-12-29 19:58:29,916 - DEBUG - response_closed.complete
2024-12-29 19:58:29,917 - DEBUG - HTTP Response: POST https://openai-hp.openai.azure.com/openai/deployments/o1-preview/chat/completions?api-version=2024-12-01-preview "200 OK" Headers({'content-length': '528', 'content-type': 'application/json', 'apim-request-id': '4d593057-caa6-4357-bfe6-8ad3782ecfad', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'x-content-type-options': 'nosniff', 'x-ratelimit-remaining-tokens': '2399500', 'x-accel-buffering': 'no', 'x-ms-rai-invoked': 'true', 'x-request-id': 'c1daf080-c72c-4afe-8a4d-a442b81f9324', 'x-ms-region': 'East US 2', 'azureml-model-session': 'd091-20241220021933', 'x-ratelimit-remaining-requests': '399', 'x-envoy-upstream-service-time': '7862', 'x-ms-client-request-id': '4d593057-caa6-4357-bfe6-8ad3782ecfad', 'date': 'Mon, 30 Dec 2024 01:58:28 GMT'})
2024-12-29 19:58:29,917 - DEBUG - request_id: c1daf080-c72c-4afe-8a4d-a442b81f9324
2024-12-29 19:58:29,920 - ERROR - API response contains empty content: ChatCompletion(id='chatcmpl-AjzKutXFrTHXaoQWDEwSwRihZ8mOa', choices=[Choice(finish_reason='length', index=0, logprobs=None, message=ChatCompletionMessage(content='', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None), content_filter_results={})], created=1735523900, model='o1-preview-2024-09-12', object='chat.completion', service_tier=None, system_fingerprint='fp_772cb6e46d', usage=CompletionUsage(completion_tokens=500, prompt_tokens=27, total_tokens=527, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=500, rejected_prediction_tokens=None), prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=0)), prompt_filter_results=[{'prompt_index': 0, 'content_filter_results': {}}])
2024-12-29 19:58:29,921 - INFO - Fallback response provided to the user.
2024-12-29 19:58:29,923 - INFO - Received response: <p>The assistant was unable to generate a response. Please try again or rephrase your input.</p>
<div class="shortcuts"><button onclick="copyToClipboard()">Copy</button><button onclick="retryMessage()">Retry</button></div>
2024-12-29 19:58:29,924 - DEBUG - Raw API response: ChatCompletion(id='chatcmpl-AjzKutXFrTHXaoQWDEwSwRihZ8mOa', choices=[Choice(finish_reason='length', index=0, logprobs=None, message=ChatCompletionMessage(content='', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None), content_filter_results={})], created=1735523900, model='o1-preview-2024-09-12', object='chat.completion', service_tier=None, system_fingerprint='fp_772cb6e46d', usage=CompletionUsage(completion_tokens=500, prompt_tokens=27, total_tokens=527, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=500, rejected_prediction_tokens=None), prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=0)), prompt_filter_results=[{'prompt_index': 0, 'content_filter_results': {}}])
2024-12-29 19:58:29,924 - DEBUG - Usage: CompletionUsage(completion_tokens=500, prompt_tokens=27, total_tokens=527, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=500, rejected_prediction_tokens=None), prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=0))
2024-12-29 19:58:29,925 - INFO - 127.0.0.1 - - [29/Dec/2024 19:58:29] "POST /chat HTTP/1.1" 200 -
2024-12-29 19:59:49,753 - INFO -  * Detected change in 'C:\\Users\\htper\\OneDrive\\Folder\\chats\\chatapp\\app.py', reloading
2024-12-29 19:59:49,862 - INFO -  * Restarting with stat
2024-12-29 19:59:52,016 - WARNING -  * Debugger is active!
2024-12-29 19:59:52,023 - INFO -  * Debugger PIN: 995-560-430
2024-12-29 20:01:12,237 - INFO -  * Detected change in 'C:\\Users\\htper\\OneDrive\\Folder\\chats\\chatapp\\app.py', reloading
2024-12-29 20:01:12,351 - INFO -  * Restarting with stat
2024-12-29 20:01:14,291 - WARNING -  * Debugger is active!
2024-12-29 20:01:14,298 - INFO -  * Debugger PIN: 995-560-430
2024-12-29 20:01:54,966 - INFO - 127.0.0.1 - - [29/Dec/2024 20:01:54] "[32mGET /chat_interface HTTP/1.1[0m" 302 -
2024-12-29 20:01:54,979 - INFO - 127.0.0.1 - - [29/Dec/2024 20:01:54] "GET /login?next=/chat_interface HTTP/1.1" 200 -
2024-12-29 20:01:57,734 - INFO - 127.0.0.1 - - [29/Dec/2024 20:01:57] "GET /login?next=/chat_interface HTTP/1.1" 200 -
2024-12-29 20:02:02,305 - INFO - 127.0.0.1 - - [29/Dec/2024 20:02:02] "[32mPOST /login HTTP/1.1[0m" 302 -
2024-12-29 20:02:02,313 - INFO - 127.0.0.1 - - [29/Dec/2024 20:02:02] "GET /chat_interface HTTP/1.1" 200 -
2024-12-29 20:02:02,444 - INFO - 127.0.0.1 - - [29/Dec/2024 20:02:02] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-29 20:02:05,366 - DEBUG - Received chat message
2024-12-29 20:02:05,367 - DEBUG - Chat ID: 7ca098c8-b2da-4f79-a7ba-f2784bb2262f
2024-12-29 20:02:05,367 - INFO - User message: Test
2024-12-29 20:02:05,367 - DEBUG - Sending request with 2 messages
2024-12-29 20:02:05,368 - DEBUG - Using deployment: o1-preview
2024-12-29 20:02:05,368 - DEBUG - Payload being sent to API: [{'role': 'user', 'content': 'Please format your responses in Markdown.'}, {'role': 'user', 'content': 'Test'}]
2024-12-29 20:02:05,368 - DEBUG - Message 0: {'role': 'user', 'content': 'Please format your responses in Markdown.'}
2024-12-29 20:02:05,368 - DEBUG - Message 1: {'role': 'user', 'content': 'Test'}
2024-12-29 20:02:05,373 - DEBUG - Request options: {'method': 'post', 'url': '/deployments/o1-preview/chat/completions', 'headers': {'api-key': '<redacted>'}, 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': 'Please format your responses in Markdown.'}, {'role': 'user', 'content': 'Test'}], 'model': 'o1-preview', 'max_completion_tokens': 500, 'response_format': {'type': 'text'}, 'temperature': 1}}
2024-12-29 20:02:05,458 - DEBUG - Sending HTTP Request: POST https://openai-hp.openai.azure.com/openai/deployments/o1-preview/chat/completions?api-version=2024-12-01-preview
2024-12-29 20:02:05,458 - DEBUG - connect_tcp.started host='openai-hp.openai.azure.com' port=443 local_address=None timeout=5.0 socket_options=None
2024-12-29 20:02:05,535 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001FE8EA95D30>
2024-12-29 20:02:05,536 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x000001FE8E9D2570> server_hostname='openai-hp.openai.azure.com' timeout=5.0
2024-12-29 20:02:05,614 - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001FE8E9AFB10>
2024-12-29 20:02:05,614 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2024-12-29 20:02:05,615 - DEBUG - send_request_headers.complete
2024-12-29 20:02:05,615 - DEBUG - send_request_body.started request=<Request [b'POST']>
2024-12-29 20:02:05,615 - DEBUG - send_request_body.complete
2024-12-29 20:02:05,616 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2024-12-29 20:02:16,926 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Length', b'528'), (b'Content-Type', b'application/json'), (b'apim-request-id', b'f720a568-1666-4fde-ab73-8a7b1faf3e67'), (b'Strict-Transport-Security', b'max-age=31536000; includeSubDomains; preload'), (b'x-content-type-options', b'nosniff'), (b'x-ratelimit-remaining-tokens', b'2399500'), (b'x-accel-buffering', b'no'), (b'x-ms-rai-invoked', b'true'), (b'x-request-id', b'8a897594-125a-4b4f-b6e7-63fffd6cc9b5'), (b'x-ms-region', b'East US 2'), (b'azureml-model-session', b'd091-20241220021933'), (b'x-ratelimit-remaining-requests', b'399'), (b'x-envoy-upstream-service-time', b'11236'), (b'x-ms-client-request-id', b'f720a568-1666-4fde-ab73-8a7b1faf3e67'), (b'Date', b'Mon, 30 Dec 2024 02:02:15 GMT')])
2024-12-29 20:02:16,927 - INFO - HTTP Request: POST https://openai-hp.openai.azure.com/openai/deployments/o1-preview/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2024-12-29 20:02:16,928 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2024-12-29 20:02:16,928 - DEBUG - receive_response_body.complete
2024-12-29 20:02:16,928 - DEBUG - response_closed.started
2024-12-29 20:02:16,929 - DEBUG - response_closed.complete
2024-12-29 20:02:16,929 - DEBUG - HTTP Response: POST https://openai-hp.openai.azure.com/openai/deployments/o1-preview/chat/completions?api-version=2024-12-01-preview "200 OK" Headers({'content-length': '528', 'content-type': 'application/json', 'apim-request-id': 'f720a568-1666-4fde-ab73-8a7b1faf3e67', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'x-content-type-options': 'nosniff', 'x-ratelimit-remaining-tokens': '2399500', 'x-accel-buffering': 'no', 'x-ms-rai-invoked': 'true', 'x-request-id': '8a897594-125a-4b4f-b6e7-63fffd6cc9b5', 'x-ms-region': 'East US 2', 'azureml-model-session': 'd091-20241220021933', 'x-ratelimit-remaining-requests': '399', 'x-envoy-upstream-service-time': '11236', 'x-ms-client-request-id': 'f720a568-1666-4fde-ab73-8a7b1faf3e67', 'date': 'Mon, 30 Dec 2024 02:02:15 GMT'})
2024-12-29 20:02:16,929 - DEBUG - request_id: 8a897594-125a-4b4f-b6e7-63fffd6cc9b5
2024-12-29 20:02:16,933 - ERROR - API response contains empty content: ChatCompletion(id='chatcmpl-AjzOWiw1Sryko4YaqX02Kxe5h6LUQ', choices=[Choice(finish_reason='length', index=0, logprobs=None, message=ChatCompletionMessage(content='', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None), content_filter_results={})], created=1735524124, model='o1-preview-2024-09-12', object='chat.completion', service_tier=None, system_fingerprint='fp_772cb6e46d', usage=CompletionUsage(completion_tokens=500, prompt_tokens=27, total_tokens=527, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=500, rejected_prediction_tokens=None), prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=0)), prompt_filter_results=[{'prompt_index': 0, 'content_filter_results': {}}])
2024-12-29 20:02:16,934 - INFO - Fallback response provided to the user.
2024-12-29 20:02:16,936 - INFO - Received response: <p>The assistant was unable to generate a response. Please try again or rephrase your input.</p>
<div class="shortcuts"><button onclick="copyToClipboard()">Copy</button><button onclick="retryMessage()">Retry</button></div>
2024-12-29 20:02:16,936 - DEBUG - Raw API response: ChatCompletion(id='chatcmpl-AjzOWiw1Sryko4YaqX02Kxe5h6LUQ', choices=[Choice(finish_reason='length', index=0, logprobs=None, message=ChatCompletionMessage(content='', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None), content_filter_results={})], created=1735524124, model='o1-preview-2024-09-12', object='chat.completion', service_tier=None, system_fingerprint='fp_772cb6e46d', usage=CompletionUsage(completion_tokens=500, prompt_tokens=27, total_tokens=527, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=500, rejected_prediction_tokens=None), prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=0)), prompt_filter_results=[{'prompt_index': 0, 'content_filter_results': {}}])
2024-12-29 20:02:16,937 - DEBUG - Usage: CompletionUsage(completion_tokens=500, prompt_tokens=27, total_tokens=527, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=500, rejected_prediction_tokens=None), prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=0))
2024-12-29 20:02:16,938 - INFO - 127.0.0.1 - - [29/Dec/2024 20:02:16] "POST /chat HTTP/1.1" 200 -
2024-12-29 20:02:23,645 - INFO - 127.0.0.1 - - [29/Dec/2024 20:02:23] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-29 20:02:50,991 - INFO -  * Detected change in 'C:\\Users\\htper\\OneDrive\\Folder\\chats\\chatapp\\app.py', reloading
2024-12-29 20:02:51,117 - INFO -  * Restarting with stat
2024-12-29 20:02:53,204 - WARNING -  * Debugger is active!
2024-12-29 20:02:53,212 - INFO -  * Debugger PIN: 995-560-430
2024-12-29 20:03:19,275 - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-12-29 20:03:19,275 - INFO - [33mPress CTRL+C to quit[0m
2024-12-29 20:03:19,277 - INFO -  * Restarting with stat
2024-12-29 20:03:21,330 - WARNING -  * Debugger is active!
2024-12-29 20:03:21,339 - INFO -  * Debugger PIN: 995-560-430
2024-12-29 20:03:22,629 - INFO - 127.0.0.1 - - [29/Dec/2024 20:03:22] "[32mGET /chat_interface HTTP/1.1[0m" 302 -
2024-12-29 20:03:22,642 - INFO - 127.0.0.1 - - [29/Dec/2024 20:03:22] "GET /login?next=/chat_interface HTTP/1.1" 200 -
2024-12-29 20:03:26,681 - INFO - 127.0.0.1 - - [29/Dec/2024 20:03:26] "POST /login HTTP/1.1" 200 -
2024-12-29 20:03:32,151 - INFO - 127.0.0.1 - - [29/Dec/2024 20:03:32] "[32mPOST /login HTTP/1.1[0m" 302 -
2024-12-29 20:03:32,160 - INFO - 127.0.0.1 - - [29/Dec/2024 20:03:32] "GET /chat_interface HTTP/1.1" 200 -
2024-12-29 20:03:32,303 - INFO - 127.0.0.1 - - [29/Dec/2024 20:03:32] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-29 20:03:34,740 - DEBUG - Received chat message
2024-12-29 20:03:34,740 - DEBUG - Chat ID: 72204c18-3bc5-4494-9d80-cc71248f4ccc
2024-12-29 20:03:34,740 - INFO - User message: Hello
2024-12-29 20:03:34,741 - DEBUG - Sending request with 2 messages
2024-12-29 20:03:34,741 - DEBUG - Using deployment: o1-preview
2024-12-29 20:03:34,741 - DEBUG - Payload being sent to API: [{'role': 'user', 'content': 'Please format your responses in Markdown.'}, {'role': 'user', 'content': 'Hello'}]
2024-12-29 20:03:34,741 - DEBUG - API Endpoint: https://openai-hp.openai.azure.com/
2024-12-29 20:03:34,741 - DEBUG - Model Deployment Name: o1-preview
2024-12-29 20:03:34,742 - DEBUG - Message 0: {'role': 'user', 'content': 'Please format your responses in Markdown.'}
2024-12-29 20:03:34,742 - DEBUG - Message 1: {'role': 'user', 'content': 'Hello'}
2024-12-29 20:03:34,748 - DEBUG - Request options: {'method': 'post', 'url': '/deployments/o1-preview/chat/completions', 'headers': {'api-key': '<redacted>'}, 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': 'Please format your responses in Markdown.'}, {'role': 'user', 'content': 'Hello'}], 'model': 'o1-preview', 'max_completion_tokens': 500, 'response_format': {'type': 'text'}, 'temperature': 1}}
2024-12-29 20:03:34,838 - DEBUG - Sending HTTP Request: POST https://openai-hp.openai.azure.com/openai/deployments/o1-preview/chat/completions?api-version=2024-12-01-preview
2024-12-29 20:03:34,839 - DEBUG - connect_tcp.started host='openai-hp.openai.azure.com' port=443 local_address=None timeout=5.0 socket_options=None
2024-12-29 20:03:34,895 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024953BB5D30>
2024-12-29 20:03:34,895 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x0000024953AF2570> server_hostname='openai-hp.openai.azure.com' timeout=5.0
2024-12-29 20:03:34,970 - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024953ACFB10>
2024-12-29 20:03:34,971 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2024-12-29 20:03:34,971 - DEBUG - send_request_headers.complete
2024-12-29 20:03:34,971 - DEBUG - send_request_body.started request=<Request [b'POST']>
2024-12-29 20:03:34,972 - DEBUG - send_request_body.complete
2024-12-29 20:03:34,972 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2024-12-29 20:03:45,395 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Length', b'568'), (b'Content-Type', b'application/json'), (b'apim-request-id', b'667409b4-7c39-4102-889e-0bfe62bb23a9'), (b'Strict-Transport-Security', b'max-age=31536000; includeSubDomains; preload'), (b'x-content-type-options', b'nosniff'), (b'x-ratelimit-remaining-tokens', b'2399500'), (b'x-accel-buffering', b'no'), (b'x-ms-rai-invoked', b'true'), (b'x-request-id', b'f52670b0-69b1-4320-b6e0-a82f83edc258'), (b'x-ms-region', b'East US 2'), (b'azureml-model-session', b'd091-20241220021933'), (b'x-ratelimit-remaining-requests', b'399'), (b'x-envoy-upstream-service-time', b'10258'), (b'x-ms-client-request-id', b'667409b4-7c39-4102-889e-0bfe62bb23a9'), (b'Date', b'Mon, 30 Dec 2024 02:03:44 GMT')])
2024-12-29 20:03:45,396 - INFO - HTTP Request: POST https://openai-hp.openai.azure.com/openai/deployments/o1-preview/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2024-12-29 20:03:45,397 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2024-12-29 20:03:45,397 - DEBUG - receive_response_body.complete
2024-12-29 20:03:45,398 - DEBUG - response_closed.started
2024-12-29 20:03:45,398 - DEBUG - response_closed.complete
2024-12-29 20:03:45,398 - DEBUG - HTTP Response: POST https://openai-hp.openai.azure.com/openai/deployments/o1-preview/chat/completions?api-version=2024-12-01-preview "200 OK" Headers({'content-length': '568', 'content-type': 'application/json', 'apim-request-id': '667409b4-7c39-4102-889e-0bfe62bb23a9', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'x-content-type-options': 'nosniff', 'x-ratelimit-remaining-tokens': '2399500', 'x-accel-buffering': 'no', 'x-ms-rai-invoked': 'true', 'x-request-id': 'f52670b0-69b1-4320-b6e0-a82f83edc258', 'x-ms-region': 'East US 2', 'azureml-model-session': 'd091-20241220021933', 'x-ratelimit-remaining-requests': '399', 'x-envoy-upstream-service-time': '10258', 'x-ms-client-request-id': '667409b4-7c39-4102-889e-0bfe62bb23a9', 'date': 'Mon, 30 Dec 2024 02:03:44 GMT'})
2024-12-29 20:03:45,399 - DEBUG - request_id: f52670b0-69b1-4320-b6e0-a82f83edc258
2024-12-29 20:03:45,477 - DEBUG - Usage: CompletionUsage(completion_tokens=408, prompt_tokens=27, total_tokens=435, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=384, rejected_prediction_tokens=None), prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=0))
2024-12-29 20:03:45,478 - INFO - 127.0.0.1 - - [29/Dec/2024 20:03:45] "POST /chat HTTP/1.1" 200 -
2024-12-29 20:04:01,367 - DEBUG - Received chat message
2024-12-29 20:04:01,367 - DEBUG - Chat ID: 72204c18-3bc5-4494-9d80-cc71248f4ccc
2024-12-29 20:04:01,367 - INFO - User message: from openai import AzureOpenAI client = AzureOpenAI(   api_version="2024-12-01-preview",   azure_endpoint="https://your-resource.openai.azure.com/",   api_key=os.getenv("AZURE_OPENAI_API_KEY"), ) response = client.chat.completions.create(     model="your-o1-preview-deployment",     messages=[{"role": "user", "content": "Your prompt here"}],     temperature=1,     max_completion_tokens=500, ) print(response.model_dump_json(indent=2))
2024-12-29 20:04:01,368 - DEBUG - Sending request with 3 messages
2024-12-29 20:04:01,368 - DEBUG - Using deployment: o1-preview
2024-12-29 20:04:01,368 - DEBUG - Payload being sent to API: [{'role': 'user', 'content': 'Please format your responses in Markdown.'}, {'role': 'user', 'content': 'Hello'}, {'role': 'user', 'content': 'from openai import AzureOpenAI client = AzureOpenAI(   api_version="2024-12-01-preview",   azure_endpoint="https://your-resource.openai.azure.com/",   api_key=os.getenv("AZURE_OPENAI_API_KEY"), ) response = client.chat.completions.create(     model="your-o1-preview-deployment",     messages=[{"role": "user", "content": "Your prompt here"}],     temperature=1,     max_completion_tokens=500, ) print(response.model_dump_json(indent=2))'}]
2024-12-29 20:04:01,368 - DEBUG - API Endpoint: https://openai-hp.openai.azure.com/
2024-12-29 20:04:01,369 - DEBUG - Model Deployment Name: o1-preview
2024-12-29 20:04:01,369 - DEBUG - Message 0: {'role': 'user', 'content': 'Please format your responses in Markdown.'}
2024-12-29 20:04:01,369 - DEBUG - Message 1: {'role': 'user', 'content': 'Hello'}
2024-12-29 20:04:01,369 - DEBUG - Message 2: {'role': 'user', 'content': 'from openai import AzureOpenAI client = AzureOpenAI(   api_version="2024-12-01-preview",   azure_endpoint="https://your-resource.openai.azure.com/",   api_key=os.getenv("AZURE_OPENAI_API_KEY"), ) response = client.chat.completions.create(     model="your-o1-preview-deployment",     messages=[{"role": "user", "content": "Your prompt here"}],     temperature=1,     max_completion_tokens=500, ) print(response.model_dump_json(indent=2))'}
2024-12-29 20:04:01,373 - DEBUG - Request options: {'method': 'post', 'url': '/deployments/o1-preview/chat/completions', 'headers': {'api-key': '<redacted>'}, 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': 'Please format your responses in Markdown.'}, {'role': 'user', 'content': 'Hello'}, {'role': 'user', 'content': 'from openai import AzureOpenAI client = AzureOpenAI(   api_version="2024-12-01-preview",   azure_endpoint="https://your-resource.openai.azure.com/",   api_key=os.getenv("AZURE_OPENAI_API_KEY"), ) response = client.chat.completions.create(     model="your-o1-preview-deployment",     messages=[{"role": "user", "content": "Your prompt here"}],     temperature=1,     max_completion_tokens=500, ) print(response.model_dump_json(indent=2))'}], 'model': 'o1-preview', 'max_completion_tokens': 500, 'response_format': {'type': 'text'}, 'temperature': 1}}
2024-12-29 20:04:01,374 - DEBUG - Sending HTTP Request: POST https://openai-hp.openai.azure.com/openai/deployments/o1-preview/chat/completions?api-version=2024-12-01-preview
2024-12-29 20:04:01,374 - DEBUG - close.started
2024-12-29 20:04:01,375 - DEBUG - close.complete
2024-12-29 20:04:01,375 - DEBUG - connect_tcp.started host='openai-hp.openai.azure.com' port=443 local_address=None timeout=5.0 socket_options=None
2024-12-29 20:04:01,453 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024953BE0410>
2024-12-29 20:04:01,453 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x0000024953AF2570> server_hostname='openai-hp.openai.azure.com' timeout=5.0
2024-12-29 20:04:01,533 - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024953BA1BA0>
2024-12-29 20:04:01,533 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2024-12-29 20:04:01,534 - DEBUG - send_request_headers.complete
2024-12-29 20:04:01,534 - DEBUG - send_request_body.started request=<Request [b'POST']>
2024-12-29 20:04:01,534 - DEBUG - send_request_body.complete
2024-12-29 20:04:01,535 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2024-12-29 20:04:09,472 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Length', b'529'), (b'Content-Type', b'application/json'), (b'apim-request-id', b'3af32d98-fb28-4e18-8936-782675188d1d'), (b'Strict-Transport-Security', b'max-age=31536000; includeSubDomains; preload'), (b'x-content-type-options', b'nosniff'), (b'x-ratelimit-remaining-tokens', b'2399000'), (b'x-accel-buffering', b'no'), (b'x-ms-rai-invoked', b'true'), (b'x-request-id', b'77ea312d-aed4-4048-b989-941817ac8355'), (b'x-ms-region', b'East US 2'), (b'azureml-model-session', b'd080-20241219162549'), (b'x-ratelimit-remaining-requests', b'398'), (b'x-envoy-upstream-service-time', b'7860'), (b'x-ms-client-request-id', b'3af32d98-fb28-4e18-8936-782675188d1d'), (b'Date', b'Mon, 30 Dec 2024 02:04:07 GMT')])
2024-12-29 20:04:09,473 - INFO - HTTP Request: POST https://openai-hp.openai.azure.com/openai/deployments/o1-preview/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2024-12-29 20:04:09,473 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2024-12-29 20:04:09,473 - DEBUG - receive_response_body.complete
2024-12-29 20:04:09,474 - DEBUG - response_closed.started
2024-12-29 20:04:09,474 - DEBUG - response_closed.complete
2024-12-29 20:04:09,474 - DEBUG - HTTP Response: POST https://openai-hp.openai.azure.com/openai/deployments/o1-preview/chat/completions?api-version=2024-12-01-preview "200 OK" Headers({'content-length': '529', 'content-type': 'application/json', 'apim-request-id': '3af32d98-fb28-4e18-8936-782675188d1d', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'x-content-type-options': 'nosniff', 'x-ratelimit-remaining-tokens': '2399000', 'x-accel-buffering': 'no', 'x-ms-rai-invoked': 'true', 'x-request-id': '77ea312d-aed4-4048-b989-941817ac8355', 'x-ms-region': 'East US 2', 'azureml-model-session': 'd080-20241219162549', 'x-ratelimit-remaining-requests': '398', 'x-envoy-upstream-service-time': '7860', 'x-ms-client-request-id': '3af32d98-fb28-4e18-8936-782675188d1d', 'date': 'Mon, 30 Dec 2024 02:04:07 GMT'})
2024-12-29 20:04:09,474 - DEBUG - request_id: 77ea312d-aed4-4048-b989-941817ac8355
2024-12-29 20:04:09,475 - ERROR - API response contains empty content: ChatCompletion(id='chatcmpl-AjzQODLnUfCVUiBBaMy6djCNth7uW', choices=[Choice(finish_reason='length', index=0, logprobs=None, message=ChatCompletionMessage(content='', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None), content_filter_results={})], created=1735524240, model='o1-preview-2024-09-12', object='chat.completion', service_tier=None, system_fingerprint='fp_772cb6e46d', usage=CompletionUsage(completion_tokens=500, prompt_tokens=149, total_tokens=649, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=500, rejected_prediction_tokens=None), prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=0)), prompt_filter_results=[{'prompt_index': 0, 'content_filter_results': {}}])
2024-12-29 20:04:09,475 - INFO - Fallback response provided to the user.
2024-12-29 20:04:09,476 - INFO - Received response: <p>The assistant was unable to generate a response. Please try again or rephrase your input.</p>
<div class="shortcuts"><button onclick="copyToClipboard()">Copy</button><button onclick="retryMessage()">Retry</button></div>
2024-12-29 20:04:09,476 - DEBUG - Raw API response: ChatCompletion(id='chatcmpl-AjzQODLnUfCVUiBBaMy6djCNth7uW', choices=[Choice(finish_reason='length', index=0, logprobs=None, message=ChatCompletionMessage(content='', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None), content_filter_results={})], created=1735524240, model='o1-preview-2024-09-12', object='chat.completion', service_tier=None, system_fingerprint='fp_772cb6e46d', usage=CompletionUsage(completion_tokens=500, prompt_tokens=149, total_tokens=649, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=500, rejected_prediction_tokens=None), prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=0)), prompt_filter_results=[{'prompt_index': 0, 'content_filter_results': {}}])
2024-12-29 20:04:09,476 - DEBUG - Usage: CompletionUsage(completion_tokens=500, prompt_tokens=149, total_tokens=649, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=500, rejected_prediction_tokens=None), prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=0))
2024-12-29 20:04:09,477 - INFO - 127.0.0.1 - - [29/Dec/2024 20:04:09] "POST /chat HTTP/1.1" 200 -
2024-12-29 20:05:11,812 - INFO -  * Detected change in 'C:\\Users\\htper\\OneDrive\\Folder\\chats\\chatapp\\app.py', reloading
2024-12-29 20:05:11,933 - INFO -  * Restarting with stat
2024-12-29 20:05:13,866 - WARNING -  * Debugger is active!
2024-12-29 20:05:13,873 - INFO -  * Debugger PIN: 995-560-430
2024-12-29 20:05:29,160 - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-12-29 20:05:29,161 - INFO - [33mPress CTRL+C to quit[0m
2024-12-29 20:05:29,162 - INFO -  * Restarting with stat
2024-12-29 20:05:31,270 - WARNING -  * Debugger is active!
2024-12-29 20:05:31,277 - INFO -  * Debugger PIN: 995-560-430
2024-12-29 20:05:31,341 - INFO - 127.0.0.1 - - [29/Dec/2024 20:05:31] "[32mGET /chat_interface HTTP/1.1[0m" 302 -
2024-12-29 20:05:31,343 - INFO - 127.0.0.1 - - [29/Dec/2024 20:05:31] "[32mGET /chat_interface HTTP/1.1[0m" 302 -
2024-12-29 20:05:31,345 - INFO - 127.0.0.1 - - [29/Dec/2024 20:05:31] "[32mGET /chat_interface HTTP/1.1[0m" 302 -
2024-12-29 20:05:31,355 - INFO - 127.0.0.1 - - [29/Dec/2024 20:05:31] "GET /login?next=/chat_interface HTTP/1.1" 200 -
2024-12-29 20:05:35,231 - INFO - 127.0.0.1 - - [29/Dec/2024 20:05:35] "[32mPOST /login HTTP/1.1[0m" 302 -
2024-12-29 20:05:35,241 - INFO - 127.0.0.1 - - [29/Dec/2024 20:05:35] "GET /chat_interface HTTP/1.1" 200 -
2024-12-29 20:05:35,400 - INFO - 127.0.0.1 - - [29/Dec/2024 20:05:35] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-29 20:05:38,218 - DEBUG - Received chat message
2024-12-29 20:05:38,218 - DEBUG - Chat ID: cdf5ad1c-d3aa-42e4-b315-d139c3e4ee73
2024-12-29 20:05:38,218 - INFO - User message: Test
2024-12-29 20:05:38,219 - DEBUG - Sending request with 2 messages
2024-12-29 20:05:38,219 - DEBUG - Using deployment: o1-preview
2024-12-29 20:05:38,219 - DEBUG - Payload being sent to API: [{'role': 'user', 'content': 'Please format your responses in Markdown.'}, {'role': 'user', 'content': 'Test'}]
2024-12-29 20:05:38,219 - DEBUG - API Endpoint: https://openai-hp.openai.azure.com/
2024-12-29 20:05:38,220 - DEBUG - Model Deployment Name: o1-preview
2024-12-29 20:05:38,220 - DEBUG - Message 0: {'role': 'user', 'content': 'Please format your responses in Markdown.'}
2024-12-29 20:05:38,220 - DEBUG - Message 1: {'role': 'user', 'content': 'Test'}
2024-12-29 20:05:38,225 - DEBUG - Request options: {'method': 'post', 'url': '/deployments/o1-preview/chat/completions', 'headers': {'api-key': '<redacted>'}, 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': 'Please format your responses in Markdown.'}, {'role': 'user', 'content': 'Test'}], 'model': 'o1-preview', 'max_completion_tokens': 500, 'response_format': {'type': 'text'}, 'temperature': 1}}
2024-12-29 20:05:38,315 - DEBUG - Sending HTTP Request: POST https://openai-hp.openai.azure.com/openai/deployments/o1-preview/chat/completions?api-version=2024-12-01-preview
2024-12-29 20:05:38,316 - DEBUG - connect_tcp.started host='openai-hp.openai.azure.com' port=443 local_address=None timeout=5.0 socket_options=None
2024-12-29 20:05:38,397 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000025CEF6457F0>
2024-12-29 20:05:38,397 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x0000025CEF582570> server_hostname='openai-hp.openai.azure.com' timeout=5.0
2024-12-29 20:05:38,485 - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000025CEF5639D0>
2024-12-29 20:05:38,486 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2024-12-29 20:05:38,486 - DEBUG - send_request_headers.complete
2024-12-29 20:05:38,486 - DEBUG - send_request_body.started request=<Request [b'POST']>
2024-12-29 20:05:38,487 - DEBUG - send_request_body.complete
2024-12-29 20:05:38,487 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2024-12-29 20:05:49,176 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Length', b'528'), (b'Content-Type', b'application/json'), (b'apim-request-id', b'4aefb502-3265-4317-8418-0c110b8b2979'), (b'Strict-Transport-Security', b'max-age=31536000; includeSubDomains; preload'), (b'x-content-type-options', b'nosniff'), (b'x-ratelimit-remaining-tokens', b'2399500'), (b'x-accel-buffering', b'no'), (b'x-ms-rai-invoked', b'true'), (b'x-request-id', b'22eca581-1f16-40b0-bff2-c84b27c3e465'), (b'x-ms-region', b'East US 2'), (b'azureml-model-session', b'd084-20241219200032'), (b'x-ratelimit-remaining-requests', b'399'), (b'x-envoy-upstream-service-time', b'10609'), (b'x-ms-client-request-id', b'4aefb502-3265-4317-8418-0c110b8b2979'), (b'Date', b'Mon, 30 Dec 2024 02:05:47 GMT')])
2024-12-29 20:05:49,177 - INFO - HTTP Request: POST https://openai-hp.openai.azure.com/openai/deployments/o1-preview/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2024-12-29 20:05:49,177 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2024-12-29 20:05:49,178 - DEBUG - receive_response_body.complete
2024-12-29 20:05:49,178 - DEBUG - response_closed.started
2024-12-29 20:05:49,178 - DEBUG - response_closed.complete
2024-12-29 20:05:49,179 - DEBUG - HTTP Response: POST https://openai-hp.openai.azure.com/openai/deployments/o1-preview/chat/completions?api-version=2024-12-01-preview "200 OK" Headers({'content-length': '528', 'content-type': 'application/json', 'apim-request-id': '4aefb502-3265-4317-8418-0c110b8b2979', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'x-content-type-options': 'nosniff', 'x-ratelimit-remaining-tokens': '2399500', 'x-accel-buffering': 'no', 'x-ms-rai-invoked': 'true', 'x-request-id': '22eca581-1f16-40b0-bff2-c84b27c3e465', 'x-ms-region': 'East US 2', 'azureml-model-session': 'd084-20241219200032', 'x-ratelimit-remaining-requests': '399', 'x-envoy-upstream-service-time': '10609', 'x-ms-client-request-id': '4aefb502-3265-4317-8418-0c110b8b2979', 'date': 'Mon, 30 Dec 2024 02:05:47 GMT'})
2024-12-29 20:05:49,179 - DEBUG - request_id: 22eca581-1f16-40b0-bff2-c84b27c3e465
2024-12-29 20:05:49,184 - ERROR - API response contains empty content: ChatCompletion(id='chatcmpl-AjzRxekGcaTJstEAKNCVwI5vZfxvc', choices=[Choice(finish_reason='length', index=0, logprobs=None, message=ChatCompletionMessage(content='', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None), content_filter_results={})], created=1735524337, model='o1-preview-2024-09-12', object='chat.completion', service_tier=None, system_fingerprint='fp_772cb6e46d', usage=CompletionUsage(completion_tokens=500, prompt_tokens=27, total_tokens=527, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=500, rejected_prediction_tokens=None), prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=0)), prompt_filter_results=[{'prompt_index': 0, 'content_filter_results': {}}])
2024-12-29 20:05:49,184 - INFO - Fallback response provided to the user.
2024-12-29 20:05:49,187 - INFO - Received response: <p>The assistant was unable to generate a response. Please try again or rephrase your input.</p>
<div class="shortcuts"><button onclick="copyToClipboard()">Copy</button><button onclick="retryMessage()">Retry</button></div>
2024-12-29 20:05:49,188 - DEBUG - Raw API response: ChatCompletion(id='chatcmpl-AjzRxekGcaTJstEAKNCVwI5vZfxvc', choices=[Choice(finish_reason='length', index=0, logprobs=None, message=ChatCompletionMessage(content='', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None), content_filter_results={})], created=1735524337, model='o1-preview-2024-09-12', object='chat.completion', service_tier=None, system_fingerprint='fp_772cb6e46d', usage=CompletionUsage(completion_tokens=500, prompt_tokens=27, total_tokens=527, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=500, rejected_prediction_tokens=None), prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=0)), prompt_filter_results=[{'prompt_index': 0, 'content_filter_results': {}}])
2024-12-29 20:05:49,188 - DEBUG - Usage: CompletionUsage(completion_tokens=500, prompt_tokens=27, total_tokens=527, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=500, rejected_prediction_tokens=None), prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=0))
2024-12-29 20:05:49,189 - INFO - 127.0.0.1 - - [29/Dec/2024 20:05:49] "POST /chat HTTP/1.1" 200 -
2024-12-29 20:06:03,387 - DEBUG - Received chat message
2024-12-29 20:06:03,387 - DEBUG - Chat ID: cdf5ad1c-d3aa-42e4-b315-d139c3e4ee73
2024-12-29 20:06:03,388 - INFO - User message: import json from typing import List, Dict, Any from openai import AzureOpenAI  class ToolManager:     def __init__(self):         self.available_tools: Dict[str, callable] = {}         self.tool_schemas: Dict[str, Dict] = {}      def register_tool(self, name: str, func: callable, schema: Dict):         self.available_tools[name] = func         self.tool_schemas[name] = {             "type": "function",             "function": {                 "name": name,                 "description": schema.get("description", ""),                 "parameters": schema             }         }     def get_tool_definitions(self) -> List[Dict]:         return list(self.tool_schemas.values())     async def execute_tool(self, tool_call: Dict[str, Any]) -> Dict[str, Any]:         tool_name = tool_call.get("function", {}).get("name")         if tool_name not in self.available_tools:             raise ValueError(f"Unknown tool: {tool_name}")         arguments = json.loads(tool_call["function"]["arguments"])         tool_func = self.available_tools[tool_name]         return await tool_func(**arguments)  class EnhancedFunctionCaller:     def __init__(self, tool_manager: ToolManager, client: AzureOpenAI):         self.tool_manager = tool_manager         self.client = client     async def process_with_tools(self, messages: List[Dict[str, str]], tool_choice: str = "auto") -> Dict[str, Any]:         try:             response = await self.client.chat.completions.acreate(                 model="gpt-4",                 messages=messages,                 tools=self.tool_manager.get_tool_definitions(),                 tool_choice=tool_choice             )             if response.choices[0].message.tool_calls:                 tool_messages = [response.choices[0].message.dict()]                 for tool_call in response.choices[0].message.tool_calls:                     tool_result = await self.tool_manager.execute_tool(tool_call.dict())                     tool_messages.append({                         "role": "tool",                         "content": json.dumps(tool_result),                         "tool_call_id": tool_call.id                     })                 final_response = await self.client.chat.completions.acreate(                     model="gpt-4",                     messages=messages + tool_messages                 )                 return final_response.choices[0].message.dict()             return response.choices[0].message.dict()         except Exception as e:             print(f"Error in function calling: {str(e)}")             raise # Example usage tool_manager = ToolManager() weather_schema = {     "type": "object",     "properties": {         "location": {             "type": "string",             "description": "City name"         },         "units": {             "type": "string",             "enum": ["celsius", "fahrenheit"]         }     },     "required": ["location"] } async def get_weather(location: str, units: str = "celsius") -> Dict[str, Any]:     return {"temperature": 20, "units": units, "location": location} tool_manager.register_tool("get_weather", get_weather, weather_schema) function_caller = EnhancedFunctionCaller(tool_manager, client)
2024-12-29 20:06:03,388 - DEBUG - Sending request with 3 messages
2024-12-29 20:06:03,388 - DEBUG - Using deployment: o1-preview
2024-12-29 20:06:03,388 - DEBUG - Payload being sent to API: [{'role': 'user', 'content': 'Please format your responses in Markdown.'}, {'role': 'user', 'content': 'Test'}, {'role': 'user', 'content': '```\nimport json from typing import List, Dict, Any from openai import AzureOpenAI  class ToolManager:     def __init__(self):         self.available_tools: Dict[str, callable] = {}         self.tool_schemas: Dict[str, Dict] = {}      def register_tool(self, name: str, func: callable, schema: Dict):         self.available_tools[name] = func         self.tool_schemas[name] = {             "type": "function",             "function": {                 "name": name,                 "description": schema.get("description", ""),                 "parameters": schema             }         }     def get_tool_definitions(self) -> List[Dict]:         return list(self.tool_schemas.values())     async def execute_tool(self, tool_call: Dict[str, Any]) -> Dict[str, Any]:         tool_name = tool_call.get("function", {}).get("name")         if tool_name not in self.available_tools:             raise ValueError(f"Unknown tool: {tool_name}")         arguments = json.loads(tool_call["function"]["arguments"])         tool_func = self.available_tools[tool_name]         return await tool_func(**arguments)  class EnhancedFunctionCaller:     def __init__(self, tool_manager: ToolManager, client: AzureOpenAI):         self.tool_manager = tool_manager         self.client = client     async def process_with_tools(self, messages: List[Dict[str, str]], tool_choice: str = "auto") -> Dict[str, Any]:         try:             response = await self.client.chat.completions.acreate(                 model="gpt-4",                 messages=messages,                 tools=self.tool_manager.get_tool_definitions(),                 tool_choice=tool_choice             )             if response.choices[0].message.tool_calls:                 tool_messages = [response.choices[0].message.dict()]                 for tool_call in response.choices[0].message.tool_calls:                     tool_result = await self.tool_manager.execute_tool(tool_call.dict())                     tool_messages.append({                         "role": "tool",                         "content": json.dumps(tool_result),                         "tool_call_id": tool_call.id                     })                 final_response = await self.client.chat.completions.acreate(                     model="gpt-4",                     messages=messages + tool_messages                 )                 return final_response.choices[0].message.dict()             return response.choices[0].message.dict()         except Exception as e:             print(f"Error in function calling: {str(e)}")             raise # Example usage tool_manager = ToolManager() weather_schema = {     "type": "object",     "properties": {         "location": {             "type": "string",             "description": "City name"         },         "units": {             "type": "string",             "enum": ["celsius", "fahrenheit"]         }     },     "required": ["location"] } async def get_weather(location: str, units: str = "celsius") -> Dict[str, Any]:     return {"temperature": 20, "units": units, "location": location} tool_manager.register_tool("get_weather", get_weather, weather_schema) function_caller = EnhancedFunctionCaller(tool_manager, client)\n```'}]
2024-12-29 20:06:03,389 - DEBUG - API Endpoint: https://openai-hp.openai.azure.com/
2024-12-29 20:06:03,389 - DEBUG - Model Deployment Name: o1-preview
2024-12-29 20:06:03,390 - DEBUG - Message 0: {'role': 'user', 'content': 'Please format your responses in Markdown.'}
2024-12-29 20:06:03,390 - DEBUG - Message 1: {'role': 'user', 'content': 'Test'}
2024-12-29 20:06:03,391 - DEBUG - Message 2: {'role': 'user', 'content': '```\nimport json from typing import List, Dict, Any from openai import AzureOpenAI  class ToolManager:     def __init__(self):         self.available_tools: Dict[str, callable] = {}         self.tool_schemas: Dict[str, Dict] = {}      def register_tool(self, name: str, func: callable, schema: Dict):         self.available_tools[name] = func         self.tool_schemas[name] = {             "type": "function",             "function": {                 "name": name,                 "description": schema.get("description", ""),                 "parameters": schema             }         }     def get_tool_definitions(self) -> List[Dict]:         return list(self.tool_schemas.values())     async def execute_tool(self, tool_call: Dict[str, Any]) -> Dict[str, Any]:         tool_name = tool_call.get("function", {}).get("name")         if tool_name not in self.available_tools:             raise ValueError(f"Unknown tool: {tool_name}")         arguments = json.loads(tool_call["function"]["arguments"])         tool_func = self.available_tools[tool_name]         return await tool_func(**arguments)  class EnhancedFunctionCaller:     def __init__(self, tool_manager: ToolManager, client: AzureOpenAI):         self.tool_manager = tool_manager         self.client = client     async def process_with_tools(self, messages: List[Dict[str, str]], tool_choice: str = "auto") -> Dict[str, Any]:         try:             response = await self.client.chat.completions.acreate(                 model="gpt-4",                 messages=messages,                 tools=self.tool_manager.get_tool_definitions(),                 tool_choice=tool_choice             )             if response.choices[0].message.tool_calls:                 tool_messages = [response.choices[0].message.dict()]                 for tool_call in response.choices[0].message.tool_calls:                     tool_result = await self.tool_manager.execute_tool(tool_call.dict())                     tool_messages.append({                         "role": "tool",                         "content": json.dumps(tool_result),                         "tool_call_id": tool_call.id                     })                 final_response = await self.client.chat.completions.acreate(                     model="gpt-4",                     messages=messages + tool_messages                 )                 return final_response.choices[0].message.dict()             return response.choices[0].message.dict()         except Exception as e:             print(f"Error in function calling: {str(e)}")             raise # Example usage tool_manager = ToolManager() weather_schema = {     "type": "object",     "properties": {         "location": {             "type": "string",             "description": "City name"         },         "units": {             "type": "string",             "enum": ["celsius", "fahrenheit"]         }     },     "required": ["location"] } async def get_weather(location: str, units: str = "celsius") -> Dict[str, Any]:     return {"temperature": 20, "units": units, "location": location} tool_manager.register_tool("get_weather", get_weather, weather_schema) function_caller = EnhancedFunctionCaller(tool_manager, client)\n```'}
2024-12-29 20:06:03,398 - DEBUG - Request options: {'method': 'post', 'url': '/deployments/o1-preview/chat/completions', 'headers': {'api-key': '<redacted>'}, 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': 'Please format your responses in Markdown.'}, {'role': 'user', 'content': 'Test'}, {'role': 'user', 'content': '```\nimport json from typing import List, Dict, Any from openai import AzureOpenAI  class ToolManager:     def __init__(self):         self.available_tools: Dict[str, callable] = {}         self.tool_schemas: Dict[str, Dict] = {}      def register_tool(self, name: str, func: callable, schema: Dict):         self.available_tools[name] = func         self.tool_schemas[name] = {             "type": "function",             "function": {                 "name": name,                 "description": schema.get("description", ""),                 "parameters": schema             }         }     def get_tool_definitions(self) -> List[Dict]:         return list(self.tool_schemas.values())     async def execute_tool(self, tool_call: Dict[str, Any]) -> Dict[str, Any]:         tool_name = tool_call.get("function", {}).get("name")         if tool_name not in self.available_tools:             raise ValueError(f"Unknown tool: {tool_name}")         arguments = json.loads(tool_call["function"]["arguments"])         tool_func = self.available_tools[tool_name]         return await tool_func(**arguments)  class EnhancedFunctionCaller:     def __init__(self, tool_manager: ToolManager, client: AzureOpenAI):         self.tool_manager = tool_manager         self.client = client     async def process_with_tools(self, messages: List[Dict[str, str]], tool_choice: str = "auto") -> Dict[str, Any]:         try:             response = await self.client.chat.completions.acreate(                 model="gpt-4",                 messages=messages,                 tools=self.tool_manager.get_tool_definitions(),                 tool_choice=tool_choice             )             if response.choices[0].message.tool_calls:                 tool_messages = [response.choices[0].message.dict()]                 for tool_call in response.choices[0].message.tool_calls:                     tool_result = await self.tool_manager.execute_tool(tool_call.dict())                     tool_messages.append({                         "role": "tool",                         "content": json.dumps(tool_result),                         "tool_call_id": tool_call.id                     })                 final_response = await self.client.chat.completions.acreate(                     model="gpt-4",                     messages=messages + tool_messages                 )                 return final_response.choices[0].message.dict()             return response.choices[0].message.dict()         except Exception as e:             print(f"Error in function calling: {str(e)}")             raise # Example usage tool_manager = ToolManager() weather_schema = {     "type": "object",     "properties": {         "location": {             "type": "string",             "description": "City name"         },         "units": {             "type": "string",             "enum": ["celsius", "fahrenheit"]         }     },     "required": ["location"] } async def get_weather(location: str, units: str = "celsius") -> Dict[str, Any]:     return {"temperature": 20, "units": units, "location": location} tool_manager.register_tool("get_weather", get_weather, weather_schema) function_caller = EnhancedFunctionCaller(tool_manager, client)\n```'}], 'model': 'o1-preview', 'max_completion_tokens': 500, 'response_format': {'type': 'text'}, 'temperature': 1}}
2024-12-29 20:06:03,399 - DEBUG - Sending HTTP Request: POST https://openai-hp.openai.azure.com/openai/deployments/o1-preview/chat/completions?api-version=2024-12-01-preview
2024-12-29 20:06:03,400 - DEBUG - close.started
2024-12-29 20:06:03,400 - DEBUG - close.complete
2024-12-29 20:06:03,401 - DEBUG - connect_tcp.started host='openai-hp.openai.azure.com' port=443 local_address=None timeout=5.0 socket_options=None
2024-12-29 20:06:03,509 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000025CEF671590>
2024-12-29 20:06:03,509 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x0000025CEF582570> server_hostname='openai-hp.openai.azure.com' timeout=5.0
2024-12-29 20:06:03,600 - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000025CEF633100>
2024-12-29 20:06:03,600 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2024-12-29 20:06:03,601 - DEBUG - send_request_headers.complete
2024-12-29 20:06:03,601 - DEBUG - send_request_body.started request=<Request [b'POST']>
2024-12-29 20:06:03,601 - DEBUG - send_request_body.complete
2024-12-29 20:06:03,602 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2024-12-29 20:06:08,187 - INFO -  * Detected change in 'C:\\Users\\htper\\OneDrive\\Folder\\chats\\chatapp\\app.py', reloading
2024-12-29 20:06:08,355 - INFO -  * Restarting with stat
2024-12-29 20:06:10,408 - WARNING -  * Debugger is active!
2024-12-29 20:06:10,419 - INFO -  * Debugger PIN: 995-560-430
2024-12-29 20:06:29,407 - INFO - 127.0.0.1 - - [29/Dec/2024 20:06:29] "[32mGET /chat_interface HTTP/1.1[0m" 302 -
2024-12-29 20:06:29,420 - INFO - 127.0.0.1 - - [29/Dec/2024 20:06:29] "GET /login?next=/chat_interface HTTP/1.1" 200 -
2024-12-29 20:06:33,768 - INFO - 127.0.0.1 - - [29/Dec/2024 20:06:33] "[32mPOST /login HTTP/1.1[0m" 302 -
2024-12-29 20:06:33,776 - INFO - 127.0.0.1 - - [29/Dec/2024 20:06:33] "GET /chat_interface HTTP/1.1" 200 -
2024-12-29 20:06:33,891 - INFO - 127.0.0.1 - - [29/Dec/2024 20:06:33] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-29 20:06:35,981 - DEBUG - Received chat message
2024-12-29 20:06:35,981 - DEBUG - Chat ID: c9e2ca02-7027-4a8f-93d9-e2621202da81
2024-12-29 20:06:35,981 - INFO - User message: Test
2024-12-29 20:06:35,982 - DEBUG - Sending request with 2 messages
2024-12-29 20:06:35,982 - DEBUG - Using deployment: o1-preview
2024-12-29 20:06:35,982 - DEBUG - Payload being sent to API: [{'role': 'user', 'content': 'Please format your responses in Markdown.'}, {'role': 'user', 'content': 'Test'}]
2024-12-29 20:06:35,983 - DEBUG - API Endpoint: https://openai-hp.openai.azure.com/
2024-12-29 20:06:35,983 - DEBUG - Model Deployment Name: o1-preview
2024-12-29 20:06:35,983 - DEBUG - Message 0: {'role': 'user', 'content': 'Please format your responses in Markdown.'}
2024-12-29 20:06:35,984 - DEBUG - Message 1: {'role': 'user', 'content': 'Test'}
2024-12-29 20:06:35,988 - DEBUG - Request options: {'method': 'post', 'url': '/deployments/o1-preview/chat/completions', 'headers': {'api-key': '<redacted>'}, 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': 'Please format your responses in Markdown.'}, {'role': 'user', 'content': 'Test'}], 'model': 'o1-preview', 'max_completion_tokens': 32000, 'response_format': {'type': 'text'}, 'temperature': 1}}
2024-12-29 20:06:36,072 - DEBUG - Sending HTTP Request: POST https://openai-hp.openai.azure.com/openai/deployments/o1-preview/chat/completions?api-version=2024-12-01-preview
2024-12-29 20:06:36,073 - DEBUG - connect_tcp.started host='openai-hp.openai.azure.com' port=443 local_address=None timeout=5.0 socket_options=None
2024-12-29 20:06:36,154 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002677AAE5D30>
2024-12-29 20:06:36,155 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x000002677AA1E570> server_hostname='openai-hp.openai.azure.com' timeout=5.0
2024-12-29 20:06:36,235 - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002677AA03B10>
2024-12-29 20:06:36,236 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2024-12-29 20:06:36,236 - DEBUG - send_request_headers.complete
2024-12-29 20:06:36,236 - DEBUG - send_request_body.started request=<Request [b'POST']>
2024-12-29 20:06:36,237 - DEBUG - send_request_body.complete
2024-12-29 20:06:36,237 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2024-12-29 20:06:53,054 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Length', b'574'), (b'Content-Type', b'application/json'), (b'apim-request-id', b'97c537b9-be17-447b-95b3-f0414c817d8a'), (b'Strict-Transport-Security', b'max-age=31536000; includeSubDomains; preload'), (b'x-content-type-options', b'nosniff'), (b'x-ratelimit-remaining-tokens', b'2367000'), (b'x-accel-buffering', b'no'), (b'x-ms-rai-invoked', b'true'), (b'x-request-id', b'4fb5a1fc-a391-4a29-b19f-211a59a82f6c'), (b'x-ms-region', b'East US 2'), (b'azureml-model-session', b'd087-20241219224417'), (b'x-ratelimit-remaining-requests', b'397'), (b'x-envoy-upstream-service-time', b'16660'), (b'x-ms-client-request-id', b'97c537b9-be17-447b-95b3-f0414c817d8a'), (b'Date', b'Mon, 30 Dec 2024 02:06:51 GMT')])
2024-12-29 20:06:53,055 - INFO - HTTP Request: POST https://openai-hp.openai.azure.com/openai/deployments/o1-preview/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2024-12-29 20:06:53,055 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2024-12-29 20:06:53,055 - DEBUG - receive_response_body.complete
2024-12-29 20:06:53,055 - DEBUG - response_closed.started
2024-12-29 20:06:53,056 - DEBUG - response_closed.complete
2024-12-29 20:06:53,056 - DEBUG - HTTP Response: POST https://openai-hp.openai.azure.com/openai/deployments/o1-preview/chat/completions?api-version=2024-12-01-preview "200 OK" Headers({'content-length': '574', 'content-type': 'application/json', 'apim-request-id': '97c537b9-be17-447b-95b3-f0414c817d8a', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'x-content-type-options': 'nosniff', 'x-ratelimit-remaining-tokens': '2367000', 'x-accel-buffering': 'no', 'x-ms-rai-invoked': 'true', 'x-request-id': '4fb5a1fc-a391-4a29-b19f-211a59a82f6c', 'x-ms-region': 'East US 2', 'azureml-model-session': 'd087-20241219224417', 'x-ratelimit-remaining-requests': '397', 'x-envoy-upstream-service-time': '16660', 'x-ms-client-request-id': '97c537b9-be17-447b-95b3-f0414c817d8a', 'date': 'Mon, 30 Dec 2024 02:06:51 GMT'})
2024-12-29 20:06:53,056 - DEBUG - request_id: 4fb5a1fc-a391-4a29-b19f-211a59a82f6c
2024-12-29 20:06:53,062 - INFO - Received response: <p><strong>Test successful!</strong> How can I assist you today?</p>
<div class="shortcuts"><button onclick="copyToClipboard()">Copy</button><button onclick="retryMessage()">Retry</button></div>
2024-12-29 20:06:53,062 - DEBUG - Raw API response: ChatCompletion(id='chatcmpl-AjzStrvl4XhsFBscn7inYpEdUV3KC', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='**Test successful!** How can I assist you today?', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None), content_filter_results={})], created=1735524395, model='o1-preview-2024-09-12', object='chat.completion', service_tier=None, system_fingerprint='fp_772cb6e46d', usage=CompletionUsage(completion_tokens=928, prompt_tokens=27, total_tokens=955, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=896, rejected_prediction_tokens=None), prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=0)), prompt_filter_results=[{'prompt_index': 0, 'content_filter_results': {}}])
2024-12-29 20:06:53,063 - DEBUG - Usage: CompletionUsage(completion_tokens=928, prompt_tokens=27, total_tokens=955, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=896, rejected_prediction_tokens=None), prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=0))
2024-12-29 20:06:53,063 - INFO - 127.0.0.1 - - [29/Dec/2024 20:06:53] "POST /chat HTTP/1.1" 200 -
2024-12-29 20:07:05,798 - DEBUG - Received file upload request
2024-12-29 20:07:05,799 - DEBUG - Processing uploaded files
2024-12-29 20:07:05,800 - DEBUG - Uploads directory ensured
2024-12-29 20:07:05,800 - INFO - File saved: uploads\Phase 2 AI Chat Enhancement.md
2024-12-29 20:07:05,801 - INFO - Files successfully uploaded: ['Phase 2 AI Chat Enhancement.md']
2024-12-29 20:07:05,802 - INFO - 127.0.0.1 - - [29/Dec/2024 20:07:05] "POST /upload HTTP/1.1" 200 -
2024-12-29 20:07:28,596 - DEBUG - Received chat message
2024-12-29 20:07:28,597 - DEBUG - Chat ID: c9e2ca02-7027-4a8f-93d9-e2621202da81
2024-12-29 20:07:28,597 - INFO - User message: Can you see this file?
2024-12-29 20:07:28,597 - DEBUG - Analyzing file: uploads\Phase 2 AI Chat Enhancement.md
2024-12-29 20:07:28,598 - DEBUG - Sending request with 4 messages
2024-12-29 20:07:28,598 - DEBUG - Using deployment: o1-preview
2024-12-29 20:07:28,598 - DEBUG - Payload being sent to API: [{'role': 'user', 'content': 'Please format your responses in Markdown.'}, {'role': 'user', 'content': 'Test'}, {'role': 'user', 'content': 'Can you see this file?'}, {'role': 'user', 'content': 'File content:\n# Phase 2: AI-Powered Chat - Enhancement \n\n## **Overview**\n\nPhase 2 focuses on enhancing the chat application with AI capabilities, now considering both project-based and independent conversations. The goal is to integrate an AI assistant to provide intelligent responses, real-time suggestions, and sentiment analysis. This phase builds upon the foundational components developed in Phase 1, adding a layer of intelligence to the chat experience. Key objectives include:\n\n-   **AI Assistant Integration:** Integrate an AI service (e.g., Claude API or OpenAI API) capable of handling different language models.\n-   **Contextual Responses:** Provide contextually relevant AI responses, including RAG for project-based chats and standard LLM responses for independent chats.\n-   **Real-time Suggestions:** Offer real-time suggestions as users type, taking into account project context or conversation history.\n-   **Sentiment Analysis:** Implement basic sentiment analysis of user messages.\n\nBy the end of Phase 2, the chat application will be able to engage in more intelligent and interactive conversations, setting the stage for advanced AI features in subsequent phases, while distinguishing between project based context and non project conversations.\n\n---\n\n## **Core Components Implementation Requirements**\n\n### **1. AI Assistant Integration**\n\n#### **Required Functionality**\n\n-   **API Integration:**\n    -   Integrate with an AI service (e.g., Claude API or OpenAI API), handling different language models.\n    -   Handle API requests and responses, including error handling and rate limiting.\n    -   **Error Handling:** Handle API errors gracefully, logging errors with relevant context and providing user-friendly feedback.\n    -   **Security:** Securely store API keys and manage access to the AI service.\n    -   **RAG and Non-RAG Support:** The AI assistant needs to be able to provide responses based on the project context (using RAG) if a `project_id` is provided, and to provide regular LLM-based responses if there is no `project_id`.\n        - If a `project_id` is given, use the language model set for that project, otherwise use a default language model.\n\n#### **Class Structure (Server-Side - Example)**\n\n```python\nclass AIAssistant:\n    def __init__(self, api_key):\n        self.api_key = api_key\n        # ... other initialization\n\n    def get_ai_response(self, message, conversation_history, project_id = None, language_model = \'gpt-3.5-turbo\') -> str:\n        """Gets a response from the AI assistant, taking into account project context or providing a normal response if there is no project_id.\n           If a project_id is given, the language model for the project should be used, else use the provided default\n        """\n        # ... logic to send API request and process response, using RAG context if project_id is not None\n        pass\n```\n\n**Additional Notes:**\n\n-   **Performance:** Optimize API calls for minimal latency. Consider caching responses where appropriate.\n-   **Context Management:** Maintain conversation history to provide context for the AI assistant, differentiating project-specific conversations from unassociated conversations.\n-   **Rate Limiting:** Implement strategies to handle API rate limits.\n\n---\n\n### **2. Real-time Suggestions**\n\n#### **Required Functionality**\n\n-   **Suggestion Generation:**\n    -   Generate real-time suggestions based on user input, using the AI assistant, taking into account project context or conversation history.\n    -   Display suggestions in the user interface.\n    -   **Error Handling:** Handle cases where suggestions cannot be generated (e.g., API errors).\n    -   **Performance:** Optimize suggestion generation for minimal latency.\n    -   If a `project_id` is given, use the RAG context to suggest responses, otherwise use the conversation history\n\n#### **Class Structure (Server-Side - Example)**\n\n```python\nclass SuggestionEngine:\n    def __init__(self, ai_assistant):\n        self.ai_assistant = ai_assistant\n\n    def get_suggestions(self, partial_message, conversation_history, project_id = None) -> list:\n        """Generates suggestions based on partial message, using RAG context if a project_id is passed, otherwise based on the conversation history."""\n        # ... logic to generate suggestions using the AI assistant, using RAG context if a project_id is passed\n        pass\n```\n\n**Additional Notes:**\n\n-   **User Interface Integration:** Integrate suggestions seamlessly into the chat UI.\n-   **Relevance:** Ensure suggestions are contextually relevant and helpful.\n-   **Filtering:** Implement filtering to avoid inappropriate or irrelevant suggestions.\n\n---\n\n### **3. Sentiment Analysis**\n\n#### **Required Functionality**\n\n-   **Message Analysis:**\n    - Use a sentiment analysis library or service to analyze user messages.\n    -   **Integration:** Integrate sentiment analysis into the AI response generation.\n    - **Performance:** Optimize sentiment analysis to avoid excessive processing.\n\n#### **Class Structure (Server-Side - Example)**\n\n```python\nclass SentimentAnalyzer:\n    def analyze_sentiment(self, message) -> float:\n        """Analyzes the sentiment of the message and returns a score"""\n        # ... logic to use an NLP library to determine the message\'s sentiment\n        pass\n```\n\n---\n\n### **Configuration System**\n\n#### **Environment Variables**\n\n-   `OPENAI_API_KEY`: The API key for the OpenAI API (or similar).\n-   `CLAUDE_API_KEY`: The API key for the Claude API (or similar).\n-   `SUGGESTION_ENGINE_ENABLED`: Flag to enable or disable the suggestion engine.\n-   `MAX_SUGGESTIONS`: The maximum number of suggestions to display.\n\n---\n\n### **Error Handling**\n\n*   Continue to refine error handling and logging for AI-related functionalities.\n\n---\n\n### **Conclusion**\n\nImplementing Phase 2 introduces AI capabilities into the chat application, while taking into account the support for project-based and independent conversations. By integrating an AI assistant that can provide responses with RAG when it is called for, providing real-time suggestions, and performing sentiment analysis, the chat experience becomes more intelligent and engaging. This enhanced foundation prepares the application for more advanced AI features and a refined user experience in subsequent phases.\n\nThat\'s Phase 2. Let me know when you\'re ready for Phase 3!\n'}]
2024-12-29 20:07:28,599 - DEBUG - API Endpoint: https://openai-hp.openai.azure.com/
2024-12-29 20:07:28,599 - DEBUG - Model Deployment Name: o1-preview
2024-12-29 20:07:28,600 - DEBUG - Message 0: {'role': 'user', 'content': 'Please format your responses in Markdown.'}
2024-12-29 20:07:28,600 - DEBUG - Message 1: {'role': 'user', 'content': 'Test'}
2024-12-29 20:07:28,600 - DEBUG - Message 2: {'role': 'user', 'content': 'Can you see this file?'}
2024-12-29 20:07:28,600 - DEBUG - Message 3: {'role': 'user', 'content': 'File content:\n# Phase 2: AI-Powered Chat - Enhancement \n\n## **Overview**\n\nPhase 2 focuses on enhancing the chat application with AI capabilities, now considering both project-based and independent conversations. The goal is to integrate an AI assistant to provide intelligent responses, real-time suggestions, and sentiment analysis. This phase builds upon the foundational components developed in Phase 1, adding a layer of intelligence to the chat experience. Key objectives include:\n\n-   **AI Assistant Integration:** Integrate an AI service (e.g., Claude API or OpenAI API) capable of handling different language models.\n-   **Contextual Responses:** Provide contextually relevant AI responses, including RAG for project-based chats and standard LLM responses for independent chats.\n-   **Real-time Suggestions:** Offer real-time suggestions as users type, taking into account project context or conversation history.\n-   **Sentiment Analysis:** Implement basic sentiment analysis of user messages.\n\nBy the end of Phase 2, the chat application will be able to engage in more intelligent and interactive conversations, setting the stage for advanced AI features in subsequent phases, while distinguishing between project based context and non project conversations.\n\n---\n\n## **Core Components Implementation Requirements**\n\n### **1. AI Assistant Integration**\n\n#### **Required Functionality**\n\n-   **API Integration:**\n    -   Integrate with an AI service (e.g., Claude API or OpenAI API), handling different language models.\n    -   Handle API requests and responses, including error handling and rate limiting.\n    -   **Error Handling:** Handle API errors gracefully, logging errors with relevant context and providing user-friendly feedback.\n    -   **Security:** Securely store API keys and manage access to the AI service.\n    -   **RAG and Non-RAG Support:** The AI assistant needs to be able to provide responses based on the project context (using RAG) if a `project_id` is provided, and to provide regular LLM-based responses if there is no `project_id`.\n        - If a `project_id` is given, use the language model set for that project, otherwise use a default language model.\n\n#### **Class Structure (Server-Side - Example)**\n\n```python\nclass AIAssistant:\n    def __init__(self, api_key):\n        self.api_key = api_key\n        # ... other initialization\n\n    def get_ai_response(self, message, conversation_history, project_id = None, language_model = \'gpt-3.5-turbo\') -> str:\n        """Gets a response from the AI assistant, taking into account project context or providing a normal response if there is no project_id.\n           If a project_id is given, the language model for the project should be used, else use the provided default\n        """\n        # ... logic to send API request and process response, using RAG context if project_id is not None\n        pass\n```\n\n**Additional Notes:**\n\n-   **Performance:** Optimize API calls for minimal latency. Consider caching responses where appropriate.\n-   **Context Management:** Maintain conversation history to provide context for the AI assistant, differentiating project-specific conversations from unassociated conversations.\n-   **Rate Limiting:** Implement strategies to handle API rate limits.\n\n---\n\n### **2. Real-time Suggestions**\n\n#### **Required Functionality**\n\n-   **Suggestion Generation:**\n    -   Generate real-time suggestions based on user input, using the AI assistant, taking into account project context or conversation history.\n    -   Display suggestions in the user interface.\n    -   **Error Handling:** Handle cases where suggestions cannot be generated (e.g., API errors).\n    -   **Performance:** Optimize suggestion generation for minimal latency.\n    -   If a `project_id` is given, use the RAG context to suggest responses, otherwise use the conversation history\n\n#### **Class Structure (Server-Side - Example)**\n\n```python\nclass SuggestionEngine:\n    def __init__(self, ai_assistant):\n        self.ai_assistant = ai_assistant\n\n    def get_suggestions(self, partial_message, conversation_history, project_id = None) -> list:\n        """Generates suggestions based on partial message, using RAG context if a project_id is passed, otherwise based on the conversation history."""\n        # ... logic to generate suggestions using the AI assistant, using RAG context if a project_id is passed\n        pass\n```\n\n**Additional Notes:**\n\n-   **User Interface Integration:** Integrate suggestions seamlessly into the chat UI.\n-   **Relevance:** Ensure suggestions are contextually relevant and helpful.\n-   **Filtering:** Implement filtering to avoid inappropriate or irrelevant suggestions.\n\n---\n\n### **3. Sentiment Analysis**\n\n#### **Required Functionality**\n\n-   **Message Analysis:**\n    - Use a sentiment analysis library or service to analyze user messages.\n    -   **Integration:** Integrate sentiment analysis into the AI response generation.\n    - **Performance:** Optimize sentiment analysis to avoid excessive processing.\n\n#### **Class Structure (Server-Side - Example)**\n\n```python\nclass SentimentAnalyzer:\n    def analyze_sentiment(self, message) -> float:\n        """Analyzes the sentiment of the message and returns a score"""\n        # ... logic to use an NLP library to determine the message\'s sentiment\n        pass\n```\n\n---\n\n### **Configuration System**\n\n#### **Environment Variables**\n\n-   `OPENAI_API_KEY`: The API key for the OpenAI API (or similar).\n-   `CLAUDE_API_KEY`: The API key for the Claude API (or similar).\n-   `SUGGESTION_ENGINE_ENABLED`: Flag to enable or disable the suggestion engine.\n-   `MAX_SUGGESTIONS`: The maximum number of suggestions to display.\n\n---\n\n### **Error Handling**\n\n*   Continue to refine error handling and logging for AI-related functionalities.\n\n---\n\n### **Conclusion**\n\nImplementing Phase 2 introduces AI capabilities into the chat application, while taking into account the support for project-based and independent conversations. By integrating an AI assistant that can provide responses with RAG when it is called for, providing real-time suggestions, and performing sentiment analysis, the chat experience becomes more intelligent and engaging. This enhanced foundation prepares the application for more advanced AI features and a refined user experience in subsequent phases.\n\nThat\'s Phase 2. Let me know when you\'re ready for Phase 3!\n'}
2024-12-29 20:07:28,605 - DEBUG - Request options: {'method': 'post', 'url': '/deployments/o1-preview/chat/completions', 'headers': {'api-key': '<redacted>'}, 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': 'Please format your responses in Markdown.'}, {'role': 'user', 'content': 'Test'}, {'role': 'user', 'content': 'Can you see this file?'}, {'role': 'user', 'content': 'File content:\n# Phase 2: AI-Powered Chat - Enhancement \n\n## **Overview**\n\nPhase 2 focuses on enhancing the chat application with AI capabilities, now considering both project-based and independent conversations. The goal is to integrate an AI assistant to provide intelligent responses, real-time suggestions, and sentiment analysis. This phase builds upon the foundational components developed in Phase 1, adding a layer of intelligence to the chat experience. Key objectives include:\n\n-   **AI Assistant Integration:** Integrate an AI service (e.g., Claude API or OpenAI API) capable of handling different language models.\n-   **Contextual Responses:** Provide contextually relevant AI responses, including RAG for project-based chats and standard LLM responses for independent chats.\n-   **Real-time Suggestions:** Offer real-time suggestions as users type, taking into account project context or conversation history.\n-   **Sentiment Analysis:** Implement basic sentiment analysis of user messages.\n\nBy the end of Phase 2, the chat application will be able to engage in more intelligent and interactive conversations, setting the stage for advanced AI features in subsequent phases, while distinguishing between project based context and non project conversations.\n\n---\n\n## **Core Components Implementation Requirements**\n\n### **1. AI Assistant Integration**\n\n#### **Required Functionality**\n\n-   **API Integration:**\n    -   Integrate with an AI service (e.g., Claude API or OpenAI API), handling different language models.\n    -   Handle API requests and responses, including error handling and rate limiting.\n    -   **Error Handling:** Handle API errors gracefully, logging errors with relevant context and providing user-friendly feedback.\n    -   **Security:** Securely store API keys and manage access to the AI service.\n    -   **RAG and Non-RAG Support:** The AI assistant needs to be able to provide responses based on the project context (using RAG) if a `project_id` is provided, and to provide regular LLM-based responses if there is no `project_id`.\n        - If a `project_id` is given, use the language model set for that project, otherwise use a default language model.\n\n#### **Class Structure (Server-Side - Example)**\n\n```python\nclass AIAssistant:\n    def __init__(self, api_key):\n        self.api_key = api_key\n        # ... other initialization\n\n    def get_ai_response(self, message, conversation_history, project_id = None, language_model = \'gpt-3.5-turbo\') -> str:\n        """Gets a response from the AI assistant, taking into account project context or providing a normal response if there is no project_id.\n           If a project_id is given, the language model for the project should be used, else use the provided default\n        """\n        # ... logic to send API request and process response, using RAG context if project_id is not None\n        pass\n```\n\n**Additional Notes:**\n\n-   **Performance:** Optimize API calls for minimal latency. Consider caching responses where appropriate.\n-   **Context Management:** Maintain conversation history to provide context for the AI assistant, differentiating project-specific conversations from unassociated conversations.\n-   **Rate Limiting:** Implement strategies to handle API rate limits.\n\n---\n\n### **2. Real-time Suggestions**\n\n#### **Required Functionality**\n\n-   **Suggestion Generation:**\n    -   Generate real-time suggestions based on user input, using the AI assistant, taking into account project context or conversation history.\n    -   Display suggestions in the user interface.\n    -   **Error Handling:** Handle cases where suggestions cannot be generated (e.g., API errors).\n    -   **Performance:** Optimize suggestion generation for minimal latency.\n    -   If a `project_id` is given, use the RAG context to suggest responses, otherwise use the conversation history\n\n#### **Class Structure (Server-Side - Example)**\n\n```python\nclass SuggestionEngine:\n    def __init__(self, ai_assistant):\n        self.ai_assistant = ai_assistant\n\n    def get_suggestions(self, partial_message, conversation_history, project_id = None) -> list:\n        """Generates suggestions based on partial message, using RAG context if a project_id is passed, otherwise based on the conversation history."""\n        # ... logic to generate suggestions using the AI assistant, using RAG context if a project_id is passed\n        pass\n```\n\n**Additional Notes:**\n\n-   **User Interface Integration:** Integrate suggestions seamlessly into the chat UI.\n-   **Relevance:** Ensure suggestions are contextually relevant and helpful.\n-   **Filtering:** Implement filtering to avoid inappropriate or irrelevant suggestions.\n\n---\n\n### **3. Sentiment Analysis**\n\n#### **Required Functionality**\n\n-   **Message Analysis:**\n    - Use a sentiment analysis library or service to analyze user messages.\n    -   **Integration:** Integrate sentiment analysis into the AI response generation.\n    - **Performance:** Optimize sentiment analysis to avoid excessive processing.\n\n#### **Class Structure (Server-Side - Example)**\n\n```python\nclass SentimentAnalyzer:\n    def analyze_sentiment(self, message) -> float:\n        """Analyzes the sentiment of the message and returns a score"""\n        # ... logic to use an NLP library to determine the message\'s sentiment\n        pass\n```\n\n---\n\n### **Configuration System**\n\n#### **Environment Variables**\n\n-   `OPENAI_API_KEY`: The API key for the OpenAI API (or similar).\n-   `CLAUDE_API_KEY`: The API key for the Claude API (or similar).\n-   `SUGGESTION_ENGINE_ENABLED`: Flag to enable or disable the suggestion engine.\n-   `MAX_SUGGESTIONS`: The maximum number of suggestions to display.\n\n---\n\n### **Error Handling**\n\n*   Continue to refine error handling and logging for AI-related functionalities.\n\n---\n\n### **Conclusion**\n\nImplementing Phase 2 introduces AI capabilities into the chat application, while taking into account the support for project-based and independent conversations. By integrating an AI assistant that can provide responses with RAG when it is called for, providing real-time suggestions, and performing sentiment analysis, the chat experience becomes more intelligent and engaging. This enhanced foundation prepares the application for more advanced AI features and a refined user experience in subsequent phases.\n\nThat\'s Phase 2. Let me know when you\'re ready for Phase 3!\n'}], 'model': 'o1-preview', 'max_completion_tokens': 32000, 'response_format': {'type': 'text'}, 'temperature': 1}}
2024-12-29 20:07:28,607 - DEBUG - Sending HTTP Request: POST https://openai-hp.openai.azure.com/openai/deployments/o1-preview/chat/completions?api-version=2024-12-01-preview
2024-12-29 20:07:28,607 - DEBUG - close.started
2024-12-29 20:07:28,608 - DEBUG - close.complete
2024-12-29 20:07:28,608 - DEBUG - connect_tcp.started host='openai-hp.openai.azure.com' port=443 local_address=None timeout=5.0 socket_options=None
2024-12-29 20:07:28,749 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002677AB10050>
2024-12-29 20:07:28,750 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x000002677AA1E570> server_hostname='openai-hp.openai.azure.com' timeout=5.0
2024-12-29 20:07:28,831 - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002677AAD2190>
2024-12-29 20:07:28,832 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2024-12-29 20:07:28,832 - DEBUG - send_request_headers.complete
2024-12-29 20:07:28,832 - DEBUG - send_request_body.started request=<Request [b'POST']>
2024-12-29 20:07:28,833 - DEBUG - send_request_body.complete
2024-12-29 20:07:28,833 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2024-12-29 20:07:42,577 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Length', b'632'), (b'Content-Type', b'application/json'), (b'apim-request-id', b'95df9f8e-482f-4d18-ada6-f26b6399473e'), (b'Strict-Transport-Security', b'max-age=31536000; includeSubDomains; preload'), (b'x-content-type-options', b'nosniff'), (b'x-ratelimit-remaining-tokens', b'2336000'), (b'x-accel-buffering', b'no'), (b'x-ms-rai-invoked', b'true'), (b'x-request-id', b'9652ef4d-aca7-4bc9-bdf5-d5b013845324'), (b'x-ms-region', b'East US 2'), (b'azureml-model-session', b'd075-20241219120032'), (b'x-ratelimit-remaining-requests', b'398'), (b'x-envoy-upstream-service-time', b'13666'), (b'x-ms-client-request-id', b'95df9f8e-482f-4d18-ada6-f26b6399473e'), (b'Date', b'Mon, 30 Dec 2024 02:07:40 GMT')])
2024-12-29 20:07:42,578 - INFO - HTTP Request: POST https://openai-hp.openai.azure.com/openai/deployments/o1-preview/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2024-12-29 20:07:42,578 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2024-12-29 20:07:42,579 - DEBUG - receive_response_body.complete
2024-12-29 20:07:42,579 - DEBUG - response_closed.started
2024-12-29 20:07:42,579 - DEBUG - response_closed.complete
2024-12-29 20:07:42,580 - DEBUG - HTTP Response: POST https://openai-hp.openai.azure.com/openai/deployments/o1-preview/chat/completions?api-version=2024-12-01-preview "200 OK" Headers({'content-length': '632', 'content-type': 'application/json', 'apim-request-id': '95df9f8e-482f-4d18-ada6-f26b6399473e', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'x-content-type-options': 'nosniff', 'x-ratelimit-remaining-tokens': '2336000', 'x-accel-buffering': 'no', 'x-ms-rai-invoked': 'true', 'x-request-id': '9652ef4d-aca7-4bc9-bdf5-d5b013845324', 'x-ms-region': 'East US 2', 'azureml-model-session': 'd075-20241219120032', 'x-ratelimit-remaining-requests': '398', 'x-envoy-upstream-service-time': '13666', 'x-ms-client-request-id': '95df9f8e-482f-4d18-ada6-f26b6399473e', 'date': 'Mon, 30 Dec 2024 02:07:40 GMT'})
2024-12-29 20:07:42,580 - DEBUG - request_id: 9652ef4d-aca7-4bc9-bdf5-d5b013845324
2024-12-29 20:07:42,581 - INFO - Received response: <p>Thank you for sharing the Phase 2 documentation. I'm ready to proceed to Phase 3 whenever you're ready!</p>
<div class="shortcuts"><button onclick="copyToClipboard()">Copy</button><button onclick="retryMessage()">Retry</button></div>
2024-12-29 20:07:42,581 - DEBUG - Raw API response: ChatCompletion(id='chatcmpl-AjzTj0GlVY4f3jzuIZIvRQrFvwxRk', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content="Thank you for sharing the Phase 2 documentation. I'm ready to proceed to Phase 3 whenever you're ready!", refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None), content_filter_results={})], created=1735524447, model='o1-preview-2024-09-12', object='chat.completion', service_tier=None, system_fingerprint='fp_772cb6e46d', usage=CompletionUsage(completion_tokens=610, prompt_tokens=1426, total_tokens=2036, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=576, rejected_prediction_tokens=None), prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=0)), prompt_filter_results=[{'prompt_index': 0, 'content_filter_results': {}}])
2024-12-29 20:07:42,581 - DEBUG - Usage: CompletionUsage(completion_tokens=610, prompt_tokens=1426, total_tokens=2036, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=576, rejected_prediction_tokens=None), prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=0))
2024-12-29 20:07:42,582 - INFO - 127.0.0.1 - - [29/Dec/2024 20:07:42] "POST /chat HTTP/1.1" 200 -
2024-12-29 20:07:56,758 - DEBUG - Received chat message
2024-12-29 20:07:56,758 - DEBUG - Chat ID: c9e2ca02-7027-4a8f-93d9-e2621202da81
2024-12-29 20:07:56,758 - INFO - User message: Show me the code you wrote for Phase 2
2024-12-29 20:07:56,759 - DEBUG - Analyzing file: uploads\Phase 2 AI Chat Enhancement.md
2024-12-29 20:07:56,760 - DEBUG - Sending request with 6 messages
2024-12-29 20:07:56,760 - DEBUG - Using deployment: o1-preview
2024-12-29 20:07:56,760 - DEBUG - Payload being sent to API: [{'role': 'user', 'content': 'Please format your responses in Markdown.'}, {'role': 'user', 'content': 'Test'}, {'role': 'user', 'content': 'Can you see this file?'}, {'role': 'user', 'content': 'File content:\n# Phase 2: AI-Powered Chat - Enhancement \n\n## **Overview**\n\nPhase 2 focuses on enhancing the chat application with AI capabilities, now considering both project-based and independent conversations. The goal is to integrate an AI assistant to provide intelligent responses, real-time suggestions, and sentiment analysis. This phase builds upon the foundational components developed in Phase 1, adding a layer of intelligence to the chat experience. Key objectives include:\n\n-   **AI Assistant Integration:** Integrate an AI service (e.g., Claude API or OpenAI API) capable of handling different language models.\n-   **Contextual Responses:** Provide contextually relevant AI responses, including RAG for project-based chats and standard LLM responses for independent chats.\n-   **Real-time Suggestions:** Offer real-time suggestions as users type, taking into account project context or conversation history.\n-   **Sentiment Analysis:** Implement basic sentiment analysis of user messages.\n\nBy the end of Phase 2, the chat application will be able to engage in more intelligent and interactive conversations, setting the stage for advanced AI features in subsequent phases, while distinguishing between project based context and non project conversations.\n\n---\n\n## **Core Components Implementation Requirements**\n\n### **1. AI Assistant Integration**\n\n#### **Required Functionality**\n\n-   **API Integration:**\n    -   Integrate with an AI service (e.g., Claude API or OpenAI API), handling different language models.\n    -   Handle API requests and responses, including error handling and rate limiting.\n    -   **Error Handling:** Handle API errors gracefully, logging errors with relevant context and providing user-friendly feedback.\n    -   **Security:** Securely store API keys and manage access to the AI service.\n    -   **RAG and Non-RAG Support:** The AI assistant needs to be able to provide responses based on the project context (using RAG) if a `project_id` is provided, and to provide regular LLM-based responses if there is no `project_id`.\n        - If a `project_id` is given, use the language model set for that project, otherwise use a default language model.\n\n#### **Class Structure (Server-Side - Example)**\n\n```python\nclass AIAssistant:\n    def __init__(self, api_key):\n        self.api_key = api_key\n        # ... other initialization\n\n    def get_ai_response(self, message, conversation_history, project_id = None, language_model = \'gpt-3.5-turbo\') -> str:\n        """Gets a response from the AI assistant, taking into account project context or providing a normal response if there is no project_id.\n           If a project_id is given, the language model for the project should be used, else use the provided default\n        """\n        # ... logic to send API request and process response, using RAG context if project_id is not None\n        pass\n```\n\n**Additional Notes:**\n\n-   **Performance:** Optimize API calls for minimal latency. Consider caching responses where appropriate.\n-   **Context Management:** Maintain conversation history to provide context for the AI assistant, differentiating project-specific conversations from unassociated conversations.\n-   **Rate Limiting:** Implement strategies to handle API rate limits.\n\n---\n\n### **2. Real-time Suggestions**\n\n#### **Required Functionality**\n\n-   **Suggestion Generation:**\n    -   Generate real-time suggestions based on user input, using the AI assistant, taking into account project context or conversation history.\n    -   Display suggestions in the user interface.\n    -   **Error Handling:** Handle cases where suggestions cannot be generated (e.g., API errors).\n    -   **Performance:** Optimize suggestion generation for minimal latency.\n    -   If a `project_id` is given, use the RAG context to suggest responses, otherwise use the conversation history\n\n#### **Class Structure (Server-Side - Example)**\n\n```python\nclass SuggestionEngine:\n    def __init__(self, ai_assistant):\n        self.ai_assistant = ai_assistant\n\n    def get_suggestions(self, partial_message, conversation_history, project_id = None) -> list:\n        """Generates suggestions based on partial message, using RAG context if a project_id is passed, otherwise based on the conversation history."""\n        # ... logic to generate suggestions using the AI assistant, using RAG context if a project_id is passed\n        pass\n```\n\n**Additional Notes:**\n\n-   **User Interface Integration:** Integrate suggestions seamlessly into the chat UI.\n-   **Relevance:** Ensure suggestions are contextually relevant and helpful.\n-   **Filtering:** Implement filtering to avoid inappropriate or irrelevant suggestions.\n\n---\n\n### **3. Sentiment Analysis**\n\n#### **Required Functionality**\n\n-   **Message Analysis:**\n    - Use a sentiment analysis library or service to analyze user messages.\n    -   **Integration:** Integrate sentiment analysis into the AI response generation.\n    - **Performance:** Optimize sentiment analysis to avoid excessive processing.\n\n#### **Class Structure (Server-Side - Example)**\n\n```python\nclass SentimentAnalyzer:\n    def analyze_sentiment(self, message) -> float:\n        """Analyzes the sentiment of the message and returns a score"""\n        # ... logic to use an NLP library to determine the message\'s sentiment\n        pass\n```\n\n---\n\n### **Configuration System**\n\n#### **Environment Variables**\n\n-   `OPENAI_API_KEY`: The API key for the OpenAI API (or similar).\n-   `CLAUDE_API_KEY`: The API key for the Claude API (or similar).\n-   `SUGGESTION_ENGINE_ENABLED`: Flag to enable or disable the suggestion engine.\n-   `MAX_SUGGESTIONS`: The maximum number of suggestions to display.\n\n---\n\n### **Error Handling**\n\n*   Continue to refine error handling and logging for AI-related functionalities.\n\n---\n\n### **Conclusion**\n\nImplementing Phase 2 introduces AI capabilities into the chat application, while taking into account the support for project-based and independent conversations. By integrating an AI assistant that can provide responses with RAG when it is called for, providing real-time suggestions, and performing sentiment analysis, the chat experience becomes more intelligent and engaging. This enhanced foundation prepares the application for more advanced AI features and a refined user experience in subsequent phases.\n\nThat\'s Phase 2. Let me know when you\'re ready for Phase 3!\n'}, {'role': 'user', 'content': 'Show me the code you wrote for Phase 2'}, {'role': 'user', 'content': 'File content:\n# Phase 2: AI-Powered Chat - Enhancement \n\n## **Overview**\n\nPhase 2 focuses on enhancing the chat application with AI capabilities, now considering both project-based and independent conversations. The goal is to integrate an AI assistant to provide intelligent responses, real-time suggestions, and sentiment analysis. This phase builds upon the foundational components developed in Phase 1, adding a layer of intelligence to the chat experience. Key objectives include:\n\n-   **AI Assistant Integration:** Integrate an AI service (e.g., Claude API or OpenAI API) capable of handling different language models.\n-   **Contextual Responses:** Provide contextually relevant AI responses, including RAG for project-based chats and standard LLM responses for independent chats.\n-   **Real-time Suggestions:** Offer real-time suggestions as users type, taking into account project context or conversation history.\n-   **Sentiment Analysis:** Implement basic sentiment analysis of user messages.\n\nBy the end of Phase 2, the chat application will be able to engage in more intelligent and interactive conversations, setting the stage for advanced AI features in subsequent phases, while distinguishing between project based context and non project conversations.\n\n---\n\n## **Core Components Implementation Requirements**\n\n### **1. AI Assistant Integration**\n\n#### **Required Functionality**\n\n-   **API Integration:**\n    -   Integrate with an AI service (e.g., Claude API or OpenAI API), handling different language models.\n    -   Handle API requests and responses, including error handling and rate limiting.\n    -   **Error Handling:** Handle API errors gracefully, logging errors with relevant context and providing user-friendly feedback.\n    -   **Security:** Securely store API keys and manage access to the AI service.\n    -   **RAG and Non-RAG Support:** The AI assistant needs to be able to provide responses based on the project context (using RAG) if a `project_id` is provided, and to provide regular LLM-based responses if there is no `project_id`.\n        - If a `project_id` is given, use the language model set for that project, otherwise use a default language model.\n\n#### **Class Structure (Server-Side - Example)**\n\n```python\nclass AIAssistant:\n    def __init__(self, api_key):\n        self.api_key = api_key\n        # ... other initialization\n\n    def get_ai_response(self, message, conversation_history, project_id = None, language_model = \'gpt-3.5-turbo\') -> str:\n        """Gets a response from the AI assistant, taking into account project context or providing a normal response if there is no project_id.\n           If a project_id is given, the language model for the project should be used, else use the provided default\n        """\n        # ... logic to send API request and process response, using RAG context if project_id is not None\n        pass\n```\n\n**Additional Notes:**\n\n-   **Performance:** Optimize API calls for minimal latency. Consider caching responses where appropriate.\n-   **Context Management:** Maintain conversation history to provide context for the AI assistant, differentiating project-specific conversations from unassociated conversations.\n-   **Rate Limiting:** Implement strategies to handle API rate limits.\n\n---\n\n### **2. Real-time Suggestions**\n\n#### **Required Functionality**\n\n-   **Suggestion Generation:**\n    -   Generate real-time suggestions based on user input, using the AI assistant, taking into account project context or conversation history.\n    -   Display suggestions in the user interface.\n    -   **Error Handling:** Handle cases where suggestions cannot be generated (e.g., API errors).\n    -   **Performance:** Optimize suggestion generation for minimal latency.\n    -   If a `project_id` is given, use the RAG context to suggest responses, otherwise use the conversation history\n\n#### **Class Structure (Server-Side - Example)**\n\n```python\nclass SuggestionEngine:\n    def __init__(self, ai_assistant):\n        self.ai_assistant = ai_assistant\n\n    def get_suggestions(self, partial_message, conversation_history, project_id = None) -> list:\n        """Generates suggestions based on partial message, using RAG context if a project_id is passed, otherwise based on the conversation history."""\n        # ... logic to generate suggestions using the AI assistant, using RAG context if a project_id is passed\n        pass\n```\n\n**Additional Notes:**\n\n-   **User Interface Integration:** Integrate suggestions seamlessly into the chat UI.\n-   **Relevance:** Ensure suggestions are contextually relevant and helpful.\n-   **Filtering:** Implement filtering to avoid inappropriate or irrelevant suggestions.\n\n---\n\n### **3. Sentiment Analysis**\n\n#### **Required Functionality**\n\n-   **Message Analysis:**\n    - Use a sentiment analysis library or service to analyze user messages.\n    -   **Integration:** Integrate sentiment analysis into the AI response generation.\n    - **Performance:** Optimize sentiment analysis to avoid excessive processing.\n\n#### **Class Structure (Server-Side - Example)**\n\n```python\nclass SentimentAnalyzer:\n    def analyze_sentiment(self, message) -> float:\n        """Analyzes the sentiment of the message and returns a score"""\n        # ... logic to use an NLP library to determine the message\'s sentiment\n        pass\n```\n\n---\n\n### **Configuration System**\n\n#### **Environment Variables**\n\n-   `OPENAI_API_KEY`: The API key for the OpenAI API (or similar).\n-   `CLAUDE_API_KEY`: The API key for the Claude API (or similar).\n-   `SUGGESTION_ENGINE_ENABLED`: Flag to enable or disable the suggestion engine.\n-   `MAX_SUGGESTIONS`: The maximum number of suggestions to display.\n\n---\n\n### **Error Handling**\n\n*   Continue to refine error handling and logging for AI-related functionalities.\n\n---\n\n### **Conclusion**\n\nImplementing Phase 2 introduces AI capabilities into the chat application, while taking into account the support for project-based and independent conversations. By integrating an AI assistant that can provide responses with RAG when it is called for, providing real-time suggestions, and performing sentiment analysis, the chat experience becomes more intelligent and engaging. This enhanced foundation prepares the application for more advanced AI features and a refined user experience in subsequent phases.\n\nThat\'s Phase 2. Let me know when you\'re ready for Phase 3!\n'}]
2024-12-29 20:07:56,762 - DEBUG - API Endpoint: https://openai-hp.openai.azure.com/
2024-12-29 20:07:56,762 - DEBUG - Model Deployment Name: o1-preview
2024-12-29 20:07:56,762 - DEBUG - Message 0: {'role': 'user', 'content': 'Please format your responses in Markdown.'}
2024-12-29 20:07:56,763 - DEBUG - Message 1: {'role': 'user', 'content': 'Test'}
2024-12-29 20:07:56,763 - DEBUG - Message 2: {'role': 'user', 'content': 'Can you see this file?'}
2024-12-29 20:07:56,763 - DEBUG - Message 3: {'role': 'user', 'content': 'File content:\n# Phase 2: AI-Powered Chat - Enhancement \n\n## **Overview**\n\nPhase 2 focuses on enhancing the chat application with AI capabilities, now considering both project-based and independent conversations. The goal is to integrate an AI assistant to provide intelligent responses, real-time suggestions, and sentiment analysis. This phase builds upon the foundational components developed in Phase 1, adding a layer of intelligence to the chat experience. Key objectives include:\n\n-   **AI Assistant Integration:** Integrate an AI service (e.g., Claude API or OpenAI API) capable of handling different language models.\n-   **Contextual Responses:** Provide contextually relevant AI responses, including RAG for project-based chats and standard LLM responses for independent chats.\n-   **Real-time Suggestions:** Offer real-time suggestions as users type, taking into account project context or conversation history.\n-   **Sentiment Analysis:** Implement basic sentiment analysis of user messages.\n\nBy the end of Phase 2, the chat application will be able to engage in more intelligent and interactive conversations, setting the stage for advanced AI features in subsequent phases, while distinguishing between project based context and non project conversations.\n\n---\n\n## **Core Components Implementation Requirements**\n\n### **1. AI Assistant Integration**\n\n#### **Required Functionality**\n\n-   **API Integration:**\n    -   Integrate with an AI service (e.g., Claude API or OpenAI API), handling different language models.\n    -   Handle API requests and responses, including error handling and rate limiting.\n    -   **Error Handling:** Handle API errors gracefully, logging errors with relevant context and providing user-friendly feedback.\n    -   **Security:** Securely store API keys and manage access to the AI service.\n    -   **RAG and Non-RAG Support:** The AI assistant needs to be able to provide responses based on the project context (using RAG) if a `project_id` is provided, and to provide regular LLM-based responses if there is no `project_id`.\n        - If a `project_id` is given, use the language model set for that project, otherwise use a default language model.\n\n#### **Class Structure (Server-Side - Example)**\n\n```python\nclass AIAssistant:\n    def __init__(self, api_key):\n        self.api_key = api_key\n        # ... other initialization\n\n    def get_ai_response(self, message, conversation_history, project_id = None, language_model = \'gpt-3.5-turbo\') -> str:\n        """Gets a response from the AI assistant, taking into account project context or providing a normal response if there is no project_id.\n           If a project_id is given, the language model for the project should be used, else use the provided default\n        """\n        # ... logic to send API request and process response, using RAG context if project_id is not None\n        pass\n```\n\n**Additional Notes:**\n\n-   **Performance:** Optimize API calls for minimal latency. Consider caching responses where appropriate.\n-   **Context Management:** Maintain conversation history to provide context for the AI assistant, differentiating project-specific conversations from unassociated conversations.\n-   **Rate Limiting:** Implement strategies to handle API rate limits.\n\n---\n\n### **2. Real-time Suggestions**\n\n#### **Required Functionality**\n\n-   **Suggestion Generation:**\n    -   Generate real-time suggestions based on user input, using the AI assistant, taking into account project context or conversation history.\n    -   Display suggestions in the user interface.\n    -   **Error Handling:** Handle cases where suggestions cannot be generated (e.g., API errors).\n    -   **Performance:** Optimize suggestion generation for minimal latency.\n    -   If a `project_id` is given, use the RAG context to suggest responses, otherwise use the conversation history\n\n#### **Class Structure (Server-Side - Example)**\n\n```python\nclass SuggestionEngine:\n    def __init__(self, ai_assistant):\n        self.ai_assistant = ai_assistant\n\n    def get_suggestions(self, partial_message, conversation_history, project_id = None) -> list:\n        """Generates suggestions based on partial message, using RAG context if a project_id is passed, otherwise based on the conversation history."""\n        # ... logic to generate suggestions using the AI assistant, using RAG context if a project_id is passed\n        pass\n```\n\n**Additional Notes:**\n\n-   **User Interface Integration:** Integrate suggestions seamlessly into the chat UI.\n-   **Relevance:** Ensure suggestions are contextually relevant and helpful.\n-   **Filtering:** Implement filtering to avoid inappropriate or irrelevant suggestions.\n\n---\n\n### **3. Sentiment Analysis**\n\n#### **Required Functionality**\n\n-   **Message Analysis:**\n    - Use a sentiment analysis library or service to analyze user messages.\n    -   **Integration:** Integrate sentiment analysis into the AI response generation.\n    - **Performance:** Optimize sentiment analysis to avoid excessive processing.\n\n#### **Class Structure (Server-Side - Example)**\n\n```python\nclass SentimentAnalyzer:\n    def analyze_sentiment(self, message) -> float:\n        """Analyzes the sentiment of the message and returns a score"""\n        # ... logic to use an NLP library to determine the message\'s sentiment\n        pass\n```\n\n---\n\n### **Configuration System**\n\n#### **Environment Variables**\n\n-   `OPENAI_API_KEY`: The API key for the OpenAI API (or similar).\n-   `CLAUDE_API_KEY`: The API key for the Claude API (or similar).\n-   `SUGGESTION_ENGINE_ENABLED`: Flag to enable or disable the suggestion engine.\n-   `MAX_SUGGESTIONS`: The maximum number of suggestions to display.\n\n---\n\n### **Error Handling**\n\n*   Continue to refine error handling and logging for AI-related functionalities.\n\n---\n\n### **Conclusion**\n\nImplementing Phase 2 introduces AI capabilities into the chat application, while taking into account the support for project-based and independent conversations. By integrating an AI assistant that can provide responses with RAG when it is called for, providing real-time suggestions, and performing sentiment analysis, the chat experience becomes more intelligent and engaging. This enhanced foundation prepares the application for more advanced AI features and a refined user experience in subsequent phases.\n\nThat\'s Phase 2. Let me know when you\'re ready for Phase 3!\n'}
2024-12-29 20:07:56,764 - DEBUG - Message 4: {'role': 'user', 'content': 'Show me the code you wrote for Phase 2'}
2024-12-29 20:07:56,764 - DEBUG - Message 5: {'role': 'user', 'content': 'File content:\n# Phase 2: AI-Powered Chat - Enhancement \n\n## **Overview**\n\nPhase 2 focuses on enhancing the chat application with AI capabilities, now considering both project-based and independent conversations. The goal is to integrate an AI assistant to provide intelligent responses, real-time suggestions, and sentiment analysis. This phase builds upon the foundational components developed in Phase 1, adding a layer of intelligence to the chat experience. Key objectives include:\n\n-   **AI Assistant Integration:** Integrate an AI service (e.g., Claude API or OpenAI API) capable of handling different language models.\n-   **Contextual Responses:** Provide contextually relevant AI responses, including RAG for project-based chats and standard LLM responses for independent chats.\n-   **Real-time Suggestions:** Offer real-time suggestions as users type, taking into account project context or conversation history.\n-   **Sentiment Analysis:** Implement basic sentiment analysis of user messages.\n\nBy the end of Phase 2, the chat application will be able to engage in more intelligent and interactive conversations, setting the stage for advanced AI features in subsequent phases, while distinguishing between project based context and non project conversations.\n\n---\n\n## **Core Components Implementation Requirements**\n\n### **1. AI Assistant Integration**\n\n#### **Required Functionality**\n\n-   **API Integration:**\n    -   Integrate with an AI service (e.g., Claude API or OpenAI API), handling different language models.\n    -   Handle API requests and responses, including error handling and rate limiting.\n    -   **Error Handling:** Handle API errors gracefully, logging errors with relevant context and providing user-friendly feedback.\n    -   **Security:** Securely store API keys and manage access to the AI service.\n    -   **RAG and Non-RAG Support:** The AI assistant needs to be able to provide responses based on the project context (using RAG) if a `project_id` is provided, and to provide regular LLM-based responses if there is no `project_id`.\n        - If a `project_id` is given, use the language model set for that project, otherwise use a default language model.\n\n#### **Class Structure (Server-Side - Example)**\n\n```python\nclass AIAssistant:\n    def __init__(self, api_key):\n        self.api_key = api_key\n        # ... other initialization\n\n    def get_ai_response(self, message, conversation_history, project_id = None, language_model = \'gpt-3.5-turbo\') -> str:\n        """Gets a response from the AI assistant, taking into account project context or providing a normal response if there is no project_id.\n           If a project_id is given, the language model for the project should be used, else use the provided default\n        """\n        # ... logic to send API request and process response, using RAG context if project_id is not None\n        pass\n```\n\n**Additional Notes:**\n\n-   **Performance:** Optimize API calls for minimal latency. Consider caching responses where appropriate.\n-   **Context Management:** Maintain conversation history to provide context for the AI assistant, differentiating project-specific conversations from unassociated conversations.\n-   **Rate Limiting:** Implement strategies to handle API rate limits.\n\n---\n\n### **2. Real-time Suggestions**\n\n#### **Required Functionality**\n\n-   **Suggestion Generation:**\n    -   Generate real-time suggestions based on user input, using the AI assistant, taking into account project context or conversation history.\n    -   Display suggestions in the user interface.\n    -   **Error Handling:** Handle cases where suggestions cannot be generated (e.g., API errors).\n    -   **Performance:** Optimize suggestion generation for minimal latency.\n    -   If a `project_id` is given, use the RAG context to suggest responses, otherwise use the conversation history\n\n#### **Class Structure (Server-Side - Example)**\n\n```python\nclass SuggestionEngine:\n    def __init__(self, ai_assistant):\n        self.ai_assistant = ai_assistant\n\n    def get_suggestions(self, partial_message, conversation_history, project_id = None) -> list:\n        """Generates suggestions based on partial message, using RAG context if a project_id is passed, otherwise based on the conversation history."""\n        # ... logic to generate suggestions using the AI assistant, using RAG context if a project_id is passed\n        pass\n```\n\n**Additional Notes:**\n\n-   **User Interface Integration:** Integrate suggestions seamlessly into the chat UI.\n-   **Relevance:** Ensure suggestions are contextually relevant and helpful.\n-   **Filtering:** Implement filtering to avoid inappropriate or irrelevant suggestions.\n\n---\n\n### **3. Sentiment Analysis**\n\n#### **Required Functionality**\n\n-   **Message Analysis:**\n    - Use a sentiment analysis library or service to analyze user messages.\n    -   **Integration:** Integrate sentiment analysis into the AI response generation.\n    - **Performance:** Optimize sentiment analysis to avoid excessive processing.\n\n#### **Class Structure (Server-Side - Example)**\n\n```python\nclass SentimentAnalyzer:\n    def analyze_sentiment(self, message) -> float:\n        """Analyzes the sentiment of the message and returns a score"""\n        # ... logic to use an NLP library to determine the message\'s sentiment\n        pass\n```\n\n---\n\n### **Configuration System**\n\n#### **Environment Variables**\n\n-   `OPENAI_API_KEY`: The API key for the OpenAI API (or similar).\n-   `CLAUDE_API_KEY`: The API key for the Claude API (or similar).\n-   `SUGGESTION_ENGINE_ENABLED`: Flag to enable or disable the suggestion engine.\n-   `MAX_SUGGESTIONS`: The maximum number of suggestions to display.\n\n---\n\n### **Error Handling**\n\n*   Continue to refine error handling and logging for AI-related functionalities.\n\n---\n\n### **Conclusion**\n\nImplementing Phase 2 introduces AI capabilities into the chat application, while taking into account the support for project-based and independent conversations. By integrating an AI assistant that can provide responses with RAG when it is called for, providing real-time suggestions, and performing sentiment analysis, the chat experience becomes more intelligent and engaging. This enhanced foundation prepares the application for more advanced AI features and a refined user experience in subsequent phases.\n\nThat\'s Phase 2. Let me know when you\'re ready for Phase 3!\n'}
2024-12-29 20:07:56,772 - DEBUG - Request options: {'method': 'post', 'url': '/deployments/o1-preview/chat/completions', 'headers': {'api-key': '<redacted>'}, 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': 'Please format your responses in Markdown.'}, {'role': 'user', 'content': 'Test'}, {'role': 'user', 'content': 'Can you see this file?'}, {'role': 'user', 'content': 'File content:\n# Phase 2: AI-Powered Chat - Enhancement \n\n## **Overview**\n\nPhase 2 focuses on enhancing the chat application with AI capabilities, now considering both project-based and independent conversations. The goal is to integrate an AI assistant to provide intelligent responses, real-time suggestions, and sentiment analysis. This phase builds upon the foundational components developed in Phase 1, adding a layer of intelligence to the chat experience. Key objectives include:\n\n-   **AI Assistant Integration:** Integrate an AI service (e.g., Claude API or OpenAI API) capable of handling different language models.\n-   **Contextual Responses:** Provide contextually relevant AI responses, including RAG for project-based chats and standard LLM responses for independent chats.\n-   **Real-time Suggestions:** Offer real-time suggestions as users type, taking into account project context or conversation history.\n-   **Sentiment Analysis:** Implement basic sentiment analysis of user messages.\n\nBy the end of Phase 2, the chat application will be able to engage in more intelligent and interactive conversations, setting the stage for advanced AI features in subsequent phases, while distinguishing between project based context and non project conversations.\n\n---\n\n## **Core Components Implementation Requirements**\n\n### **1. AI Assistant Integration**\n\n#### **Required Functionality**\n\n-   **API Integration:**\n    -   Integrate with an AI service (e.g., Claude API or OpenAI API), handling different language models.\n    -   Handle API requests and responses, including error handling and rate limiting.\n    -   **Error Handling:** Handle API errors gracefully, logging errors with relevant context and providing user-friendly feedback.\n    -   **Security:** Securely store API keys and manage access to the AI service.\n    -   **RAG and Non-RAG Support:** The AI assistant needs to be able to provide responses based on the project context (using RAG) if a `project_id` is provided, and to provide regular LLM-based responses if there is no `project_id`.\n        - If a `project_id` is given, use the language model set for that project, otherwise use a default language model.\n\n#### **Class Structure (Server-Side - Example)**\n\n```python\nclass AIAssistant:\n    def __init__(self, api_key):\n        self.api_key = api_key\n        # ... other initialization\n\n    def get_ai_response(self, message, conversation_history, project_id = None, language_model = \'gpt-3.5-turbo\') -> str:\n        """Gets a response from the AI assistant, taking into account project context or providing a normal response if there is no project_id.\n           If a project_id is given, the language model for the project should be used, else use the provided default\n        """\n        # ... logic to send API request and process response, using RAG context if project_id is not None\n        pass\n```\n\n**Additional Notes:**\n\n-   **Performance:** Optimize API calls for minimal latency. Consider caching responses where appropriate.\n-   **Context Management:** Maintain conversation history to provide context for the AI assistant, differentiating project-specific conversations from unassociated conversations.\n-   **Rate Limiting:** Implement strategies to handle API rate limits.\n\n---\n\n### **2. Real-time Suggestions**\n\n#### **Required Functionality**\n\n-   **Suggestion Generation:**\n    -   Generate real-time suggestions based on user input, using the AI assistant, taking into account project context or conversation history.\n    -   Display suggestions in the user interface.\n    -   **Error Handling:** Handle cases where suggestions cannot be generated (e.g., API errors).\n    -   **Performance:** Optimize suggestion generation for minimal latency.\n    -   If a `project_id` is given, use the RAG context to suggest responses, otherwise use the conversation history\n\n#### **Class Structure (Server-Side - Example)**\n\n```python\nclass SuggestionEngine:\n    def __init__(self, ai_assistant):\n        self.ai_assistant = ai_assistant\n\n    def get_suggestions(self, partial_message, conversation_history, project_id = None) -> list:\n        """Generates suggestions based on partial message, using RAG context if a project_id is passed, otherwise based on the conversation history."""\n        # ... logic to generate suggestions using the AI assistant, using RAG context if a project_id is passed\n        pass\n```\n\n**Additional Notes:**\n\n-   **User Interface Integration:** Integrate suggestions seamlessly into the chat UI.\n-   **Relevance:** Ensure suggestions are contextually relevant and helpful.\n-   **Filtering:** Implement filtering to avoid inappropriate or irrelevant suggestions.\n\n---\n\n### **3. Sentiment Analysis**\n\n#### **Required Functionality**\n\n-   **Message Analysis:**\n    - Use a sentiment analysis library or service to analyze user messages.\n    -   **Integration:** Integrate sentiment analysis into the AI response generation.\n    - **Performance:** Optimize sentiment analysis to avoid excessive processing.\n\n#### **Class Structure (Server-Side - Example)**\n\n```python\nclass SentimentAnalyzer:\n    def analyze_sentiment(self, message) -> float:\n        """Analyzes the sentiment of the message and returns a score"""\n        # ... logic to use an NLP library to determine the message\'s sentiment\n        pass\n```\n\n---\n\n### **Configuration System**\n\n#### **Environment Variables**\n\n-   `OPENAI_API_KEY`: The API key for the OpenAI API (or similar).\n-   `CLAUDE_API_KEY`: The API key for the Claude API (or similar).\n-   `SUGGESTION_ENGINE_ENABLED`: Flag to enable or disable the suggestion engine.\n-   `MAX_SUGGESTIONS`: The maximum number of suggestions to display.\n\n---\n\n### **Error Handling**\n\n*   Continue to refine error handling and logging for AI-related functionalities.\n\n---\n\n### **Conclusion**\n\nImplementing Phase 2 introduces AI capabilities into the chat application, while taking into account the support for project-based and independent conversations. By integrating an AI assistant that can provide responses with RAG when it is called for, providing real-time suggestions, and performing sentiment analysis, the chat experience becomes more intelligent and engaging. This enhanced foundation prepares the application for more advanced AI features and a refined user experience in subsequent phases.\n\nThat\'s Phase 2. Let me know when you\'re ready for Phase 3!\n'}, {'role': 'user', 'content': 'Show me the code you wrote for Phase 2'}, {'role': 'user', 'content': 'File content:\n# Phase 2: AI-Powered Chat - Enhancement \n\n## **Overview**\n\nPhase 2 focuses on enhancing the chat application with AI capabilities, now considering both project-based and independent conversations. The goal is to integrate an AI assistant to provide intelligent responses, real-time suggestions, and sentiment analysis. This phase builds upon the foundational components developed in Phase 1, adding a layer of intelligence to the chat experience. Key objectives include:\n\n-   **AI Assistant Integration:** Integrate an AI service (e.g., Claude API or OpenAI API) capable of handling different language models.\n-   **Contextual Responses:** Provide contextually relevant AI responses, including RAG for project-based chats and standard LLM responses for independent chats.\n-   **Real-time Suggestions:** Offer real-time suggestions as users type, taking into account project context or conversation history.\n-   **Sentiment Analysis:** Implement basic sentiment analysis of user messages.\n\nBy the end of Phase 2, the chat application will be able to engage in more intelligent and interactive conversations, setting the stage for advanced AI features in subsequent phases, while distinguishing between project based context and non project conversations.\n\n---\n\n## **Core Components Implementation Requirements**\n\n### **1. AI Assistant Integration**\n\n#### **Required Functionality**\n\n-   **API Integration:**\n    -   Integrate with an AI service (e.g., Claude API or OpenAI API), handling different language models.\n    -   Handle API requests and responses, including error handling and rate limiting.\n    -   **Error Handling:** Handle API errors gracefully, logging errors with relevant context and providing user-friendly feedback.\n    -   **Security:** Securely store API keys and manage access to the AI service.\n    -   **RAG and Non-RAG Support:** The AI assistant needs to be able to provide responses based on the project context (using RAG) if a `project_id` is provided, and to provide regular LLM-based responses if there is no `project_id`.\n        - If a `project_id` is given, use the language model set for that project, otherwise use a default language model.\n\n#### **Class Structure (Server-Side - Example)**\n\n```python\nclass AIAssistant:\n    def __init__(self, api_key):\n        self.api_key = api_key\n        # ... other initialization\n\n    def get_ai_response(self, message, conversation_history, project_id = None, language_model = \'gpt-3.5-turbo\') -> str:\n        """Gets a response from the AI assistant, taking into account project context or providing a normal response if there is no project_id.\n           If a project_id is given, the language model for the project should be used, else use the provided default\n        """\n        # ... logic to send API request and process response, using RAG context if project_id is not None\n        pass\n```\n\n**Additional Notes:**\n\n-   **Performance:** Optimize API calls for minimal latency. Consider caching responses where appropriate.\n-   **Context Management:** Maintain conversation history to provide context for the AI assistant, differentiating project-specific conversations from unassociated conversations.\n-   **Rate Limiting:** Implement strategies to handle API rate limits.\n\n---\n\n### **2. Real-time Suggestions**\n\n#### **Required Functionality**\n\n-   **Suggestion Generation:**\n    -   Generate real-time suggestions based on user input, using the AI assistant, taking into account project context or conversation history.\n    -   Display suggestions in the user interface.\n    -   **Error Handling:** Handle cases where suggestions cannot be generated (e.g., API errors).\n    -   **Performance:** Optimize suggestion generation for minimal latency.\n    -   If a `project_id` is given, use the RAG context to suggest responses, otherwise use the conversation history\n\n#### **Class Structure (Server-Side - Example)**\n\n```python\nclass SuggestionEngine:\n    def __init__(self, ai_assistant):\n        self.ai_assistant = ai_assistant\n\n    def get_suggestions(self, partial_message, conversation_history, project_id = None) -> list:\n        """Generates suggestions based on partial message, using RAG context if a project_id is passed, otherwise based on the conversation history."""\n        # ... logic to generate suggestions using the AI assistant, using RAG context if a project_id is passed\n        pass\n```\n\n**Additional Notes:**\n\n-   **User Interface Integration:** Integrate suggestions seamlessly into the chat UI.\n-   **Relevance:** Ensure suggestions are contextually relevant and helpful.\n-   **Filtering:** Implement filtering to avoid inappropriate or irrelevant suggestions.\n\n---\n\n### **3. Sentiment Analysis**\n\n#### **Required Functionality**\n\n-   **Message Analysis:**\n    - Use a sentiment analysis library or service to analyze user messages.\n    -   **Integration:** Integrate sentiment analysis into the AI response generation.\n    - **Performance:** Optimize sentiment analysis to avoid excessive processing.\n\n#### **Class Structure (Server-Side - Example)**\n\n```python\nclass SentimentAnalyzer:\n    def analyze_sentiment(self, message) -> float:\n        """Analyzes the sentiment of the message and returns a score"""\n        # ... logic to use an NLP library to determine the message\'s sentiment\n        pass\n```\n\n---\n\n### **Configuration System**\n\n#### **Environment Variables**\n\n-   `OPENAI_API_KEY`: The API key for the OpenAI API (or similar).\n-   `CLAUDE_API_KEY`: The API key for the Claude API (or similar).\n-   `SUGGESTION_ENGINE_ENABLED`: Flag to enable or disable the suggestion engine.\n-   `MAX_SUGGESTIONS`: The maximum number of suggestions to display.\n\n---\n\n### **Error Handling**\n\n*   Continue to refine error handling and logging for AI-related functionalities.\n\n---\n\n### **Conclusion**\n\nImplementing Phase 2 introduces AI capabilities into the chat application, while taking into account the support for project-based and independent conversations. By integrating an AI assistant that can provide responses with RAG when it is called for, providing real-time suggestions, and performing sentiment analysis, the chat experience becomes more intelligent and engaging. This enhanced foundation prepares the application for more advanced AI features and a refined user experience in subsequent phases.\n\nThat\'s Phase 2. Let me know when you\'re ready for Phase 3!\n'}], 'model': 'o1-preview', 'max_completion_tokens': 32000, 'response_format': {'type': 'text'}, 'temperature': 1}}
2024-12-29 20:07:56,774 - DEBUG - Sending HTTP Request: POST https://openai-hp.openai.azure.com/openai/deployments/o1-preview/chat/completions?api-version=2024-12-01-preview
2024-12-29 20:07:56,775 - DEBUG - close.started
2024-12-29 20:07:56,775 - DEBUG - close.complete
2024-12-29 20:07:56,775 - DEBUG - connect_tcp.started host='openai-hp.openai.azure.com' port=443 local_address=None timeout=5.0 socket_options=None
2024-12-29 20:07:56,865 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002677AAD2060>
2024-12-29 20:07:56,865 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x000002677AA1E570> server_hostname='openai-hp.openai.azure.com' timeout=5.0
2024-12-29 20:07:56,935 - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002677AAF96D0>
2024-12-29 20:07:56,935 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2024-12-29 20:07:56,936 - DEBUG - send_request_headers.complete
2024-12-29 20:07:56,936 - DEBUG - send_request_body.started request=<Request [b'POST']>
2024-12-29 20:07:56,936 - DEBUG - send_request_body.complete
2024-12-29 20:07:56,937 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2024-12-29 20:09:46,113 - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-12-29 20:09:46,114 - INFO - [33mPress CTRL+C to quit[0m
2024-12-29 20:09:46,115 - INFO -  * Restarting with stat
2024-12-29 20:09:47,975 - WARNING -  * Debugger is active!
2024-12-29 20:09:47,983 - INFO -  * Debugger PIN: 995-560-430
2024-12-29 20:09:48,710 - INFO - 127.0.0.1 - - [29/Dec/2024 20:09:48] "[32mGET /chat_interface HTTP/1.1[0m" 302 -
2024-12-29 20:09:48,730 - INFO - 127.0.0.1 - - [29/Dec/2024 20:09:48] "GET /login?next=/chat_interface HTTP/1.1" 200 -
2024-12-29 20:09:52,780 - INFO - 127.0.0.1 - - [29/Dec/2024 20:09:52] "[32mPOST /login HTTP/1.1[0m" 302 -
2024-12-29 20:09:52,789 - INFO - 127.0.0.1 - - [29/Dec/2024 20:09:52] "GET /chat_interface HTTP/1.1" 200 -
2024-12-29 20:09:52,902 - INFO - 127.0.0.1 - - [29/Dec/2024 20:09:52] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-29 20:09:55,726 - DEBUG - Received chat message
2024-12-29 20:09:55,727 - DEBUG - Chat ID: 48d9ca5b-1804-410a-a52f-bb9bf9e1ca81
2024-12-29 20:09:55,727 - INFO - User message: Test
2024-12-29 20:09:55,727 - DEBUG - Sending request with 2 messages
2024-12-29 20:09:55,727 - DEBUG - Using deployment: o1-preview
2024-12-29 20:09:55,728 - DEBUG - Payload being sent to API: [{'role': 'user', 'content': 'Please format your responses in Markdown.'}, {'role': 'user', 'content': 'Test'}]
2024-12-29 20:09:55,728 - DEBUG - API Endpoint: https://openai-hp.openai.azure.com/
2024-12-29 20:09:55,728 - DEBUG - Model Deployment Name: o1-preview
2024-12-29 20:09:55,728 - DEBUG - Message 0: {'role': 'user', 'content': 'Please format your responses in Markdown.'}
2024-12-29 20:09:55,728 - DEBUG - Message 1: {'role': 'user', 'content': 'Test'}
2024-12-29 20:09:55,733 - DEBUG - Request options: {'method': 'post', 'url': '/deployments/o1-preview/chat/completions', 'headers': {'api-key': '<redacted>'}, 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': 'Please format your responses in Markdown.'}, {'role': 'user', 'content': 'Test'}], 'model': 'o1-preview', 'max_completion_tokens': 32000, 'response_format': {'type': 'text'}, 'temperature': 1}}
2024-12-29 20:09:55,818 - DEBUG - Sending HTTP Request: POST https://openai-hp.openai.azure.com/openai/deployments/o1-preview/chat/completions?api-version=2024-12-01-preview
2024-12-29 20:09:55,819 - DEBUG - connect_tcp.started host='openai-hp.openai.azure.com' port=443 local_address=None timeout=5.0 socket_options=None
2024-12-29 20:09:55,889 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002E1195E5A90>
2024-12-29 20:09:55,890 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x000002E1195227B0> server_hostname='openai-hp.openai.azure.com' timeout=5.0
2024-12-29 20:09:55,966 - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002E1195039D0>
2024-12-29 20:09:55,967 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2024-12-29 20:09:55,967 - DEBUG - send_request_headers.complete
2024-12-29 20:09:55,968 - DEBUG - send_request_body.started request=<Request [b'POST']>
2024-12-29 20:09:55,968 - DEBUG - send_request_body.complete
2024-12-29 20:09:55,968 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2024-12-29 20:10:08,602 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Length', b'631'), (b'Content-Type', b'application/json'), (b'apim-request-id', b'4b30ddbf-9a6c-4337-bc08-b40fb410ba50'), (b'Strict-Transport-Security', b'max-age=31536000; includeSubDomains; preload'), (b'x-content-type-options', b'nosniff'), (b'x-ratelimit-remaining-tokens', b'2368000'), (b'x-accel-buffering', b'no'), (b'x-ms-rai-invoked', b'true'), (b'x-request-id', b'276a6676-564c-4a1f-bd81-ff54e22cfa7d'), (b'x-ms-region', b'East US 2'), (b'azureml-model-session', b'd091-20241220021933'), (b'x-ratelimit-remaining-requests', b'399'), (b'x-envoy-upstream-service-time', b'12561'), (b'x-ms-client-request-id', b'4b30ddbf-9a6c-4337-bc08-b40fb410ba50'), (b'Date', b'Mon, 30 Dec 2024 02:10:06 GMT')])
2024-12-29 20:10:08,603 - INFO - HTTP Request: POST https://openai-hp.openai.azure.com/openai/deployments/o1-preview/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2024-12-29 20:10:08,603 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2024-12-29 20:10:08,603 - DEBUG - receive_response_body.complete
2024-12-29 20:10:08,604 - DEBUG - response_closed.started
2024-12-29 20:10:08,604 - DEBUG - response_closed.complete
2024-12-29 20:10:08,604 - DEBUG - HTTP Response: POST https://openai-hp.openai.azure.com/openai/deployments/o1-preview/chat/completions?api-version=2024-12-01-preview "200 OK" Headers({'content-length': '631', 'content-type': 'application/json', 'apim-request-id': '4b30ddbf-9a6c-4337-bc08-b40fb410ba50', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'x-content-type-options': 'nosniff', 'x-ratelimit-remaining-tokens': '2368000', 'x-accel-buffering': 'no', 'x-ms-rai-invoked': 'true', 'x-request-id': '276a6676-564c-4a1f-bd81-ff54e22cfa7d', 'x-ms-region': 'East US 2', 'azureml-model-session': 'd091-20241220021933', 'x-ratelimit-remaining-requests': '399', 'x-envoy-upstream-service-time': '12561', 'x-ms-client-request-id': '4b30ddbf-9a6c-4337-bc08-b40fb410ba50', 'date': 'Mon, 30 Dec 2024 02:10:06 GMT'})
2024-12-29 20:10:08,604 - DEBUG - request_id: 276a6676-564c-4a1f-bd81-ff54e22cfa7d
2024-12-29 20:10:08,609 - INFO - Received response: <p><strong>Test successful!</strong></p>

<p>I'll format all my responses in Markdown from now on. How can I assist you today?</p>
<div class="shortcuts"><button onclick="copyToClipboard()">Copy</button><button onclick="retryMessage()">Retry</button></div>
2024-12-29 20:10:08,610 - DEBUG - Response being sent to chat interface: <p><strong>Test successful!</strong></p>

<p>I'll format all my responses in Markdown from now on. How can I assist you today?</p>
<div class="shortcuts"><button onclick="copyToClipboard()">Copy</button><button onclick="retryMessage()">Retry</button></div>
2024-12-29 20:10:08,610 - DEBUG - Raw API response: ChatCompletion(id='chatcmpl-AjzW6pgRC9EPySadRbcXIzXYI0FDz', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content="**Test successful!**\n\nI'll format all my responses in Markdown from now on. How can I assist you today?", refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None), content_filter_results={})], created=1735524594, model='o1-preview-2024-09-12', object='chat.completion', service_tier=None, system_fingerprint='fp_772cb6e46d', usage=CompletionUsage(completion_tokens=822, prompt_tokens=27, total_tokens=849, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=768, rejected_prediction_tokens=None), prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=0)), prompt_filter_results=[{'prompt_index': 0, 'content_filter_results': {}}])
2024-12-29 20:10:08,610 - DEBUG - Usage: CompletionUsage(completion_tokens=822, prompt_tokens=27, total_tokens=849, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=768, rejected_prediction_tokens=None), prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=0))
2024-12-29 20:10:08,611 - INFO - 127.0.0.1 - - [29/Dec/2024 20:10:08] "POST /chat HTTP/1.1" 200 -
2024-12-29 20:10:25,202 - DEBUG - Received chat message
2024-12-29 20:10:25,202 - DEBUG - Chat ID: 48d9ca5b-1804-410a-a52f-bb9bf9e1ca81
2024-12-29 20:10:25,203 - INFO - User message: Can you see my attached file?
2024-12-29 20:10:25,203 - DEBUG - Sending request with 3 messages
2024-12-29 20:10:25,203 - DEBUG - Using deployment: o1-preview
2024-12-29 20:10:25,203 - DEBUG - Payload being sent to API: [{'role': 'user', 'content': 'Please format your responses in Markdown.'}, {'role': 'user', 'content': 'Test'}, {'role': 'user', 'content': 'Can you see my attached file?'}]
2024-12-29 20:10:25,204 - DEBUG - API Endpoint: https://openai-hp.openai.azure.com/
2024-12-29 20:10:25,204 - DEBUG - Model Deployment Name: o1-preview
2024-12-29 20:10:25,204 - DEBUG - Message 0: {'role': 'user', 'content': 'Please format your responses in Markdown.'}
2024-12-29 20:10:25,204 - DEBUG - Message 1: {'role': 'user', 'content': 'Test'}
2024-12-29 20:10:25,204 - DEBUG - Message 2: {'role': 'user', 'content': 'Can you see my attached file?'}
2024-12-29 20:10:25,209 - DEBUG - Request options: {'method': 'post', 'url': '/deployments/o1-preview/chat/completions', 'headers': {'api-key': '<redacted>'}, 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': 'Please format your responses in Markdown.'}, {'role': 'user', 'content': 'Test'}, {'role': 'user', 'content': 'Can you see my attached file?'}], 'model': 'o1-preview', 'max_completion_tokens': 32000, 'response_format': {'type': 'text'}, 'temperature': 1}}
2024-12-29 20:10:25,210 - DEBUG - Sending HTTP Request: POST https://openai-hp.openai.azure.com/openai/deployments/o1-preview/chat/completions?api-version=2024-12-01-preview
2024-12-29 20:10:25,211 - DEBUG - close.started
2024-12-29 20:10:25,211 - DEBUG - close.complete
2024-12-29 20:10:25,212 - DEBUG - connect_tcp.started host='openai-hp.openai.azure.com' port=443 local_address=None timeout=5.0 socket_options=None
2024-12-29 20:10:25,272 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002E11961C2D0>
2024-12-29 20:10:25,272 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x000002E1195227B0> server_hostname='openai-hp.openai.azure.com' timeout=5.0
2024-12-29 20:10:25,346 - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002E1195D0FC0>
2024-12-29 20:10:25,347 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2024-12-29 20:10:25,347 - DEBUG - send_request_headers.complete
2024-12-29 20:10:25,348 - DEBUG - send_request_body.started request=<Request [b'POST']>
2024-12-29 20:10:25,348 - DEBUG - send_request_body.complete
2024-12-29 20:10:25,348 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2024-12-29 20:10:26,726 - DEBUG - Received file upload request
2024-12-29 20:10:26,727 - DEBUG - Processing uploaded files
2024-12-29 20:10:26,727 - DEBUG - Uploads directory ensured
2024-12-29 20:10:26,728 - INFO - File saved: uploads\AI Chat Enhancement Phase 2.md
2024-12-29 20:10:26,728 - INFO - Files successfully uploaded: ['AI Chat Enhancement Phase 2.md']
2024-12-29 20:10:26,728 - INFO - 127.0.0.1 - - [29/Dec/2024 20:10:26] "POST /upload HTTP/1.1" 200 -
2024-12-29 20:10:33,671 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Length', b'650'), (b'Content-Type', b'application/json'), (b'apim-request-id', b'05af449c-ecad-43ad-935f-82ab16f169d4'), (b'Strict-Transport-Security', b'max-age=31536000; includeSubDomains; preload'), (b'x-content-type-options', b'nosniff'), (b'x-ratelimit-remaining-tokens', b'2336000'), (b'x-accel-buffering', b'no'), (b'x-ms-rai-invoked', b'true'), (b'x-request-id', b'89120173-bd37-42ed-a6e8-7e1d764bb96a'), (b'x-ms-region', b'East US 2'), (b'azureml-model-session', b'd082-20241219181412'), (b'x-ratelimit-remaining-requests', b'398'), (b'x-envoy-upstream-service-time', b'8257'), (b'x-ms-client-request-id', b'05af449c-ecad-43ad-935f-82ab16f169d4'), (b'Date', b'Mon, 30 Dec 2024 02:10:31 GMT')])
2024-12-29 20:10:33,672 - INFO - HTTP Request: POST https://openai-hp.openai.azure.com/openai/deployments/o1-preview/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2024-12-29 20:10:33,672 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2024-12-29 20:10:33,673 - DEBUG - receive_response_body.complete
2024-12-29 20:10:33,673 - DEBUG - response_closed.started
2024-12-29 20:10:33,673 - DEBUG - response_closed.complete
2024-12-29 20:10:33,673 - DEBUG - HTTP Response: POST https://openai-hp.openai.azure.com/openai/deployments/o1-preview/chat/completions?api-version=2024-12-01-preview "200 OK" Headers({'content-length': '650', 'content-type': 'application/json', 'apim-request-id': '05af449c-ecad-43ad-935f-82ab16f169d4', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'x-content-type-options': 'nosniff', 'x-ratelimit-remaining-tokens': '2336000', 'x-accel-buffering': 'no', 'x-ms-rai-invoked': 'true', 'x-request-id': '89120173-bd37-42ed-a6e8-7e1d764bb96a', 'x-ms-region': 'East US 2', 'azureml-model-session': 'd082-20241219181412', 'x-ratelimit-remaining-requests': '398', 'x-envoy-upstream-service-time': '8257', 'x-ms-client-request-id': '05af449c-ecad-43ad-935f-82ab16f169d4', 'date': 'Mon, 30 Dec 2024 02:10:31 GMT'})
2024-12-29 20:10:33,674 - DEBUG - request_id: 89120173-bd37-42ed-a6e8-7e1d764bb96a
2024-12-29 20:10:33,675 - INFO - Received response: <p>I'm sorry, but I can't see any attached files. If you could describe or share the content, I'd be happy to help you with it!</p>
<div class="shortcuts"><button onclick="copyToClipboard()">Copy</button><button onclick="retryMessage()">Retry</button></div>
2024-12-29 20:10:33,675 - DEBUG - Response being sent to chat interface: <p>I'm sorry, but I can't see any attached files. If you could describe or share the content, I'd be happy to help you with it!</p>
<div class="shortcuts"><button onclick="copyToClipboard()">Copy</button><button onclick="retryMessage()">Retry</button></div>
2024-12-29 20:10:33,675 - DEBUG - Raw API response: ChatCompletion(id='chatcmpl-AjzWaaFYoqYyq0iVOdbozQHrXPau3', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content="I'm sorry, but I can't see any attached files. If you could describe or share the content, I'd be happy to help you with it!", refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None), content_filter_results={})], created=1735524624, model='o1-preview-2024-09-12', object='chat.completion', service_tier=None, system_fingerprint='fp_772cb6e46d', usage=CompletionUsage(completion_tokens=489, prompt_tokens=45, total_tokens=534, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=448, rejected_prediction_tokens=None), prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=0)), prompt_filter_results=[{'prompt_index': 0, 'content_filter_results': {}}])
2024-12-29 20:10:33,675 - DEBUG - Usage: CompletionUsage(completion_tokens=489, prompt_tokens=45, total_tokens=534, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=448, rejected_prediction_tokens=None), prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=0))
2024-12-29 20:10:33,676 - INFO - 127.0.0.1 - - [29/Dec/2024 20:10:33] "POST /chat HTTP/1.1" 200 -
2024-12-29 20:11:00,350 - INFO -  * Detected change in 'C:\\Users\\htper\\OneDrive\\Folder\\chats\\chatapp\\app.py', reloading
2024-12-29 20:11:00,521 - INFO -  * Restarting with stat
2024-12-29 20:11:02,580 - WARNING -  * Debugger is active!
2024-12-29 20:11:02,588 - INFO -  * Debugger PIN: 995-560-430
2024-12-29 20:11:09,804 - INFO - 127.0.0.1 - - [29/Dec/2024 20:11:09] "[32mPOST /chat HTTP/1.1[0m" 302 -
2024-12-29 20:11:09,814 - INFO - 127.0.0.1 - - [29/Dec/2024 20:11:09] "GET /login?next=/chat HTTP/1.1" 200 -
2024-12-29 20:11:36,574 - INFO - 127.0.0.1 - - [29/Dec/2024 20:11:36] "[32mPOST /upload HTTP/1.1[0m" 302 -
2024-12-29 20:11:36,580 - INFO - 127.0.0.1 - - [29/Dec/2024 20:11:36] "GET /login?next=/upload HTTP/1.1" 200 -
2024-12-29 20:11:40,810 - INFO - 127.0.0.1 - - [29/Dec/2024 20:11:40] "[32mPOST /chat HTTP/1.1[0m" 302 -
2024-12-29 20:11:40,815 - INFO - 127.0.0.1 - - [29/Dec/2024 20:11:40] "GET /login?next=/chat HTTP/1.1" 200 -
2024-12-29 20:12:11,315 - INFO -  * Detected change in 'C:\\Users\\htper\\OneDrive\\Folder\\chats\\chatapp\\app.py', reloading
2024-12-29 20:12:11,455 - INFO -  * Restarting with stat
2024-12-29 20:12:13,592 - WARNING -  * Debugger is active!
2024-12-29 20:12:13,599 - INFO -  * Debugger PIN: 995-560-430
2024-12-29 20:12:25,790 - INFO - 127.0.0.1 - - [29/Dec/2024 20:12:25] "[32mGET /chat_interface HTTP/1.1[0m" 302 -
2024-12-29 20:12:25,806 - INFO - 127.0.0.1 - - [29/Dec/2024 20:12:25] "GET /login?next=/chat_interface HTTP/1.1" 200 -
2024-12-29 20:12:29,849 - INFO - 127.0.0.1 - - [29/Dec/2024 20:12:29] "[32mPOST /login HTTP/1.1[0m" 302 -
2024-12-29 20:12:29,857 - INFO - 127.0.0.1 - - [29/Dec/2024 20:12:29] "GET /chat_interface HTTP/1.1" 200 -
2024-12-29 20:12:29,977 - INFO - 127.0.0.1 - - [29/Dec/2024 20:12:29] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-29 20:12:32,043 - DEBUG - Received chat message
2024-12-29 20:12:32,043 - DEBUG - Chat ID: 46b12ccf-3e9c-4e88-9153-e288f9434c3d
2024-12-29 20:12:32,044 - INFO - User message: Test
2024-12-29 20:12:32,044 - DEBUG - Sending request with 2 messages
2024-12-29 20:12:32,044 - DEBUG - Using deployment: o1-preview
2024-12-29 20:12:32,044 - DEBUG - Payload being sent to API: [{'role': 'user', 'content': 'Please format your responses in Markdown.'}, {'role': 'user', 'content': 'Test'}]
2024-12-29 20:12:32,045 - DEBUG - Exact file content included in payload: []
2024-12-29 20:12:32,045 - DEBUG - API Endpoint: https://openai-hp.openai.azure.com/
2024-12-29 20:12:32,045 - DEBUG - Model Deployment Name: o1-preview
2024-12-29 20:12:32,045 - DEBUG - Message 0: {'role': 'user', 'content': 'Please format your responses in Markdown.'}
2024-12-29 20:12:32,045 - DEBUG - Message 1: {'role': 'user', 'content': 'Test'}
2024-12-29 20:12:32,050 - DEBUG - Request options: {'method': 'post', 'url': '/deployments/o1-preview/chat/completions', 'headers': {'api-key': '<redacted>'}, 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': 'Please format your responses in Markdown.'}, {'role': 'user', 'content': 'Test'}], 'model': 'o1-preview', 'max_completion_tokens': 32000, 'response_format': {'type': 'text'}, 'temperature': 1}}
2024-12-29 20:12:32,133 - DEBUG - Sending HTTP Request: POST https://openai-hp.openai.azure.com/openai/deployments/o1-preview/chat/completions?api-version=2024-12-01-preview
2024-12-29 20:12:32,133 - DEBUG - connect_tcp.started host='openai-hp.openai.azure.com' port=443 local_address=None timeout=5.0 socket_options=None
2024-12-29 20:12:32,270 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024581A25BE0>
2024-12-29 20:12:32,270 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x000002458195E7B0> server_hostname='openai-hp.openai.azure.com' timeout=5.0
2024-12-29 20:12:32,349 - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000245819479D0>
2024-12-29 20:12:32,349 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2024-12-29 20:12:32,350 - DEBUG - send_request_headers.complete
2024-12-29 20:12:32,350 - DEBUG - send_request_body.started request=<Request [b'POST']>
2024-12-29 20:12:32,351 - DEBUG - send_request_body.complete
2024-12-29 20:12:32,351 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2024-12-29 20:12:43,953 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Length', b'600'), (b'Content-Type', b'application/json'), (b'apim-request-id', b'ad9cb75e-d465-4f5d-89b5-157e3a686fc6'), (b'Strict-Transport-Security', b'max-age=31536000; includeSubDomains; preload'), (b'x-content-type-options', b'nosniff'), (b'x-ratelimit-remaining-tokens', b'2368000'), (b'x-accel-buffering', b'no'), (b'x-ms-rai-invoked', b'true'), (b'x-request-id', b'3444578f-c476-49f0-b0af-c1cf04267d39'), (b'x-ms-region', b'East US 2'), (b'azureml-model-session', b'd091-20241220021933'), (b'x-ratelimit-remaining-requests', b'399'), (b'x-envoy-upstream-service-time', b'11535'), (b'x-ms-client-request-id', b'ad9cb75e-d465-4f5d-89b5-157e3a686fc6'), (b'Date', b'Mon, 30 Dec 2024 02:12:42 GMT')])
2024-12-29 20:12:43,954 - INFO - HTTP Request: POST https://openai-hp.openai.azure.com/openai/deployments/o1-preview/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2024-12-29 20:12:43,954 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2024-12-29 20:12:43,954 - DEBUG - receive_response_body.complete
2024-12-29 20:12:43,955 - DEBUG - response_closed.started
2024-12-29 20:12:43,955 - DEBUG - response_closed.complete
2024-12-29 20:12:43,955 - DEBUG - HTTP Response: POST https://openai-hp.openai.azure.com/openai/deployments/o1-preview/chat/completions?api-version=2024-12-01-preview "200 OK" Headers({'content-length': '600', 'content-type': 'application/json', 'apim-request-id': 'ad9cb75e-d465-4f5d-89b5-157e3a686fc6', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'x-content-type-options': 'nosniff', 'x-ratelimit-remaining-tokens': '2368000', 'x-accel-buffering': 'no', 'x-ms-rai-invoked': 'true', 'x-request-id': '3444578f-c476-49f0-b0af-c1cf04267d39', 'x-ms-region': 'East US 2', 'azureml-model-session': 'd091-20241220021933', 'x-ratelimit-remaining-requests': '399', 'x-envoy-upstream-service-time': '11535', 'x-ms-client-request-id': 'ad9cb75e-d465-4f5d-89b5-157e3a686fc6', 'date': 'Mon, 30 Dec 2024 02:12:42 GMT'})
2024-12-29 20:12:43,955 - DEBUG - request_id: 3444578f-c476-49f0-b0af-c1cf04267d39
2024-12-29 20:12:43,965 - INFO - Received response: <p><strong>Test received!</strong></p>

<p>I will format my responses in Markdown as requested.</p>
<div class="shortcuts"><button onclick="copyToClipboard()">Copy</button><button onclick="retryMessage()">Retry</button></div>
2024-12-29 20:12:43,965 - DEBUG - Response being sent to chat interface: <p><strong>Test received!</strong></p>

<p>I will format my responses in Markdown as requested.</p>
<div class="shortcuts"><button onclick="copyToClipboard()">Copy</button><button onclick="retryMessage()">Retry</button></div>
2024-12-29 20:12:43,966 - DEBUG - Raw API response: ChatCompletion(id='chatcmpl-AjzYd0kIqxVowdq2wgHX24N3Ui45F', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='**Test received!**\n\nI will format my responses in Markdown as requested.', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None), content_filter_results={})], created=1735524751, model='o1-preview-2024-09-12', object='chat.completion', service_tier=None, system_fingerprint='fp_772cb6e46d', usage=CompletionUsage(completion_tokens=795, prompt_tokens=27, total_tokens=822, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=768, rejected_prediction_tokens=None), prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=0)), prompt_filter_results=[{'prompt_index': 0, 'content_filter_results': {}}])
2024-12-29 20:12:43,966 - DEBUG - Usage: CompletionUsage(completion_tokens=795, prompt_tokens=27, total_tokens=822, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=768, rejected_prediction_tokens=None), prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=0))
2024-12-29 20:12:43,966 - INFO - 127.0.0.1 - - [29/Dec/2024 20:12:43] "POST /chat HTTP/1.1" 200 -
2024-12-29 20:12:46,979 - DEBUG - Received file upload request
2024-12-29 20:12:46,980 - DEBUG - Processing uploaded files
2024-12-29 20:12:46,981 - DEBUG - Uploads directory ensured
2024-12-29 20:12:46,982 - INFO - File saved: uploads\Phase 4 Advanced AI Features.md
2024-12-29 20:12:46,982 - INFO - Files successfully uploaded: ['Phase 4 Advanced AI Features.md']
2024-12-29 20:12:46,983 - INFO - 127.0.0.1 - - [29/Dec/2024 20:12:46] "POST /upload HTTP/1.1" 200 -
2024-12-29 20:14:52,752 - INFO - 127.0.0.1 - - [29/Dec/2024 20:14:52] "GET /chat_interface HTTP/1.1" 200 -
2024-12-29 20:14:52,807 - INFO - 127.0.0.1 - - [29/Dec/2024 20:14:52] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-29 20:14:55,038 - DEBUG - Received chat message
2024-12-29 20:14:55,039 - DEBUG - Chat ID: 46b12ccf-3e9c-4e88-9153-e288f9434c3d
2024-12-29 20:14:55,039 - INFO - User message: Test
2024-12-29 20:14:55,039 - DEBUG - Analyzing file: uploads\Phase 4 Advanced AI Features.md
2024-12-29 20:14:55,040 - DEBUG - Sending request with 4 messages
2024-12-29 20:14:55,041 - DEBUG - Using deployment: o1-preview
2024-12-29 20:14:55,041 - DEBUG - Payload being sent to API: [{'role': 'user', 'content': 'Please format your responses in Markdown.'}, {'role': 'user', 'content': 'Test'}, {'role': 'user', 'content': 'Test'}, {'role': 'user', 'content': 'File content:\n```\nAlright, here\'s the prompt for **Phase 4**, focusing on advanced AI and conversation features, while maintaining the distinction between project-based and independent conversations:\n\n**# Phase 4: Advanced AI and Conversation Features (Revised)**\n\n## **Overview**\n\nPhase 4 focuses on integrating advanced AI capabilities and enhancing conversation management features for both project-based and independent conversations. Building upon the robust foundation established in Phase 3, this phase aims to create a more intelligent and interactive chat experience. Key objectives include:\n\n-   **Advanced AI Integration:** Implement personality adjustments, proactive assistance, and sentiment analysis, taking into account if a project is active.\n-   **Enhanced Conversation Management:** Implement semantic search, conversation sharing, and exporting, taking into account if a project is active.\n-   **Performance Optimization:** Begin optimizing core components for performance and scalability.\n\nBy the end of Phase 4, the chat application will offer a significantly more sophisticated and user-friendly experience, ready for the final polishing and broader user testing in the remaining phases, making use of the project features from Phase 1 to improve the AI performance.\n\n---\n\n## **Core Components Implementation Requirements**\n\n### **1. Advanced AI Enhancements**\n\n#### **Required Functionality**\n\n-   **Personality Adjustments:**\n    - Allow users to customize the AI\'s personality (tone, creativity) using parameters like temperature or predefined profiles.\n    - The AI should adapt its personality based on the selected language model for project based conversations, *or* based on a default setting for unassociated conversations.\n    -   **UI/UX:** Design intuitive controls for personality adjustments, allowing for both project-specific and default settings.\n    -   **Error Handling:** Handle invalid parameter values or profile configurations.\n\n-   **Proactive Assistance:**\n    - Implement proactive suggestions and assistance based on conversation context, using RAG for project-based conversations and using chat history for independent conversations.\n    -   **Relevance:** Ensure suggestions are relevant and helpful in the context of project or independent chats.\n    -   **Performance:** Optimize proactive assistance to avoid excessive API calls or processing.\n\n-   **Sentiment Analysis:**\n    - Analyze user messages for sentiment and adjust the AI\'s responses accordingly.\n    -   **Accuracy:** Use reliable sentiment analysis libraries and techniques.\n    -   **Integration:** Integrate sentiment analysis seamlessly into the AI response generation process, affecting both project-based RAG responses and unassociated conversation responses.\n\n#### **Example Code (Backend - Python - Conceptual)**\n\n```python\n# Example for personality adjustments\ndef get_ai_response(message, conversation_history, personality_profile="default", project_id=None, language_model=\'gpt-3.5-turbo\'):\n    """\n      Adjust AI response based on personality and also by using the given project_id or the language_model to determine the RAG context or conversation history context.\n    """\n    # ... use personality_profile to adjust API parameters (e.g., temperature), also using the given project_id or the language_model to determine the RAG context or conversation history context\n    pass\n\n# Example for proactive assistance (conceptual)\ndef suggest_next_action(conversation_history, project_id = None):\n    """\n       Provide proactive assistance, using the RAG functionality if a project_id is provided\n    """\n    # ... analyze conversation history to suggest relevant actions or information\n    # ... if project_id is provided, make sure to use RAG to create the proactive response\n    pass\n```\n\n**Additional Notes:**\n\n-   **AI Model Selection:** Choose appropriate AI models and libraries for personality adjustments, proactive assistance, and sentiment analysis.\n-   **Testing:** Thoroughly test these features to ensure they function as expected and enhance the user experience.\n\n---\n\n### **2. Enhanced Conversation Management**\n\n#### **Required Functionality**\n\n-   **Semantic Search:**\n    - Implement semantic search functionality to allow users to search conversations based on meaning, including project-based conversations and also independent conversations.\n    -   **NLP Integration:** Integrate with NLP libraries (e.g., spaCy, NLTK) for semantic analysis.\n    -   **Performance:** Optimize search indexing and retrieval for speed and efficiency.\n\n-   **Conversation Sharing and Exporting:**\n    - Allow users to share conversations with others via shareable links or export them in various formats (JSON, PDF, etc.), allowing the user to select either project-based or unassociated conversations for sharing/exporting.\n    -   **Security:** Implement appropriate access controls for shared conversations.\n    -   **Error Handling:** Handle share/export failures gracefully.\n\n#### **Example Code (Backend - Python - Conceptual)**\n\n```python\n# Example for semantic search (conceptual)\ndef search_conversations(query, user_id, project_id = None):\n    """\n      Search the conversations, using RAG if a project_id is provided, and searching user messages if not.\n    """\n    # ... use NLP techniques to perform semantic search on conversation data, optionally filtering results by project_id or by user messages if not specified\n    pass\n\n# Example for conversation export (conceptual)\ndef export_conversation(conversation_id, format="json", project_id = None):\n    """Export the data for a conversation, or project data if a project_id is provided"""\n    # ... retrieve conversation data and export it in the specified format, using project_id to filter\n    pass\n```\n\n**Additional Notes:**\n\n-   **User Interface:** Design clear and intuitive UI elements for search, sharing, and exporting.\n-   **Data Handling:** Manage shared conversation data and access permissions effectively.\n\n---\n\n### **Configuration System**\n\n#### **Environment Variables**\n\n-   `NLP_MODEL_PATH`: Path to the NLP model for semantic search.\n-   `ALLOWED_EXPORT_FORMATS`: Comma-separated list of allowed export formats.\n\n---\n\n### **Error Handling**\n\n*  Continue to refine error handling and logging for all AI and Conversation features, accounting for project based data and unassociated data.\n\n---\n\n### **Conclusion**\n\nImplementing Phase 4 integrates advanced AI capabilities and enhances conversation management, taking into account both project-based and independent conversation modes. This brings the chat application closer to its final form. The next phases will focus on refining the user experience, optimizing performance, and conducting thorough testing.\n\nThat\'s Phase 4! Let me know if you\'d like to move on to Phase 5 or have any other questions!\n\n```'}]
2024-12-29 20:14:55,042 - DEBUG - Exact file content included in payload: ['Alright, here\'s the prompt for **Phase 4**, focusing on advanced AI and conversation features, while maintaining the distinction between project-based and independent conversations:\n\n**# Phase 4: Advanced AI and Conversation Features (Revised)**\n\n## **Overview**\n\nPhase 4 focuses on integrating advanced AI capabilities and enhancing conversation management features for both project-based and independent conversations. Building upon the robust foundation established in Phase 3, this phase aims to create a more intelligent and interactive chat experience. Key objectives include:\n\n-   **Advanced AI Integration:** Implement personality adjustments, proactive assistance, and sentiment analysis, taking into account if a project is active.\n-   **Enhanced Conversation Management:** Implement semantic search, conversation sharing, and exporting, taking into account if a project is active.\n-   **Performance Optimization:** Begin optimizing core components for performance and scalability.\n\nBy the end of Phase 4, the chat application will offer a significantly more sophisticated and user-friendly experience, ready for the final polishing and broader user testing in the remaining phases, making use of the project features from Phase 1 to improve the AI performance.\n\n---\n\n## **Core Components Implementation Requirements**\n\n### **1. Advanced AI Enhancements**\n\n#### **Required Functionality**\n\n-   **Personality Adjustments:**\n    - Allow users to customize the AI\'s personality (tone, creativity) using parameters like temperature or predefined profiles.\n    - The AI should adapt its personality based on the selected language model for project based conversations, *or* based on a default setting for unassociated conversations.\n    -   **UI/UX:** Design intuitive controls for personality adjustments, allowing for both project-specific and default settings.\n    -   **Error Handling:** Handle invalid parameter values or profile configurations.\n\n-   **Proactive Assistance:**\n    - Implement proactive suggestions and assistance based on conversation context, using RAG for project-based conversations and using chat history for independent conversations.\n    -   **Relevance:** Ensure suggestions are relevant and helpful in the context of project or independent chats.\n    -   **Performance:** Optimize proactive assistance to avoid excessive API calls or processing.\n\n-   **Sentiment Analysis:**\n    - Analyze user messages for sentiment and adjust the AI\'s responses accordingly.\n    -   **Accuracy:** Use reliable sentiment analysis libraries and techniques.\n    -   **Integration:** Integrate sentiment analysis seamlessly into the AI response generation process, affecting both project-based RAG responses and unassociated conversation responses.\n\n#### **Example Code (Backend - Python - Conceptual)**\n\n```python\n# Example for personality adjustments\ndef get_ai_response(message, conversation_history, personality_profile="default", project_id=None, language_model=\'gpt-3.5-turbo\'):\n    """\n      Adjust AI response based on personality and also by using the given project_id or the language_model to determine the RAG context or conversation history context.\n    """\n    # ... use personality_profile to adjust API parameters (e.g., temperature), also using the given project_id or the language_model to determine the RAG context or conversation history context\n    pass\n\n# Example for proactive assistance (conceptual)\ndef suggest_next_action(conversation_history, project_id = None):\n    """\n       Provide proactive assistance, using the RAG functionality if a project_id is provided\n    """\n    # ... analyze conversation history to suggest relevant actions or information\n    # ... if project_id is provided, make sure to use RAG to create the proactive response\n    pass\n```\n\n**Additional Notes:**\n\n-   **AI Model Selection:** Choose appropriate AI models and libraries for personality adjustments, proactive assistance, and sentiment analysis.\n-   **Testing:** Thoroughly test these features to ensure they function as expected and enhance the user experience.\n\n---\n\n### **2. Enhanced Conversation Management**\n\n#### **Required Functionality**\n\n-   **Semantic Search:**\n    - Implement semantic search functionality to allow users to search conversations based on meaning, including project-based conversations and also independent conversations.\n    -   **NLP Integration:** Integrate with NLP libraries (e.g., spaCy, NLTK) for semantic analysis.\n    -   **Performance:** Optimize search indexing and retrieval for speed and efficiency.\n\n-   **Conversation Sharing and Exporting:**\n    - Allow users to share conversations with others via shareable links or export them in various formats (JSON, PDF, etc.), allowing the user to select either project-based or unassociated conversations for sharing/exporting.\n    -   **Security:** Implement appropriate access controls for shared conversations.\n    -   **Error Handling:** Handle share/export failures gracefully.\n\n#### **Example Code (Backend - Python - Conceptual)**\n\n```python\n# Example for semantic search (conceptual)\ndef search_conversations(query, user_id, project_id = None):\n    """\n      Search the conversations, using RAG if a project_id is provided, and searching user messages if not.\n    """\n    # ... use NLP techniques to perform semantic search on conversation data, optionally filtering results by project_id or by user messages if not specified\n    pass\n\n# Example for conversation export (conceptual)\ndef export_conversation(conversation_id, format="json", project_id = None):\n    """Export the data for a conversation, or project data if a project_id is provided"""\n    # ... retrieve conversation data and export it in the specified format, using project_id to filter\n    pass\n```\n\n**Additional Notes:**\n\n-   **User Interface:** Design clear and intuitive UI elements for search, sharing, and exporting.\n-   **Data Handling:** Manage shared conversation data and access permissions effectively.\n\n---\n\n### **Configuration System**\n\n#### **Environment Variables**\n\n-   `NLP_MODEL_PATH`: Path to the NLP model for semantic search.\n-   `ALLOWED_EXPORT_FORMATS`: Comma-separated list of allowed export formats.\n\n---\n\n### **Error Handling**\n\n*  Continue to refine error handling and logging for all AI and Conversation features, accounting for project based data and unassociated data.\n\n---\n\n### **Conclusion**\n\nImplementing Phase 4 integrates advanced AI capabilities and enhances conversation management, taking into account both project-based and independent conversation modes. This brings the chat application closer to its final form. The next phases will focus on refining the user experience, optimizing performance, and conducting thorough testing.\n\nThat\'s Phase 4! Let me know if you\'d like to move on to Phase 5 or have any other questions!\n']
2024-12-29 20:14:55,043 - DEBUG - API Endpoint: https://openai-hp.openai.azure.com/
2024-12-29 20:14:55,043 - DEBUG - Model Deployment Name: o1-preview
2024-12-29 20:14:55,043 - DEBUG - Message 0: {'role': 'user', 'content': 'Please format your responses in Markdown.'}
2024-12-29 20:14:55,044 - DEBUG - Message 1: {'role': 'user', 'content': 'Test'}
2024-12-29 20:14:55,044 - DEBUG - Message 2: {'role': 'user', 'content': 'Test'}
2024-12-29 20:14:55,044 - DEBUG - Message 3: {'role': 'user', 'content': 'File content:\n```\nAlright, here\'s the prompt for **Phase 4**, focusing on advanced AI and conversation features, while maintaining the distinction between project-based and independent conversations:\n\n**# Phase 4: Advanced AI and Conversation Features (Revised)**\n\n## **Overview**\n\nPhase 4 focuses on integrating advanced AI capabilities and enhancing conversation management features for both project-based and independent conversations. Building upon the robust foundation established in Phase 3, this phase aims to create a more intelligent and interactive chat experience. Key objectives include:\n\n-   **Advanced AI Integration:** Implement personality adjustments, proactive assistance, and sentiment analysis, taking into account if a project is active.\n-   **Enhanced Conversation Management:** Implement semantic search, conversation sharing, and exporting, taking into account if a project is active.\n-   **Performance Optimization:** Begin optimizing core components for performance and scalability.\n\nBy the end of Phase 4, the chat application will offer a significantly more sophisticated and user-friendly experience, ready for the final polishing and broader user testing in the remaining phases, making use of the project features from Phase 1 to improve the AI performance.\n\n---\n\n## **Core Components Implementation Requirements**\n\n### **1. Advanced AI Enhancements**\n\n#### **Required Functionality**\n\n-   **Personality Adjustments:**\n    - Allow users to customize the AI\'s personality (tone, creativity) using parameters like temperature or predefined profiles.\n    - The AI should adapt its personality based on the selected language model for project based conversations, *or* based on a default setting for unassociated conversations.\n    -   **UI/UX:** Design intuitive controls for personality adjustments, allowing for both project-specific and default settings.\n    -   **Error Handling:** Handle invalid parameter values or profile configurations.\n\n-   **Proactive Assistance:**\n    - Implement proactive suggestions and assistance based on conversation context, using RAG for project-based conversations and using chat history for independent conversations.\n    -   **Relevance:** Ensure suggestions are relevant and helpful in the context of project or independent chats.\n    -   **Performance:** Optimize proactive assistance to avoid excessive API calls or processing.\n\n-   **Sentiment Analysis:**\n    - Analyze user messages for sentiment and adjust the AI\'s responses accordingly.\n    -   **Accuracy:** Use reliable sentiment analysis libraries and techniques.\n    -   **Integration:** Integrate sentiment analysis seamlessly into the AI response generation process, affecting both project-based RAG responses and unassociated conversation responses.\n\n#### **Example Code (Backend - Python - Conceptual)**\n\n```python\n# Example for personality adjustments\ndef get_ai_response(message, conversation_history, personality_profile="default", project_id=None, language_model=\'gpt-3.5-turbo\'):\n    """\n      Adjust AI response based on personality and also by using the given project_id or the language_model to determine the RAG context or conversation history context.\n    """\n    # ... use personality_profile to adjust API parameters (e.g., temperature), also using the given project_id or the language_model to determine the RAG context or conversation history context\n    pass\n\n# Example for proactive assistance (conceptual)\ndef suggest_next_action(conversation_history, project_id = None):\n    """\n       Provide proactive assistance, using the RAG functionality if a project_id is provided\n    """\n    # ... analyze conversation history to suggest relevant actions or information\n    # ... if project_id is provided, make sure to use RAG to create the proactive response\n    pass\n```\n\n**Additional Notes:**\n\n-   **AI Model Selection:** Choose appropriate AI models and libraries for personality adjustments, proactive assistance, and sentiment analysis.\n-   **Testing:** Thoroughly test these features to ensure they function as expected and enhance the user experience.\n\n---\n\n### **2. Enhanced Conversation Management**\n\n#### **Required Functionality**\n\n-   **Semantic Search:**\n    - Implement semantic search functionality to allow users to search conversations based on meaning, including project-based conversations and also independent conversations.\n    -   **NLP Integration:** Integrate with NLP libraries (e.g., spaCy, NLTK) for semantic analysis.\n    -   **Performance:** Optimize search indexing and retrieval for speed and efficiency.\n\n-   **Conversation Sharing and Exporting:**\n    - Allow users to share conversations with others via shareable links or export them in various formats (JSON, PDF, etc.), allowing the user to select either project-based or unassociated conversations for sharing/exporting.\n    -   **Security:** Implement appropriate access controls for shared conversations.\n    -   **Error Handling:** Handle share/export failures gracefully.\n\n#### **Example Code (Backend - Python - Conceptual)**\n\n```python\n# Example for semantic search (conceptual)\ndef search_conversations(query, user_id, project_id = None):\n    """\n      Search the conversations, using RAG if a project_id is provided, and searching user messages if not.\n    """\n    # ... use NLP techniques to perform semantic search on conversation data, optionally filtering results by project_id or by user messages if not specified\n    pass\n\n# Example for conversation export (conceptual)\ndef export_conversation(conversation_id, format="json", project_id = None):\n    """Export the data for a conversation, or project data if a project_id is provided"""\n    # ... retrieve conversation data and export it in the specified format, using project_id to filter\n    pass\n```\n\n**Additional Notes:**\n\n-   **User Interface:** Design clear and intuitive UI elements for search, sharing, and exporting.\n-   **Data Handling:** Manage shared conversation data and access permissions effectively.\n\n---\n\n### **Configuration System**\n\n#### **Environment Variables**\n\n-   `NLP_MODEL_PATH`: Path to the NLP model for semantic search.\n-   `ALLOWED_EXPORT_FORMATS`: Comma-separated list of allowed export formats.\n\n---\n\n### **Error Handling**\n\n*  Continue to refine error handling and logging for all AI and Conversation features, accounting for project based data and unassociated data.\n\n---\n\n### **Conclusion**\n\nImplementing Phase 4 integrates advanced AI capabilities and enhances conversation management, taking into account both project-based and independent conversation modes. This brings the chat application closer to its final form. The next phases will focus on refining the user experience, optimizing performance, and conducting thorough testing.\n\nThat\'s Phase 4! Let me know if you\'d like to move on to Phase 5 or have any other questions!\n\n```'}
2024-12-29 20:14:55,050 - DEBUG - Request options: {'method': 'post', 'url': '/deployments/o1-preview/chat/completions', 'headers': {'api-key': '<redacted>'}, 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': 'Please format your responses in Markdown.'}, {'role': 'user', 'content': 'Test'}, {'role': 'user', 'content': 'Test'}, {'role': 'user', 'content': 'File content:\n```\nAlright, here\'s the prompt for **Phase 4**, focusing on advanced AI and conversation features, while maintaining the distinction between project-based and independent conversations:\n\n**# Phase 4: Advanced AI and Conversation Features (Revised)**\n\n## **Overview**\n\nPhase 4 focuses on integrating advanced AI capabilities and enhancing conversation management features for both project-based and independent conversations. Building upon the robust foundation established in Phase 3, this phase aims to create a more intelligent and interactive chat experience. Key objectives include:\n\n-   **Advanced AI Integration:** Implement personality adjustments, proactive assistance, and sentiment analysis, taking into account if a project is active.\n-   **Enhanced Conversation Management:** Implement semantic search, conversation sharing, and exporting, taking into account if a project is active.\n-   **Performance Optimization:** Begin optimizing core components for performance and scalability.\n\nBy the end of Phase 4, the chat application will offer a significantly more sophisticated and user-friendly experience, ready for the final polishing and broader user testing in the remaining phases, making use of the project features from Phase 1 to improve the AI performance.\n\n---\n\n## **Core Components Implementation Requirements**\n\n### **1. Advanced AI Enhancements**\n\n#### **Required Functionality**\n\n-   **Personality Adjustments:**\n    - Allow users to customize the AI\'s personality (tone, creativity) using parameters like temperature or predefined profiles.\n    - The AI should adapt its personality based on the selected language model for project based conversations, *or* based on a default setting for unassociated conversations.\n    -   **UI/UX:** Design intuitive controls for personality adjustments, allowing for both project-specific and default settings.\n    -   **Error Handling:** Handle invalid parameter values or profile configurations.\n\n-   **Proactive Assistance:**\n    - Implement proactive suggestions and assistance based on conversation context, using RAG for project-based conversations and using chat history for independent conversations.\n    -   **Relevance:** Ensure suggestions are relevant and helpful in the context of project or independent chats.\n    -   **Performance:** Optimize proactive assistance to avoid excessive API calls or processing.\n\n-   **Sentiment Analysis:**\n    - Analyze user messages for sentiment and adjust the AI\'s responses accordingly.\n    -   **Accuracy:** Use reliable sentiment analysis libraries and techniques.\n    -   **Integration:** Integrate sentiment analysis seamlessly into the AI response generation process, affecting both project-based RAG responses and unassociated conversation responses.\n\n#### **Example Code (Backend - Python - Conceptual)**\n\n```python\n# Example for personality adjustments\ndef get_ai_response(message, conversation_history, personality_profile="default", project_id=None, language_model=\'gpt-3.5-turbo\'):\n    """\n      Adjust AI response based on personality and also by using the given project_id or the language_model to determine the RAG context or conversation history context.\n    """\n    # ... use personality_profile to adjust API parameters (e.g., temperature), also using the given project_id or the language_model to determine the RAG context or conversation history context\n    pass\n\n# Example for proactive assistance (conceptual)\ndef suggest_next_action(conversation_history, project_id = None):\n    """\n       Provide proactive assistance, using the RAG functionality if a project_id is provided\n    """\n    # ... analyze conversation history to suggest relevant actions or information\n    # ... if project_id is provided, make sure to use RAG to create the proactive response\n    pass\n```\n\n**Additional Notes:**\n\n-   **AI Model Selection:** Choose appropriate AI models and libraries for personality adjustments, proactive assistance, and sentiment analysis.\n-   **Testing:** Thoroughly test these features to ensure they function as expected and enhance the user experience.\n\n---\n\n### **2. Enhanced Conversation Management**\n\n#### **Required Functionality**\n\n-   **Semantic Search:**\n    - Implement semantic search functionality to allow users to search conversations based on meaning, including project-based conversations and also independent conversations.\n    -   **NLP Integration:** Integrate with NLP libraries (e.g., spaCy, NLTK) for semantic analysis.\n    -   **Performance:** Optimize search indexing and retrieval for speed and efficiency.\n\n-   **Conversation Sharing and Exporting:**\n    - Allow users to share conversations with others via shareable links or export them in various formats (JSON, PDF, etc.), allowing the user to select either project-based or unassociated conversations for sharing/exporting.\n    -   **Security:** Implement appropriate access controls for shared conversations.\n    -   **Error Handling:** Handle share/export failures gracefully.\n\n#### **Example Code (Backend - Python - Conceptual)**\n\n```python\n# Example for semantic search (conceptual)\ndef search_conversations(query, user_id, project_id = None):\n    """\n      Search the conversations, using RAG if a project_id is provided, and searching user messages if not.\n    """\n    # ... use NLP techniques to perform semantic search on conversation data, optionally filtering results by project_id or by user messages if not specified\n    pass\n\n# Example for conversation export (conceptual)\ndef export_conversation(conversation_id, format="json", project_id = None):\n    """Export the data for a conversation, or project data if a project_id is provided"""\n    # ... retrieve conversation data and export it in the specified format, using project_id to filter\n    pass\n```\n\n**Additional Notes:**\n\n-   **User Interface:** Design clear and intuitive UI elements for search, sharing, and exporting.\n-   **Data Handling:** Manage shared conversation data and access permissions effectively.\n\n---\n\n### **Configuration System**\n\n#### **Environment Variables**\n\n-   `NLP_MODEL_PATH`: Path to the NLP model for semantic search.\n-   `ALLOWED_EXPORT_FORMATS`: Comma-separated list of allowed export formats.\n\n---\n\n### **Error Handling**\n\n*  Continue to refine error handling and logging for all AI and Conversation features, accounting for project based data and unassociated data.\n\n---\n\n### **Conclusion**\n\nImplementing Phase 4 integrates advanced AI capabilities and enhances conversation management, taking into account both project-based and independent conversation modes. This brings the chat application closer to its final form. The next phases will focus on refining the user experience, optimizing performance, and conducting thorough testing.\n\nThat\'s Phase 4! Let me know if you\'d like to move on to Phase 5 or have any other questions!\n\n```'}], 'model': 'o1-preview', 'max_completion_tokens': 32000, 'response_format': {'type': 'text'}, 'temperature': 1}}
2024-12-29 20:14:55,051 - DEBUG - Sending HTTP Request: POST https://openai-hp.openai.azure.com/openai/deployments/o1-preview/chat/completions?api-version=2024-12-01-preview
2024-12-29 20:14:55,052 - DEBUG - close.started
2024-12-29 20:14:55,053 - DEBUG - close.complete
2024-12-29 20:14:55,053 - DEBUG - connect_tcp.started host='openai-hp.openai.azure.com' port=443 local_address=None timeout=5.0 socket_options=None
2024-12-29 20:14:55,136 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024581A5C410>
2024-12-29 20:14:55,136 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x000002458195E7B0> server_hostname='openai-hp.openai.azure.com' timeout=5.0
2024-12-29 20:14:55,229 - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024581A11CD0>
2024-12-29 20:14:55,229 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2024-12-29 20:14:55,230 - DEBUG - send_request_headers.complete
2024-12-29 20:14:55,230 - DEBUG - send_request_body.started request=<Request [b'POST']>
2024-12-29 20:14:55,230 - DEBUG - send_request_body.complete
2024-12-29 20:14:55,231 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2024-12-29 20:14:56,962 - INFO - 127.0.0.1 - - [29/Dec/2024 20:14:56] "GET /chat_interface HTTP/1.1" 200 -
2024-12-29 20:14:57,115 - INFO - 127.0.0.1 - - [29/Dec/2024 20:14:57] "GET /chat_interface HTTP/1.1" 200 -
2024-12-29 20:14:57,142 - INFO - 127.0.0.1 - - [29/Dec/2024 20:14:57] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-29 20:14:57,175 - INFO - 127.0.0.1 - - [29/Dec/2024 20:14:57] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-29 20:15:06,678 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Length', b'986'), (b'Content-Type', b'application/json'), (b'apim-request-id', b'47832375-2b87-4661-a5db-f815b7cec7f8'), (b'Strict-Transport-Security', b'max-age=31536000; includeSubDomains; preload'), (b'x-content-type-options', b'nosniff'), (b'x-ratelimit-remaining-tokens', b'2368000'), (b'x-accel-buffering', b'no'), (b'x-ms-rai-invoked', b'true'), (b'x-request-id', b'28eed77e-6e02-4415-b7f0-3e8831b6d1c8'), (b'x-ms-region', b'East US 2'), (b'azureml-model-session', b'd091-20241220021933'), (b'x-ratelimit-remaining-requests', b'399'), (b'x-envoy-upstream-service-time', b'11368'), (b'x-ms-client-request-id', b'47832375-2b87-4661-a5db-f815b7cec7f8'), (b'Date', b'Mon, 30 Dec 2024 02:15:04 GMT')])
2024-12-29 20:15:06,679 - INFO - HTTP Request: POST https://openai-hp.openai.azure.com/openai/deployments/o1-preview/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2024-12-29 20:15:06,679 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2024-12-29 20:15:06,679 - DEBUG - receive_response_body.complete
2024-12-29 20:15:06,680 - DEBUG - response_closed.started
2024-12-29 20:15:06,680 - DEBUG - response_closed.complete
2024-12-29 20:15:06,680 - DEBUG - HTTP Response: POST https://openai-hp.openai.azure.com/openai/deployments/o1-preview/chat/completions?api-version=2024-12-01-preview "200 OK" Headers({'content-length': '986', 'content-type': 'application/json', 'apim-request-id': '47832375-2b87-4661-a5db-f815b7cec7f8', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'x-content-type-options': 'nosniff', 'x-ratelimit-remaining-tokens': '2368000', 'x-accel-buffering': 'no', 'x-ms-rai-invoked': 'true', 'x-request-id': '28eed77e-6e02-4415-b7f0-3e8831b6d1c8', 'x-ms-region': 'East US 2', 'azureml-model-session': 'd091-20241220021933', 'x-ratelimit-remaining-requests': '399', 'x-envoy-upstream-service-time': '11368', 'x-ms-client-request-id': '47832375-2b87-4661-a5db-f815b7cec7f8', 'date': 'Mon, 30 Dec 2024 02:15:04 GMT'})
2024-12-29 20:15:06,681 - DEBUG - request_id: 28eed77e-6e02-4415-b7f0-3e8831b6d1c8
2024-12-29 20:15:06,684 - INFO - Received response: <p>Thank you for sharing <strong>Phase 4</strong> of your project! It provides a comprehensive plan for integrating advanced AI capabilities and enhancing conversation management in your chat application. </p>

<p>Would you like assistance with:</p>

<ul>
<li><strong>Implementing any specific components outlined in Phase 4?</strong></li>
<li><strong>Moving on to outline or develop Phase 5?</strong></li>
<li><strong>Clarifying any of the features or requirements listed?</strong></li>
</ul>

<p>Feel free to let me know how I can help you proceed!</p>
<div class="shortcuts"><button onclick="copyToClipboard()">Copy</button><button onclick="retryMessage()">Retry</button></div>
2024-12-29 20:15:06,685 - DEBUG - Response being sent to chat interface: <p>Thank you for sharing <strong>Phase 4</strong> of your project! It provides a comprehensive plan for integrating advanced AI capabilities and enhancing conversation management in your chat application. </p>

<p>Would you like assistance with:</p>

<ul>
<li><strong>Implementing any specific components outlined in Phase 4?</strong></li>
<li><strong>Moving on to outline or develop Phase 5?</strong></li>
<li><strong>Clarifying any of the features or requirements listed?</strong></li>
</ul>

<p>Feel free to let me know how I can help you proceed!</p>
<div class="shortcuts"><button onclick="copyToClipboard()">Copy</button><button onclick="retryMessage()">Retry</button></div>
2024-12-29 20:15:06,685 - DEBUG - Raw API response: ChatCompletion(id='chatcmpl-AjzawctoYmy1MfGPWDAkgdVehsROw', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Thank you for sharing **Phase 4** of your project! It provides a comprehensive plan for integrating advanced AI capabilities and enhancing conversation management in your chat application. \n\nWould you like assistance with:\n\n- **Implementing any specific components outlined in Phase 4?**\n- **Moving on to outline or develop Phase 5?**\n- **Clarifying any of the features or requirements listed?**\n\nFeel free to let me know how I can help you proceed!', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None), content_filter_results={})], created=1735524894, model='o1-preview-2024-09-12', object='chat.completion', service_tier=None, system_fingerprint='fp_772cb6e46d', usage=CompletionUsage(completion_tokens=680, prompt_tokens=1433, total_tokens=2113, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=576, rejected_prediction_tokens=None), prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=0)), prompt_filter_results=[{'prompt_index': 0, 'content_filter_results': {}}])
2024-12-29 20:15:06,686 - DEBUG - Usage: CompletionUsage(completion_tokens=680, prompt_tokens=1433, total_tokens=2113, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=576, rejected_prediction_tokens=None), prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=0))
2024-12-29 20:15:06,686 - INFO - 127.0.0.1 - - [29/Dec/2024 20:15:06] "POST /chat HTTP/1.1" 200 -
2024-12-29 20:15:16,134 - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-12-29 20:15:16,134 - INFO - [33mPress CTRL+C to quit[0m
2024-12-29 20:15:16,136 - INFO -  * Restarting with stat
2024-12-29 20:15:18,200 - WARNING -  * Debugger is active!
2024-12-29 20:15:18,207 - INFO -  * Debugger PIN: 995-560-430
2024-12-29 20:15:18,281 - INFO - 127.0.0.1 - - [29/Dec/2024 20:15:18] "[32mGET /chat_interface HTTP/1.1[0m" 302 -
2024-12-29 20:15:18,294 - INFO - 127.0.0.1 - - [29/Dec/2024 20:15:18] "GET /login?next=/chat_interface HTTP/1.1" 200 -
2024-12-29 20:15:21,851 - INFO - 127.0.0.1 - - [29/Dec/2024 20:15:21] "[32mPOST /login HTTP/1.1[0m" 302 -
2024-12-29 20:15:21,859 - INFO - 127.0.0.1 - - [29/Dec/2024 20:15:21] "GET /chat_interface HTTP/1.1" 200 -
2024-12-29 20:15:21,968 - INFO - 127.0.0.1 - - [29/Dec/2024 20:15:21] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-29 20:19:04,380 - INFO - 127.0.0.1 - - [29/Dec/2024 20:19:04] "GET /chat_interface HTTP/1.1" 200 -
2024-12-29 20:19:04,418 - INFO - 127.0.0.1 - - [29/Dec/2024 20:19:04] "GET /static/css/style.css HTTP/1.1" 200 -
2024-12-29 20:19:12,474 - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-12-29 20:19:12,475 - INFO - [33mPress CTRL+C to quit[0m
2024-12-29 20:19:12,476 - INFO -  * Restarting with stat
2024-12-29 20:19:14,752 - WARNING -  * Debugger is active!
2024-12-29 20:19:14,760 - INFO -  * Debugger PIN: 995-560-430
2024-12-29 20:20:54,164 - INFO -  * Detected change in 'C:\\Users\\htper\\OneDrive\\Folder\\chats\\chatapp\\app.py', reloading
2024-12-29 20:20:54,304 - INFO -  * Restarting with stat
2024-12-29 20:20:56,440 - WARNING -  * Debugger is active!
2024-12-29 20:20:56,448 - INFO -  * Debugger PIN: 995-560-430
2024-12-29 20:21:10,351 - INFO -  * Detected change in 'C:\\Users\\htper\\OneDrive\\Folder\\chats\\chatapp\\app.py', reloading
2024-12-29 20:21:10,461 - INFO -  * Restarting with stat
2024-12-29 20:21:12,578 - WARNING -  * Debugger is active!
2024-12-29 20:21:12,586 - INFO -  * Debugger PIN: 995-560-430
2024-12-29 20:21:27,498 - INFO -  * Detected change in 'C:\\Users\\htper\\OneDrive\\Folder\\chats\\chatapp\\app.py', reloading
2024-12-29 20:21:27,611 - INFO -  * Restarting with stat
2024-12-29 20:21:29,772 - WARNING -  * Debugger is active!
2024-12-29 20:21:29,781 - INFO -  * Debugger PIN: 995-560-430
2024-12-29 20:21:43,666 - INFO -  * Detected change in 'C:\\Users\\htper\\OneDrive\\Folder\\chats\\chatapp\\app.py', reloading
2024-12-29 20:21:43,787 - INFO -  * Restarting with stat
2024-12-29 20:21:45,956 - WARNING -  * Debugger is active!
2024-12-29 20:21:45,964 - INFO -  * Debugger PIN: 995-560-430
2024-12-29 20:21:52,918 - INFO - 127.0.0.1 - - [29/Dec/2024 20:21:52] "[32mPOST /chat HTTP/1.1[0m" 302 -
2024-12-29 20:21:52,929 - INFO - 127.0.0.1 - - [29/Dec/2024 20:21:52] "GET /login?next=/chat HTTP/1.1" 200 -
2024-12-29 20:21:54,663 - INFO - 127.0.0.1 - - [29/Dec/2024 20:21:54] "[32mGET /chat_interface HTTP/1.1[0m" 302 -
2024-12-29 20:21:54,668 - INFO - 127.0.0.1 - - [29/Dec/2024 20:21:54] "GET /login?next=/chat_interface HTTP/1.1" 200 -
2024-12-29 20:21:59,321 - INFO - 127.0.0.1 - - [29/Dec/2024 20:21:59] "[32mPOST /login HTTP/1.1[0m" 302 -
2024-12-29 20:21:59,330 - INFO - 127.0.0.1 - - [29/Dec/2024 20:21:59] "GET /chat_interface HTTP/1.1" 200 -
2024-12-29 20:21:59,474 - INFO - 127.0.0.1 - - [29/Dec/2024 20:21:59] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-29 20:22:01,664 - DEBUG - Received chat message
2024-12-29 20:22:01,664 - DEBUG - Chat ID: 036523a0-e0ab-404f-88be-3c21caa29968
2024-12-29 20:22:01,665 - INFO - Processing user message: Tewst
2024-12-29 20:22:01,665 - DEBUG - Full message data: {'chat_id': '036523a0-e0ab-404f-88be-3c21caa29968', 'message': 'Tewst'}
2024-12-29 20:22:01,665 - DEBUG - Sending request with 2 messages
2024-12-29 20:22:01,666 - DEBUG - Using deployment: o1-preview
2024-12-29 20:22:01,666 - DEBUG - Payload being sent to API: [{'role': 'user', 'content': 'Please format your responses in Markdown.'}, {'role': 'user', 'content': 'Tewst'}]
2024-12-29 20:22:01,666 - DEBUG - Exact file content included in payload: []
2024-12-29 20:22:01,666 - DEBUG - API Endpoint: https://openai-hp.openai.azure.com/
2024-12-29 20:22:01,667 - DEBUG - Model Deployment Name: o1-preview
2024-12-29 20:22:01,667 - DEBUG - Message 0: {'role': 'user', 'content': 'Please format your responses in Markdown.'}
2024-12-29 20:22:01,667 - DEBUG - Message 1: {'role': 'user', 'content': 'Tewst'}
2024-12-29 20:22:01,673 - DEBUG - Request options: {'method': 'post', 'url': '/deployments/o1-preview/chat/completions', 'headers': {'api-key': '<redacted>'}, 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': 'Please format your responses in Markdown.'}, {'role': 'user', 'content': 'Tewst'}], 'model': 'o1-preview', 'max_completion_tokens': 32000, 'response_format': {'type': 'text'}, 'temperature': 1}}
2024-12-29 20:22:01,774 - DEBUG - Sending HTTP Request: POST https://openai-hp.openai.azure.com/openai/deployments/o1-preview/chat/completions?api-version=2024-12-01-preview
2024-12-29 20:22:01,775 - DEBUG - connect_tcp.started host='openai-hp.openai.azure.com' port=443 local_address=None timeout=5.0 socket_options=None
2024-12-29 20:22:01,856 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001C1548F5BE0>
2024-12-29 20:22:01,856 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x000001C15482E570> server_hostname='openai-hp.openai.azure.com' timeout=5.0
2024-12-29 20:22:01,938 - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001C1548179D0>
2024-12-29 20:22:01,939 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2024-12-29 20:22:01,939 - DEBUG - send_request_headers.complete
2024-12-29 20:22:01,939 - DEBUG - send_request_body.started request=<Request [b'POST']>
2024-12-29 20:22:01,940 - DEBUG - send_request_body.complete
2024-12-29 20:22:01,940 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2024-12-29 20:22:04,216 - INFO -  * Detected change in 'C:\\Users\\htper\\OneDrive\\Folder\\chats\\chatapp\\app.py', reloading
2024-12-29 20:22:04,382 - INFO -  * Restarting with stat
2024-12-29 20:22:06,647 - WARNING -  * Debugger is active!
2024-12-29 20:22:06,656 - INFO -  * Debugger PIN: 995-560-430
2024-12-29 20:22:25,909 - INFO -  * Detected change in 'C:\\Users\\htper\\OneDrive\\Folder\\chats\\chatapp\\app.py', reloading
2024-12-29 20:22:26,021 - INFO -  * Restarting with stat
2024-12-29 20:22:28,198 - WARNING -  * Debugger is active!
2024-12-29 20:22:28,206 - INFO -  * Debugger PIN: 995-560-430
2024-12-29 20:23:22,256 - INFO -  * Detected change in 'C:\\Users\\htper\\OneDrive\\Folder\\chats\\chatapp\\app.py', reloading
2024-12-29 20:23:22,419 - INFO -  * Restarting with stat
2024-12-29 20:23:24,632 - WARNING -  * Debugger is active!
2024-12-29 20:23:24,640 - INFO -  * Debugger PIN: 995-560-430
2024-12-29 20:23:47,021 - INFO -  * Detected change in 'C:\\Users\\htper\\OneDrive\\Folder\\chats\\chatapp\\app.py', reloading
2024-12-29 20:23:47,151 - INFO -  * Restarting with stat
2024-12-29 20:23:49,294 - WARNING -  * Debugger is active!
2024-12-29 20:23:49,302 - INFO -  * Debugger PIN: 995-560-430
2024-12-29 20:24:04,246 - INFO -  * Detected change in 'C:\\Users\\htper\\OneDrive\\Folder\\chats\\chatapp\\app.py', reloading
2024-12-29 20:24:04,413 - INFO -  * Restarting with stat
2024-12-29 20:24:06,615 - WARNING -  * Debugger is active!
2024-12-29 20:24:06,624 - INFO -  * Debugger PIN: 995-560-430
2024-12-29 20:24:34,200 - INFO -  * Detected change in 'C:\\Users\\htper\\OneDrive\\Folder\\chats\\chatapp\\azure_config.py', reloading
2024-12-29 20:24:34,328 - INFO -  * Restarting with stat
2024-12-29 20:24:36,481 - WARNING -  * Debugger is active!
2024-12-29 20:24:36,489 - INFO -  * Debugger PIN: 995-560-430
2024-12-29 20:24:44,005 - INFO -  * Detected change in 'C:\\Users\\htper\\OneDrive\\Folder\\chats\\chatapp\\azure_config.py', reloading
2024-12-29 20:24:44,118 - INFO -  * Restarting with stat
2024-12-29 20:24:46,230 - WARNING -  * Debugger is active!
2024-12-29 20:24:46,238 - INFO -  * Debugger PIN: 995-560-430
2024-12-29 20:25:32,775 - INFO -  * Detected change in 'C:\\Users\\htper\\OneDrive\\Folder\\chats\\chatapp\\azure_config.py', reloading
2024-12-29 20:25:32,898 - INFO -  * Restarting with stat
2024-12-29 20:25:35,049 - WARNING -  * Debugger is active!
2024-12-29 20:25:35,058 - INFO -  * Debugger PIN: 995-560-430
2024-12-29 20:25:42,604 - INFO -  * Detected change in 'C:\\Users\\htper\\OneDrive\\Folder\\chats\\chatapp\\azure_config.py', reloading
2024-12-29 20:25:42,728 - INFO -  * Restarting with stat
2024-12-29 20:25:44,891 - WARNING -  * Debugger is active!
2024-12-29 20:25:44,899 - INFO -  * Debugger PIN: 995-560-430
2024-12-29 20:26:11,606 - INFO -  * Detected change in 'C:\\Users\\htper\\OneDrive\\Folder\\chats\\chatapp\\app.py', reloading
2024-12-29 20:26:11,734 - INFO -  * Restarting with stat
2024-12-29 20:26:14,010 - WARNING -  * Debugger is active!
2024-12-29 20:26:14,018 - INFO -  * Debugger PIN: 995-560-430
2024-12-29 20:26:16,238 - INFO -  * Detected change in 'C:\\Users\\htper\\OneDrive\\Folder\\chats\\chatapp\\app.py', reloading
2024-12-29 20:26:16,415 - INFO -  * Restarting with stat
2024-12-29 20:26:19,073 - WARNING -  * Debugger is active!
2024-12-29 20:26:19,084 - INFO -  * Debugger PIN: 995-560-430
2024-12-29 20:26:32,018 - INFO -  * Detected change in 'C:\\Users\\htper\\OneDrive\\Folder\\chats\\chatapp\\app.py', reloading
2024-12-29 20:26:32,128 - INFO -  * Restarting with stat
2024-12-29 20:26:34,263 - WARNING -  * Debugger is active!
2024-12-29 20:26:34,270 - INFO -  * Debugger PIN: 995-560-430
2024-12-29 20:26:43,911 - INFO -  * Detected change in 'C:\\Users\\htper\\OneDrive\\Folder\\chats\\chatapp\\app.py', reloading
2024-12-29 20:26:44,061 - INFO -  * Restarting with stat
2024-12-29 20:26:46,426 - WARNING -  * Debugger is active!
2024-12-29 20:26:46,433 - INFO -  * Debugger PIN: 995-560-430
2024-12-29 20:27:20,333 - INFO -  * Detected change in 'C:\\Users\\htper\\OneDrive\\Folder\\chats\\chatapp\\conversation_manager.py', reloading
2024-12-29 20:27:20,528 - INFO -  * Restarting with stat
2024-12-29 20:27:22,957 - WARNING -  * Debugger is active!
2024-12-29 20:27:22,965 - INFO -  * Debugger PIN: 995-560-430
2024-12-29 20:27:29,427 - INFO -  * Detected change in 'C:\\Users\\htper\\OneDrive\\Folder\\chats\\chatapp\\conversation_manager.py', reloading
2024-12-29 20:27:29,552 - INFO -  * Restarting with stat
2024-12-29 20:27:31,716 - WARNING -  * Debugger is active!
2024-12-29 20:27:31,723 - INFO -  * Debugger PIN: 995-560-430
2024-12-29 20:27:38,175 - INFO -  * Detected change in 'C:\\Users\\htper\\OneDrive\\Folder\\chats\\chatapp\\conversation_manager.py', reloading
2024-12-29 20:27:38,328 - INFO -  * Restarting with stat
2024-12-29 20:27:40,512 - WARNING -  * Debugger is active!
2024-12-29 20:27:40,519 - INFO -  * Debugger PIN: 995-560-430
2024-12-29 20:27:45,952 - INFO -  * Detected change in 'C:\\Users\\htper\\OneDrive\\Folder\\chats\\chatapp\\conversation_manager.py', reloading
2024-12-29 20:27:46,095 - INFO -  * Restarting with stat
2024-12-29 20:27:48,231 - WARNING -  * Debugger is active!
2024-12-29 20:27:48,239 - INFO -  * Debugger PIN: 995-560-430
2024-12-29 20:28:08,570 - INFO -  * Detected change in 'C:\\Users\\htper\\OneDrive\\Folder\\chats\\chatapp\\app.py', reloading
2024-12-29 20:28:08,730 - INFO -  * Restarting with stat
2024-12-29 20:28:10,980 - WARNING -  * Debugger is active!
2024-12-29 20:28:10,988 - INFO -  * Debugger PIN: 995-560-430
2024-12-29 20:28:27,105 - INFO -  * Detected change in 'C:\\Users\\htper\\OneDrive\\Folder\\chats\\chatapp\\app.py', reloading
2024-12-29 20:28:27,274 - INFO -  * Restarting with stat
2024-12-29 20:28:29,496 - WARNING -  * Debugger is active!
2024-12-29 20:28:29,505 - INFO -  * Debugger PIN: 995-560-430
2024-12-29 20:51:39,357 - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-12-29 20:51:39,357 - INFO - [33mPress CTRL+C to quit[0m
2024-12-29 20:51:39,359 - INFO -  * Restarting with stat
2024-12-29 20:51:41,677 - WARNING -  * Debugger is active!
2024-12-29 20:51:41,687 - INFO -  * Debugger PIN: 995-560-430
2024-12-29 20:51:42,621 - INFO - 127.0.0.1 - - [29/Dec/2024 20:51:42] "[32mGET /chat_interface HTTP/1.1[0m" 302 -
2024-12-29 20:51:42,643 - INFO - 127.0.0.1 - - [29/Dec/2024 20:51:42] "GET /login?next=/chat_interface HTTP/1.1" 200 -
2024-12-29 20:51:47,708 - INFO - 127.0.0.1 - - [29/Dec/2024 20:51:47] "[32mPOST /login HTTP/1.1[0m" 302 -
2024-12-29 20:51:47,748 - INFO - 127.0.0.1 - - [29/Dec/2024 20:51:47] "[35m[1mGET /chat_interface HTTP/1.1[0m" 500 -
2024-12-29 20:51:47,926 - INFO - 127.0.0.1 - - [29/Dec/2024 20:51:47] "GET /chat_interface?__debugger__=yes&cmd=resource&f=style.css HTTP/1.1" 200 -
2024-12-29 20:51:47,938 - INFO - 127.0.0.1 - - [29/Dec/2024 20:51:47] "GET /chat_interface?__debugger__=yes&cmd=resource&f=debugger.js HTTP/1.1" 200 -
2024-12-29 20:51:47,977 - INFO - 127.0.0.1 - - [29/Dec/2024 20:51:47] "GET /chat_interface?__debugger__=yes&cmd=resource&f=console.png&s=BqkqlGzkZOdjWtpweXwL HTTP/1.1" 200 -
2024-12-29 20:51:48,007 - INFO - 127.0.0.1 - - [29/Dec/2024 20:51:48] "GET /chat_interface?__debugger__=yes&cmd=resource&f=console.png HTTP/1.1" 200 -
2024-12-29 20:52:28,473 - INFO -  * Detected change in 'C:\\Users\\htper\\OneDrive\\Folder\\chats\\chatapp\\app.py', reloading
2024-12-29 20:52:28,591 - INFO -  * Restarting with stat
2024-12-29 20:52:30,961 - WARNING -  * Debugger is active!
2024-12-29 20:52:30,970 - INFO -  * Debugger PIN: 995-560-430
2024-12-29 20:52:52,284 - INFO -  * Detected change in 'C:\\Users\\htper\\OneDrive\\Folder\\chats\\chatapp\\app.py', reloading
2024-12-29 20:52:52,404 - INFO -  * Restarting with stat
2024-12-29 20:52:54,689 - WARNING -  * Debugger is active!
2024-12-29 20:52:54,699 - INFO -  * Debugger PIN: 995-560-430
2024-12-29 20:53:09,682 - INFO -  * Detected change in 'C:\\Users\\htper\\OneDrive\\Folder\\chats\\chatapp\\app.py', reloading
2024-12-29 20:53:09,796 - INFO -  * Restarting with stat
2024-12-29 20:53:12,241 - WARNING -  * Debugger is active!
2024-12-29 20:53:12,252 - INFO -  * Debugger PIN: 995-560-430
2024-12-29 20:53:19,820 - INFO -  * Detected change in 'C:\\Users\\htper\\OneDrive\\Folder\\chats\\chatapp\\app.py', reloading
2024-12-29 20:53:19,974 - INFO -  * Restarting with stat
2024-12-29 20:53:22,639 - WARNING -  * Debugger is active!
2024-12-29 20:53:22,647 - INFO -  * Debugger PIN: 995-560-430
2024-12-29 20:53:36,632 - INFO -  * Detected change in 'C:\\Users\\htper\\OneDrive\\Folder\\chats\\chatapp\\app.py', reloading
2024-12-29 20:53:36,782 - INFO -  * Restarting with stat
2024-12-29 20:53:39,149 - WARNING -  * Debugger is active!
2024-12-29 20:53:39,157 - INFO -  * Debugger PIN: 995-560-430
2024-12-29 20:54:23,792 - INFO -  * Detected change in 'C:\\Users\\htper\\OneDrive\\Folder\\chats\\chatapp\\routes\\auth_routes.py', reloading
2024-12-29 20:54:23,964 - INFO -  * Restarting with stat
2024-12-29 20:54:26,756 - WARNING -  * Debugger is active!
2024-12-29 20:54:26,764 - INFO -  * Debugger PIN: 995-560-430
2024-12-29 20:54:43,836 - INFO -  * Detected change in 'C:\\Users\\htper\\OneDrive\\Folder\\chats\\chatapp\\models.py', reloading
2024-12-29 20:54:44,021 - INFO -  * Restarting with stat
2024-12-29 20:54:46,394 - WARNING -  * Debugger is active!
2024-12-29 20:54:46,405 - INFO -  * Debugger PIN: 995-560-430
2024-12-29 20:54:51,847 - INFO -  * Detected change in 'C:\\Users\\htper\\OneDrive\\Folder\\chats\\chatapp\\database.py', reloading
2024-12-29 20:54:52,004 - INFO -  * Restarting with stat
2024-12-29 20:54:54,565 - WARNING -  * Debugger is active!
2024-12-29 20:54:54,573 - INFO -  * Debugger PIN: 995-560-430
2024-12-29 20:56:26,950 - INFO -  * Detected change in 'C:\\Users\\htper\\OneDrive\\Folder\\chats\\chatapp\\routes\\auth_routes.py', reloading
2024-12-29 20:56:27,066 - INFO -  * Restarting with stat
2024-12-29 20:56:29,652 - WARNING -  * Debugger is active!
2024-12-29 20:56:29,660 - INFO -  * Debugger PIN: 995-560-430
2024-12-29 20:57:03,867 - INFO -  * Detected change in 'C:\\Users\\htper\\OneDrive\\Folder\\chats\\chatapp\\routes\\chat_routes.py', reloading
2024-12-29 20:57:04,024 - INFO -  * Restarting with stat
2024-12-29 20:57:06,664 - WARNING -  * Debugger is active!
2024-12-29 20:57:06,672 - INFO -  * Debugger PIN: 995-560-430
2024-12-29 20:57:30,432 - INFO -  * Detected change in 'C:\\Users\\htper\\OneDrive\\Folder\\chats\\chatapp\\routes\\chat_routes.py', reloading
2024-12-29 20:57:30,591 - INFO -  * Restarting with stat
2024-12-29 20:57:33,049 - WARNING -  * Debugger is active!
2024-12-29 20:57:33,056 - INFO -  * Debugger PIN: 995-560-430
2024-12-29 20:57:49,123 - INFO -  * Detected change in 'C:\\Users\\htper\\OneDrive\\Folder\\chats\\chatapp\\routes\\chat_routes.py', reloading
2024-12-29 20:57:49,251 - INFO -  * Restarting with stat
2024-12-29 20:57:51,653 - WARNING -  * Debugger is active!
2024-12-29 20:57:51,661 - INFO -  * Debugger PIN: 995-560-430
2024-12-29 20:58:13,237 - INFO -  * Detected change in 'C:\\Users\\htper\\OneDrive\\Folder\\chats\\chatapp\\routes\\chat_routes.py', reloading
2024-12-29 20:58:13,359 - INFO -  * Restarting with stat
2024-12-29 20:58:15,881 - WARNING -  * Debugger is active!
2024-12-29 20:58:15,890 - INFO -  * Debugger PIN: 995-560-430
2024-12-29 20:58:19,888 - INFO - 127.0.0.1 - - [29/Dec/2024 20:58:19] "[32mGET /chat_interface HTTP/1.1[0m" 302 -
2024-12-29 20:58:19,908 - INFO - 127.0.0.1 - - [29/Dec/2024 20:58:19] "GET /login?next=/chat_interface HTTP/1.1" 200 -
2024-12-29 20:58:24,987 - INFO - 127.0.0.1 - - [29/Dec/2024 20:58:24] "[32mPOST /login HTTP/1.1[0m" 302 -
2024-12-29 20:58:24,993 - DEBUG - Added message to chat 4e449585-94f7-4313-ac8c-cd2a4d165952: user: Please format your responses in Markdown....
2024-12-29 20:58:25,047 - INFO - 127.0.0.1 - - [29/Dec/2024 20:58:25] "GET /chat_interface HTTP/1.1" 200 -
2024-12-29 20:58:25,254 - INFO - 127.0.0.1 - - [29/Dec/2024 20:58:25] "GET /static/css/style.css HTTP/1.1" 200 -
2024-12-29 20:58:26,734 - INFO -  * Detected change in 'C:\\Users\\htper\\OneDrive\\Folder\\chats\\chatapp\\routes\\chat_routes.py', reloading
2024-12-29 20:58:26,921 - INFO -  * Restarting with stat
2024-12-29 20:58:29,588 - WARNING -  * Debugger is active!
2024-12-29 20:58:29,595 - INFO -  * Debugger PIN: 995-560-430
2024-12-29 20:58:34,558 - INFO - 127.0.0.1 - - [29/Dec/2024 20:58:34] "[32mPOST /chat HTTP/1.1[0m" 302 -
2024-12-29 20:58:34,568 - INFO - 127.0.0.1 - - [29/Dec/2024 20:58:34] "GET /login?next=/chat HTTP/1.1" 200 -
2024-12-29 20:58:45,633 - INFO -  * Detected change in 'C:\\Users\\htper\\OneDrive\\Folder\\chats\\chatapp\\routes\\chat_routes.py', reloading
2024-12-29 20:58:45,759 - INFO -  * Restarting with stat
2024-12-29 20:58:48,461 - WARNING -  * Debugger is active!
2024-12-29 20:58:48,469 - INFO -  * Debugger PIN: 995-560-430
2024-12-29 20:58:58,125 - INFO -  * Detected change in 'C:\\Users\\htper\\OneDrive\\Folder\\chats\\chatapp\\routes\\chat_routes.py', reloading
2024-12-29 20:58:58,252 - INFO -  * Restarting with stat
2024-12-29 20:59:01,019 - WARNING -  * Debugger is active!
2024-12-29 20:59:01,027 - INFO -  * Debugger PIN: 995-560-430
2024-12-29 20:59:08,541 - INFO -  * Detected change in 'C:\\Users\\htper\\OneDrive\\Folder\\chats\\chatapp\\routes\\chat_routes.py', reloading
2024-12-29 20:59:08,708 - INFO -  * Restarting with stat
2024-12-29 20:59:11,255 - WARNING -  * Debugger is active!
2024-12-29 20:59:11,263 - INFO -  * Debugger PIN: 995-560-430
2024-12-29 21:00:34,041 - INFO -  * Detected change in 'C:\\Users\\htper\\OneDrive\\Folder\\chats\\chatapp\\routes\\chat_routes.py', reloading
2024-12-29 21:00:34,147 - INFO -  * Restarting with stat
2024-12-29 21:00:36,438 - WARNING -  * Debugger is active!
2024-12-29 21:00:36,446 - INFO -  * Debugger PIN: 995-560-430
2024-12-29 21:00:56,672 - INFO -  * Detected change in 'C:\\Users\\htper\\OneDrive\\Folder\\chats\\chatapp\\routes\\chat_routes.py', reloading
2024-12-29 21:00:56,845 - INFO -  * Restarting with stat
2024-12-29 21:00:59,581 - WARNING -  * Debugger is active!
2024-12-29 21:00:59,589 - INFO -  * Debugger PIN: 995-560-430
2024-12-29 21:01:08,136 - INFO -  * Detected change in 'C:\\Users\\htper\\OneDrive\\Folder\\chats\\chatapp\\routes\\chat_routes.py', reloading
2024-12-29 21:01:08,241 - INFO -  * Restarting with stat
2024-12-29 21:01:10,404 - WARNING -  * Debugger is active!
2024-12-29 21:01:10,412 - INFO -  * Debugger PIN: 995-560-430
2024-12-29 21:01:25,291 - INFO -  * Detected change in 'C:\\Users\\htper\\OneDrive\\Folder\\chats\\chatapp\\routes\\chat_routes.py', reloading
2024-12-29 21:01:25,409 - INFO -  * Restarting with stat
2024-12-29 21:01:27,636 - WARNING -  * Debugger is active!
2024-12-29 21:01:27,643 - INFO -  * Debugger PIN: 995-560-430
2024-12-29 21:01:42,515 - INFO -  * Detected change in 'C:\\Users\\htper\\OneDrive\\Folder\\chats\\chatapp\\database.py', reloading
2024-12-29 21:01:42,615 - INFO -  * Restarting with stat
2024-12-29 21:01:44,854 - WARNING -  * Debugger is active!
2024-12-29 21:01:44,861 - INFO -  * Debugger PIN: 995-560-430
2024-12-29 21:01:47,056 - INFO -  * Detected change in 'C:\\Users\\htper\\OneDrive\\Folder\\chats\\chatapp\\routes\\auth_routes.py', reloading
2024-12-29 21:01:47,155 - INFO -  * Restarting with stat
2024-12-29 21:01:49,391 - WARNING -  * Debugger is active!
2024-12-29 21:01:49,401 - INFO -  * Debugger PIN: 995-560-430
2024-12-29 21:01:50,557 - INFO -  * Detected change in 'C:\\Users\\htper\\OneDrive\\Folder\\chats\\chatapp\\routes\\auth_routes.py', reloading
2024-12-29 21:01:50,672 - INFO -  * Restarting with stat
2024-12-29 21:01:53,072 - WARNING -  * Debugger is active!
2024-12-29 21:01:53,081 - INFO -  * Debugger PIN: 995-560-430
2024-12-29 21:02:03,752 - INFO -  * Detected change in 'C:\\Users\\htper\\OneDrive\\Folder\\chats\\chatapp\\models.py', reloading
2024-12-29 21:02:03,856 - INFO -  * Restarting with stat
2024-12-29 21:02:06,072 - WARNING -  * Debugger is active!
2024-12-29 21:02:06,079 - INFO -  * Debugger PIN: 995-560-430
2024-12-30 02:28:57,565 - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-12-30 02:28:57,567 - INFO - [33mPress CTRL+C to quit[0m
2024-12-30 02:28:57,569 - INFO -  * Restarting with stat
2024-12-30 02:28:59,793 - WARNING -  * Debugger is active!
2024-12-30 02:28:59,802 - INFO -  * Debugger PIN: 122-746-019
2024-12-30 02:29:07,891 - INFO - 127.0.0.1 - - [30/Dec/2024 02:29:07] "[32mGET / HTTP/1.1[0m" 302 -
2024-12-30 02:29:07,928 - INFO - 127.0.0.1 - - [30/Dec/2024 02:29:07] "GET /login?next=/ HTTP/1.1" 200 -
2024-12-30 02:29:08,155 - INFO - 127.0.0.1 - - [30/Dec/2024 02:29:08] "[33mGET /favicon.ico HTTP/1.1[0m" 404 -
2024-12-30 02:29:12,167 - INFO - 127.0.0.1 - - [30/Dec/2024 02:29:12] "[32mPOST /login HTTP/1.1[0m" 302 -
2024-12-30 02:29:12,358 - DEBUG - Added message to chat b9760302-bb33-453e-a727-d49268634336: user: Please format your responses in Markdown....
2024-12-30 02:29:12,395 - INFO - 127.0.0.1 - - [30/Dec/2024 02:29:12] "GET /chat_interface HTTP/1.1" 200 -
2024-12-30 02:29:12,828 - INFO - 127.0.0.1 - - [30/Dec/2024 02:29:12] "GET /static/css/style.css HTTP/1.1" 200 -
2024-12-30 02:29:15,542 - DEBUG - Received chat message
2024-12-30 02:29:15,542 - DEBUG - Chat ID: b9760302-bb33-453e-a727-d49268634336
2024-12-30 02:29:15,543 - INFO - Processing user message: Test
2024-12-30 02:29:15,544 - DEBUG - Full message data: {'chat_id': 'b9760302-bb33-453e-a727-d49268634336', 'message': 'Test'}
2024-12-30 02:29:15,544 - DEBUG - Sending request with 2 messages
2024-12-30 02:29:15,545 - DEBUG - Using deployment: o1-preview
2024-12-30 02:29:15,545 - DEBUG - Payload being sent to API: [{'role': 'user', 'content': 'Please format your responses in Markdown.'}, {'role': 'user', 'content': 'Test'}]
2024-12-30 02:29:15,546 - DEBUG - Exact file content included in payload: []
2024-12-30 02:29:15,546 - DEBUG - API Endpoint: https://openai-hp.openai.azure.com/openai/
2024-12-30 02:29:15,547 - DEBUG - Model Deployment Name: o1-preview
2024-12-30 02:29:15,547 - DEBUG - Message 0: {'role': 'user', 'content': 'Please format your responses in Markdown.'}
2024-12-30 02:29:15,548 - DEBUG - Message 1: {'role': 'user', 'content': 'Test'}
2024-12-30 02:29:15,552 - DEBUG - Request options: {'method': 'post', 'url': '/deployments/o1-preview/chat/completions', 'headers': {'api-key': '<redacted>'}, 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': 'Please format your responses in Markdown.'}, {'role': 'user', 'content': 'Test'}], 'model': 'o1-preview', 'max_completion_tokens': 32000, 'response_format': {'type': 'text'}, 'temperature': 1}}
2024-12-30 02:29:15,641 - DEBUG - Sending HTTP Request: POST https://openai-hp.openai.azure.com/openai/deployments/o1-preview/chat/completions?api-version=2024-12-01-preview
2024-12-30 02:29:15,642 - DEBUG - connect_tcp.started host='openai-hp.openai.azure.com' port=443 local_address=None timeout=5.0 socket_options=None
2024-12-30 02:29:15,721 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001BDE0E0AF90>
2024-12-30 02:29:15,722 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x000001BDE0D527B0> server_hostname='openai-hp.openai.azure.com' timeout=5.0
2024-12-30 02:29:15,805 - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001BDE0E3D090>
2024-12-30 02:29:15,806 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2024-12-30 02:29:15,806 - DEBUG - send_request_headers.complete
2024-12-30 02:29:15,806 - DEBUG - send_request_body.started request=<Request [b'POST']>
2024-12-30 02:29:15,807 - DEBUG - send_request_body.complete
2024-12-30 02:29:15,807 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2024-12-30 02:29:21,653 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Length', b'530'), (b'Content-Type', b'application/json'), (b'apim-request-id', b'780df9ef-a4cf-466c-a4ac-1f94839c6b84'), (b'Strict-Transport-Security', b'max-age=31536000; includeSubDomains; preload'), (b'x-content-type-options', b'nosniff'), (b'x-ratelimit-remaining-tokens', b'2368000'), (b'x-accel-buffering', b'no'), (b'x-ms-rai-invoked', b'true'), (b'x-request-id', b'3bcfd03f-99f4-46fb-99dc-6a321f23f4e4'), (b'x-ms-region', b'East US 2'), (b'azureml-model-session', b'd091-20241220021933'), (b'x-ratelimit-remaining-requests', b'399'), (b'x-envoy-upstream-service-time', b'5768'), (b'x-ms-client-request-id', b'780df9ef-a4cf-466c-a4ac-1f94839c6b84'), (b'Date', b'Mon, 30 Dec 2024 08:29:20 GMT')])
2024-12-30 02:29:21,654 - INFO - HTTP Request: POST https://openai-hp.openai.azure.com/openai/deployments/o1-preview/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2024-12-30 02:29:21,654 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2024-12-30 02:29:21,654 - DEBUG - receive_response_body.complete
2024-12-30 02:29:21,655 - DEBUG - response_closed.started
2024-12-30 02:29:21,655 - DEBUG - response_closed.complete
2024-12-30 02:29:21,655 - DEBUG - HTTP Response: POST https://openai-hp.openai.azure.com/openai/deployments/o1-preview/chat/completions?api-version=2024-12-01-preview "200 OK" Headers({'content-length': '530', 'content-type': 'application/json', 'apim-request-id': '780df9ef-a4cf-466c-a4ac-1f94839c6b84', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'x-content-type-options': 'nosniff', 'x-ratelimit-remaining-tokens': '2368000', 'x-accel-buffering': 'no', 'x-ms-rai-invoked': 'true', 'x-request-id': '3bcfd03f-99f4-46fb-99dc-6a321f23f4e4', 'x-ms-region': 'East US 2', 'azureml-model-session': 'd091-20241220021933', 'x-ratelimit-remaining-requests': '399', 'x-envoy-upstream-service-time': '5768', 'x-ms-client-request-id': '780df9ef-a4cf-466c-a4ac-1f94839c6b84', 'date': 'Mon, 30 Dec 2024 08:29:20 GMT'})
2024-12-30 02:29:21,656 - DEBUG - request_id: 3bcfd03f-99f4-46fb-99dc-6a321f23f4e4
2024-12-30 02:29:21,659 - INFO - API response received successfully
2024-12-30 02:29:21,659 - DEBUG - Response content: Test
2024-12-30 02:29:21,659 - DEBUG - Token usage - Prompt: 27, Completion: 398, Total: 425
2024-12-30 02:29:21,659 - DEBUG - Response finish reason: stop
2024-12-30 02:29:21,662 - INFO - Received response: <p>Test</p>
<div class="shortcuts"><button onclick="copyToClipboard()">Copy</button><button onclick="retryMessage()">Retry</button></div>
2024-12-30 02:29:21,662 - DEBUG - Response being sent to chat interface: <p>Test</p>
<div class="shortcuts"><button onclick="copyToClipboard()">Copy</button><button onclick="retryMessage()">Retry</button></div>
2024-12-30 02:29:21,663 - DEBUG - Raw API response: ChatCompletion(id='chatcmpl-Ak5RCT6xzDlVCFJxVFI2p4P3zWPC0', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Test', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None), content_filter_results={})], created=1735547354, model='o1-preview-2024-09-12', object='chat.completion', service_tier=None, system_fingerprint='fp_772cb6e46d', usage=CompletionUsage(completion_tokens=398, prompt_tokens=27, total_tokens=425, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=384, rejected_prediction_tokens=None), prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=0)), prompt_filter_results=[{'prompt_index': 0, 'content_filter_results': {}}])
2024-12-30 02:29:21,663 - DEBUG - Usage: CompletionUsage(completion_tokens=398, prompt_tokens=27, total_tokens=425, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=384, rejected_prediction_tokens=None), prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=0))
2024-12-30 02:29:21,663 - DEBUG - Cleared context for chat b9760302-bb33-453e-a727-d49268634336
2024-12-30 02:29:21,664 - DEBUG - Added message to chat b9760302-bb33-453e-a727-d49268634336: user: Please format your responses in Markdown....
2024-12-30 02:29:21,664 - DEBUG - Added message to chat b9760302-bb33-453e-a727-d49268634336: user: Test...
2024-12-30 02:29:21,664 - DEBUG - Added message to chat b9760302-bb33-453e-a727-d49268634336: assistant: <p>Test</p>
<div class="shortcuts"><button onclick...
2024-12-30 02:29:21,665 - INFO - 127.0.0.1 - - [30/Dec/2024 02:29:21] "POST /chat HTTP/1.1" 200 -
2024-12-30 02:29:36,984 - DEBUG - Received file upload request
2024-12-30 02:29:36,985 - DEBUG - Processing uploaded files
2024-12-30 02:29:36,986 - DEBUG - Uploads directory ensured
2024-12-30 02:29:36,987 - INFO - File saved successfully: uploads\AI Chat Enhancement Phase 2.md
2024-12-30 02:29:36,988 - DEBUG - File size: 7274 bytes
2024-12-30 02:29:36,988 - INFO - Files successfully uploaded: ['AI Chat Enhancement Phase 2.md']
2024-12-30 02:29:36,989 - INFO - 127.0.0.1 - - [30/Dec/2024 02:29:36] "POST /upload HTTP/1.1" 200 -
2024-12-30 02:35:20,046 - DEBUG - Received chat message
2024-12-30 02:35:20,046 - DEBUG - Chat ID: b9760302-bb33-453e-a727-d49268634336
2024-12-30 02:35:20,047 - INFO - Processing user message: Test
2024-12-30 02:35:20,047 - DEBUG - Full message data: {'chat_id': 'b9760302-bb33-453e-a727-d49268634336', 'message': 'Test'}
2024-12-30 02:35:20,047 - DEBUG - Analyzing file: uploads\AI Chat Enhancement Phase 2.md
2024-12-30 02:35:20,048 - DEBUG - Sending request with 4 messages
2024-12-30 02:35:20,048 - DEBUG - Using deployment: o1-preview
2024-12-30 02:35:20,048 - DEBUG - Payload being sent to API: [{'role': 'user', 'content': 'Please format your responses in Markdown.'}, {'role': 'user', 'content': 'Test'}, {'role': 'user', 'content': 'Test'}, {'role': 'user', 'content': 'File content:\n```\nYes, absolutely! Adding the ability to use any OpenAI format-compatible LLM is a natural fit for **Phase 2: AI-Powered Chat - Enhancement** and complements the addition of multiple Azure OpenAI deployments. It expands the AI capabilities even further, making the application more versatile.\n\nHere\'s how you can incorporate this into Phase 2, building on the previous modifications for multiple Azure OpenAI deployments:\n\n**1. Generalized Configuration:**\n\n*   **Flexible Endpoints:** Instead of just specifying "Azure OpenAI endpoints," generalize the configuration to handle any OpenAI-compatible endpoint. This means the configuration should accommodate different base URLs for different providers.\n*   **API Key Handling:**  The configuration should continue to support API keys, as most OpenAI-compatible services will require them.\n*   **Model/Deployment Name:**  The configuration should handle either "model names" (common for many providers) or "deployment names" (used by Azure OpenAI). You might need a field to specify which one to use, or you could try to infer it based on the provider.\n\n**Example Configuration (Environment Variables):**\n\n```\n# Azure OpenAI Deployment\nPROJECT_1_PROVIDER=azure\nPROJECT_1_ENDPOINT=your_azure_endpoint_for_project_1\nPROJECT_1_API_KEY=your_azure_api_key_for_project_1\nPROJECT_1_DEPLOYMENT_NAME=your_azure_deployment_name_for_project_1\n\n# Another Provider (e.g., a self-hosted LLM)\nPROJECT_2_PROVIDER=openai_compatible\nPROJECT_2_ENDPOINT=http://your-llm-server:8000/v1\nPROJECT_2_API_KEY=your_llm_api_key\nPROJECT_2_MODEL_NAME=your_llm_model_name\n\n# Default (can be Azure or any other provider)\nDEFAULT_PROVIDER=azure\nDEFAULT_ENDPOINT=your_default_azure_endpoint\nDEFAULT_API_KEY=your_default_azure_api_key\nDEFAULT_DEPLOYMENT_NAME=your_default_azure_deployment_name\n```\n\n**Example Configuration (JSON):**\n\n```json\n{\n  "project_deployments": {\n    "project_1_id": {\n      "provider": "azure",\n      "endpoint": "your_azure_endpoint_for_project_1",\n      "api_key": "your_azure_api_key_for_project_1",\n      "deployment_name": "your_azure_deployment_name_for_project_1"\n    },\n    "project_2_id": {\n      "provider": "openai_compatible",\n      "endpoint": "http://your-llm-server:8000/v1",\n      "api_key": "your_llm_api_key",\n      "model_name": "your_llm_model_name"\n    }\n  },\n  "default_deployment": {\n    "provider": "azure",\n    "endpoint": "your_default_azure_endpoint",\n    "api_key": "your_default_azure_api_key",\n    "deployment_name": "your_default_azure_deployment_name"\n  }\n}\n```\n\n**2. Enhanced `AIAssistant`:**\n\n*   **Provider-Specific Logic:**  Modify the `AIAssistant` class to handle different providers. You might need to introduce a "provider" field (e.g., "azure," "openai\\_compatible") to determine how to construct the API request.\n*   **Client Library Handling:** If you are using the official OpenAI Python client library, you\'ll likely need to create separate client instances for Azure OpenAI and other providers, as the configuration (endpoint, API key) will be different. You might consider creating a factory method within `AIAssistant` to create the appropriate client based on the provider.\n*   **Unified Interface:**  Despite the differences in how API calls are made, ensure that the `get_ai_response` method still provides a consistent interface, returning the AI\'s response in the same format regardless of the underlying provider.\n\n**Example Conceptual Code (Phase 2 - Python):**\n\n```python\nimport openai\n\nclass AIAssistant:\n    def __init__(self, deployment_config=None):\n        self.deployment_config = deployment_config or {}\n        self.default_deployment = self.deployment_config.get("default_deployment", {})\n\n    def _get_client(self, deployment):\n        provider = deployment.get("provider")\n        api_key = deployment.get("api_key")\n        endpoint = deployment.get("endpoint")\n\n        if provider == "azure":\n            # Configure for Azure OpenAI\n            openai.api_type = "azure"\n            openai.api_key = api_key\n            openai.api_base = endpoint\n            openai.api_version = "2023-05-15" # Or your desired API version\n            return openai\n        elif provider == "openai_compatible":\n            # Configure for other OpenAI-compatible providers\n            client = openai.OpenAI(\n                api_key=api_key,\n                base_url=endpoint\n            )\n            return client\n        else:\n            raise ValueError(f"Unsupported provider: {provider}")\n\n    def get_ai_response(self, message, conversation_history, project_id=None, language_model=\'gpt-3.5-turbo\') -> str:\n        deployment = self.default_deployment\n        if project_id:\n            project_deployment = self.deployment_config.get("project_deployments", {}).get(project_id)\n            if project_deployment:\n                deployment = project_deployment\n\n        client = self._get_client(deployment)\n\n        # Construct the messages payload for the API call\n        messages = [{"role": "system", "content": "You are a helpful assistant."}]  # Customize as needed\n        for msg in conversation_history:\n            messages.append({"role": msg["role"], "content": msg["content"]})\n        messages.append({"role": "user", "content": message})\n\n        # Use the appropriate client and parameters for the API call\n        if deployment.get("provider") == "azure":\n            # Call Azure OpenAI using deployment_name\n            response = client.chat.completions.create(\n                model=deployment.get("deployment_name"),\n                messages=messages,\n                # ... other parameters like temperature, max_tokens, etc.\n            )\n        else:\n            # Call other OpenAI-compatible providers using model_name\n            response = client.chat.completions.create(\n                model=deployment.get("model_name"),\n                messages=messages,\n                # ... other parameters\n            )\n\n        return response.choices[0].message.content\n```\n\n**3. ProjectManager Integration:**\n\n*   When creating or updating a project, you\'ll now need to allow the user to select a "provider" in addition to the deployment/model details.\n*   Store the provider information along with the project data.\n\n**Key Considerations:**\n\n*   **Error Handling:** Implement robust error handling to catch issues like invalid API keys, incorrect endpoints, unsupported models, and API rate limiting.\n*   **Abstraction:** Design your `AIAssistant` class in a way that abstracts away the differences between providers as much as possible. This will make it easier to add support for more providers in the future.\n*   **Testing:** Thoroughly test your implementation with different providers to ensure that your application works correctly with each one.\n*   **User Interface:** Consider how you want to expose the choice of providers and models/deployments in the user interface. You might need to add dropdowns or other UI elements to allow users to select the desired provider and model/deployment for each project.\n\nBy incorporating these changes, your Phase 2 AI enhancements will be even more powerful, allowing your chat application to leverage a wide range of OpenAI-compatible LLMs, giving you and your users greater flexibility and choice.\n\n```'}]
2024-12-30 02:35:20,049 - DEBUG - Exact file content included in payload: ['Yes, absolutely! Adding the ability to use any OpenAI format-compatible LLM is a natural fit for **Phase 2: AI-Powered Chat - Enhancement** and complements the addition of multiple Azure OpenAI deployments. It expands the AI capabilities even further, making the application more versatile.\n\nHere\'s how you can incorporate this into Phase 2, building on the previous modifications for multiple Azure OpenAI deployments:\n\n**1. Generalized Configuration:**\n\n*   **Flexible Endpoints:** Instead of just specifying "Azure OpenAI endpoints," generalize the configuration to handle any OpenAI-compatible endpoint. This means the configuration should accommodate different base URLs for different providers.\n*   **API Key Handling:**  The configuration should continue to support API keys, as most OpenAI-compatible services will require them.\n*   **Model/Deployment Name:**  The configuration should handle either "model names" (common for many providers) or "deployment names" (used by Azure OpenAI). You might need a field to specify which one to use, or you could try to infer it based on the provider.\n\n**Example Configuration (Environment Variables):**\n\n```\n# Azure OpenAI Deployment\nPROJECT_1_PROVIDER=azure\nPROJECT_1_ENDPOINT=your_azure_endpoint_for_project_1\nPROJECT_1_API_KEY=your_azure_api_key_for_project_1\nPROJECT_1_DEPLOYMENT_NAME=your_azure_deployment_name_for_project_1\n\n# Another Provider (e.g., a self-hosted LLM)\nPROJECT_2_PROVIDER=openai_compatible\nPROJECT_2_ENDPOINT=http://your-llm-server:8000/v1\nPROJECT_2_API_KEY=your_llm_api_key\nPROJECT_2_MODEL_NAME=your_llm_model_name\n\n# Default (can be Azure or any other provider)\nDEFAULT_PROVIDER=azure\nDEFAULT_ENDPOINT=your_default_azure_endpoint\nDEFAULT_API_KEY=your_default_azure_api_key\nDEFAULT_DEPLOYMENT_NAME=your_default_azure_deployment_name\n```\n\n**Example Configuration (JSON):**\n\n```json\n{\n  "project_deployments": {\n    "project_1_id": {\n      "provider": "azure",\n      "endpoint": "your_azure_endpoint_for_project_1",\n      "api_key": "your_azure_api_key_for_project_1",\n      "deployment_name": "your_azure_deployment_name_for_project_1"\n    },\n    "project_2_id": {\n      "provider": "openai_compatible",\n      "endpoint": "http://your-llm-server:8000/v1",\n      "api_key": "your_llm_api_key",\n      "model_name": "your_llm_model_name"\n    }\n  },\n  "default_deployment": {\n    "provider": "azure",\n    "endpoint": "your_default_azure_endpoint",\n    "api_key": "your_default_azure_api_key",\n    "deployment_name": "your_default_azure_deployment_name"\n  }\n}\n```\n\n**2. Enhanced `AIAssistant`:**\n\n*   **Provider-Specific Logic:**  Modify the `AIAssistant` class to handle different providers. You might need to introduce a "provider" field (e.g., "azure," "openai\\_compatible") to determine how to construct the API request.\n*   **Client Library Handling:** If you are using the official OpenAI Python client library, you\'ll likely need to create separate client instances for Azure OpenAI and other providers, as the configuration (endpoint, API key) will be different. You might consider creating a factory method within `AIAssistant` to create the appropriate client based on the provider.\n*   **Unified Interface:**  Despite the differences in how API calls are made, ensure that the `get_ai_response` method still provides a consistent interface, returning the AI\'s response in the same format regardless of the underlying provider.\n\n**Example Conceptual Code (Phase 2 - Python):**\n\n```python\nimport openai\n\nclass AIAssistant:\n    def __init__(self, deployment_config=None):\n        self.deployment_config = deployment_config or {}\n        self.default_deployment = self.deployment_config.get("default_deployment", {})\n\n    def _get_client(self, deployment):\n        provider = deployment.get("provider")\n        api_key = deployment.get("api_key")\n        endpoint = deployment.get("endpoint")\n\n        if provider == "azure":\n            # Configure for Azure OpenAI\n            openai.api_type = "azure"\n            openai.api_key = api_key\n            openai.api_base = endpoint\n            openai.api_version = "2023-05-15" # Or your desired API version\n            return openai\n        elif provider == "openai_compatible":\n            # Configure for other OpenAI-compatible providers\n            client = openai.OpenAI(\n                api_key=api_key,\n                base_url=endpoint\n            )\n            return client\n        else:\n            raise ValueError(f"Unsupported provider: {provider}")\n\n    def get_ai_response(self, message, conversation_history, project_id=None, language_model=\'gpt-3.5-turbo\') -> str:\n        deployment = self.default_deployment\n        if project_id:\n            project_deployment = self.deployment_config.get("project_deployments", {}).get(project_id)\n            if project_deployment:\n                deployment = project_deployment\n\n        client = self._get_client(deployment)\n\n        # Construct the messages payload for the API call\n        messages = [{"role": "system", "content": "You are a helpful assistant."}]  # Customize as needed\n        for msg in conversation_history:\n            messages.append({"role": msg["role"], "content": msg["content"]})\n        messages.append({"role": "user", "content": message})\n\n        # Use the appropriate client and parameters for the API call\n        if deployment.get("provider") == "azure":\n            # Call Azure OpenAI using deployment_name\n            response = client.chat.completions.create(\n                model=deployment.get("deployment_name"),\n                messages=messages,\n                # ... other parameters like temperature, max_tokens, etc.\n            )\n        else:\n            # Call other OpenAI-compatible providers using model_name\n            response = client.chat.completions.create(\n                model=deployment.get("model_name"),\n                messages=messages,\n                # ... other parameters\n            )\n\n        return response.choices[0].message.content\n```\n\n**3. ProjectManager Integration:**\n\n*   When creating or updating a project, you\'ll now need to allow the user to select a "provider" in addition to the deployment/model details.\n*   Store the provider information along with the project data.\n\n**Key Considerations:**\n\n*   **Error Handling:** Implement robust error handling to catch issues like invalid API keys, incorrect endpoints, unsupported models, and API rate limiting.\n*   **Abstraction:** Design your `AIAssistant` class in a way that abstracts away the differences between providers as much as possible. This will make it easier to add support for more providers in the future.\n*   **Testing:** Thoroughly test your implementation with different providers to ensure that your application works correctly with each one.\n*   **User Interface:** Consider how you want to expose the choice of providers and models/deployments in the user interface. You might need to add dropdowns or other UI elements to allow users to select the desired provider and model/deployment for each project.\n\nBy incorporating these changes, your Phase 2 AI enhancements will be even more powerful, allowing your chat application to leverage a wide range of OpenAI-compatible LLMs, giving you and your users greater flexibility and choice.\n']
2024-12-30 02:35:20,050 - DEBUG - API Endpoint: https://openai-hp.openai.azure.com/openai/
2024-12-30 02:35:20,050 - DEBUG - Model Deployment Name: o1-preview
2024-12-30 02:35:20,050 - DEBUG - Message 0: {'role': 'user', 'content': 'Please format your responses in Markdown.'}
2024-12-30 02:35:20,050 - DEBUG - Message 1: {'role': 'user', 'content': 'Test'}
2024-12-30 02:35:20,051 - DEBUG - Message 2: {'role': 'user', 'content': 'Test'}
2024-12-30 02:35:20,051 - DEBUG - Message 3: {'role': 'user', 'content': 'File content:\n```\nYes, absolutely! Adding the ability to use any OpenAI format-compatible LLM is a natural fit for **Phase 2: AI-Powered Chat - Enhancement** and complements the addition of multiple Azure OpenAI deployments. It expands the AI capabilities even further, making the application more versatile.\n\nHere\'s how you can incorporate this into Phase 2, building on the previous modifications for multiple Azure OpenAI deployments:\n\n**1. Generalized Configuration:**\n\n*   **Flexible Endpoints:** Instead of just specifying "Azure OpenAI endpoints," generalize the configuration to handle any OpenAI-compatible endpoint. This means the configuration should accommodate different base URLs for different providers.\n*   **API Key Handling:**  The configuration should continue to support API keys, as most OpenAI-compatible services will require them.\n*   **Model/Deployment Name:**  The configuration should handle either "model names" (common for many providers) or "deployment names" (used by Azure OpenAI). You might need a field to specify which one to use, or you could try to infer it based on the provider.\n\n**Example Configuration (Environment Variables):**\n\n```\n# Azure OpenAI Deployment\nPROJECT_1_PROVIDER=azure\nPROJECT_1_ENDPOINT=your_azure_endpoint_for_project_1\nPROJECT_1_API_KEY=your_azure_api_key_for_project_1\nPROJECT_1_DEPLOYMENT_NAME=your_azure_deployment_name_for_project_1\n\n# Another Provider (e.g., a self-hosted LLM)\nPROJECT_2_PROVIDER=openai_compatible\nPROJECT_2_ENDPOINT=http://your-llm-server:8000/v1\nPROJECT_2_API_KEY=your_llm_api_key\nPROJECT_2_MODEL_NAME=your_llm_model_name\n\n# Default (can be Azure or any other provider)\nDEFAULT_PROVIDER=azure\nDEFAULT_ENDPOINT=your_default_azure_endpoint\nDEFAULT_API_KEY=your_default_azure_api_key\nDEFAULT_DEPLOYMENT_NAME=your_default_azure_deployment_name\n```\n\n**Example Configuration (JSON):**\n\n```json\n{\n  "project_deployments": {\n    "project_1_id": {\n      "provider": "azure",\n      "endpoint": "your_azure_endpoint_for_project_1",\n      "api_key": "your_azure_api_key_for_project_1",\n      "deployment_name": "your_azure_deployment_name_for_project_1"\n    },\n    "project_2_id": {\n      "provider": "openai_compatible",\n      "endpoint": "http://your-llm-server:8000/v1",\n      "api_key": "your_llm_api_key",\n      "model_name": "your_llm_model_name"\n    }\n  },\n  "default_deployment": {\n    "provider": "azure",\n    "endpoint": "your_default_azure_endpoint",\n    "api_key": "your_default_azure_api_key",\n    "deployment_name": "your_default_azure_deployment_name"\n  }\n}\n```\n\n**2. Enhanced `AIAssistant`:**\n\n*   **Provider-Specific Logic:**  Modify the `AIAssistant` class to handle different providers. You might need to introduce a "provider" field (e.g., "azure," "openai\\_compatible") to determine how to construct the API request.\n*   **Client Library Handling:** If you are using the official OpenAI Python client library, you\'ll likely need to create separate client instances for Azure OpenAI and other providers, as the configuration (endpoint, API key) will be different. You might consider creating a factory method within `AIAssistant` to create the appropriate client based on the provider.\n*   **Unified Interface:**  Despite the differences in how API calls are made, ensure that the `get_ai_response` method still provides a consistent interface, returning the AI\'s response in the same format regardless of the underlying provider.\n\n**Example Conceptual Code (Phase 2 - Python):**\n\n```python\nimport openai\n\nclass AIAssistant:\n    def __init__(self, deployment_config=None):\n        self.deployment_config = deployment_config or {}\n        self.default_deployment = self.deployment_config.get("default_deployment", {})\n\n    def _get_client(self, deployment):\n        provider = deployment.get("provider")\n        api_key = deployment.get("api_key")\n        endpoint = deployment.get("endpoint")\n\n        if provider == "azure":\n            # Configure for Azure OpenAI\n            openai.api_type = "azure"\n            openai.api_key = api_key\n            openai.api_base = endpoint\n            openai.api_version = "2023-05-15" # Or your desired API version\n            return openai\n        elif provider == "openai_compatible":\n            # Configure for other OpenAI-compatible providers\n            client = openai.OpenAI(\n                api_key=api_key,\n                base_url=endpoint\n            )\n            return client\n        else:\n            raise ValueError(f"Unsupported provider: {provider}")\n\n    def get_ai_response(self, message, conversation_history, project_id=None, language_model=\'gpt-3.5-turbo\') -> str:\n        deployment = self.default_deployment\n        if project_id:\n            project_deployment = self.deployment_config.get("project_deployments", {}).get(project_id)\n            if project_deployment:\n                deployment = project_deployment\n\n        client = self._get_client(deployment)\n\n        # Construct the messages payload for the API call\n        messages = [{"role": "system", "content": "You are a helpful assistant."}]  # Customize as needed\n        for msg in conversation_history:\n            messages.append({"role": msg["role"], "content": msg["content"]})\n        messages.append({"role": "user", "content": message})\n\n        # Use the appropriate client and parameters for the API call\n        if deployment.get("provider") == "azure":\n            # Call Azure OpenAI using deployment_name\n            response = client.chat.completions.create(\n                model=deployment.get("deployment_name"),\n                messages=messages,\n                # ... other parameters like temperature, max_tokens, etc.\n            )\n        else:\n            # Call other OpenAI-compatible providers using model_name\n            response = client.chat.completions.create(\n                model=deployment.get("model_name"),\n                messages=messages,\n                # ... other parameters\n            )\n\n        return response.choices[0].message.content\n```\n\n**3. ProjectManager Integration:**\n\n*   When creating or updating a project, you\'ll now need to allow the user to select a "provider" in addition to the deployment/model details.\n*   Store the provider information along with the project data.\n\n**Key Considerations:**\n\n*   **Error Handling:** Implement robust error handling to catch issues like invalid API keys, incorrect endpoints, unsupported models, and API rate limiting.\n*   **Abstraction:** Design your `AIAssistant` class in a way that abstracts away the differences between providers as much as possible. This will make it easier to add support for more providers in the future.\n*   **Testing:** Thoroughly test your implementation with different providers to ensure that your application works correctly with each one.\n*   **User Interface:** Consider how you want to expose the choice of providers and models/deployments in the user interface. You might need to add dropdowns or other UI elements to allow users to select the desired provider and model/deployment for each project.\n\nBy incorporating these changes, your Phase 2 AI enhancements will be even more powerful, allowing your chat application to leverage a wide range of OpenAI-compatible LLMs, giving you and your users greater flexibility and choice.\n\n```'}
2024-12-30 02:35:20,058 - DEBUG - Request options: {'method': 'post', 'url': '/deployments/o1-preview/chat/completions', 'headers': {'api-key': '<redacted>'}, 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': 'Please format your responses in Markdown.'}, {'role': 'user', 'content': 'Test'}, {'role': 'user', 'content': 'Test'}, {'role': 'user', 'content': 'File content:\n```\nYes, absolutely! Adding the ability to use any OpenAI format-compatible LLM is a natural fit for **Phase 2: AI-Powered Chat - Enhancement** and complements the addition of multiple Azure OpenAI deployments. It expands the AI capabilities even further, making the application more versatile.\n\nHere\'s how you can incorporate this into Phase 2, building on the previous modifications for multiple Azure OpenAI deployments:\n\n**1. Generalized Configuration:**\n\n*   **Flexible Endpoints:** Instead of just specifying "Azure OpenAI endpoints," generalize the configuration to handle any OpenAI-compatible endpoint. This means the configuration should accommodate different base URLs for different providers.\n*   **API Key Handling:**  The configuration should continue to support API keys, as most OpenAI-compatible services will require them.\n*   **Model/Deployment Name:**  The configuration should handle either "model names" (common for many providers) or "deployment names" (used by Azure OpenAI). You might need a field to specify which one to use, or you could try to infer it based on the provider.\n\n**Example Configuration (Environment Variables):**\n\n```\n# Azure OpenAI Deployment\nPROJECT_1_PROVIDER=azure\nPROJECT_1_ENDPOINT=your_azure_endpoint_for_project_1\nPROJECT_1_API_KEY=your_azure_api_key_for_project_1\nPROJECT_1_DEPLOYMENT_NAME=your_azure_deployment_name_for_project_1\n\n# Another Provider (e.g., a self-hosted LLM)\nPROJECT_2_PROVIDER=openai_compatible\nPROJECT_2_ENDPOINT=http://your-llm-server:8000/v1\nPROJECT_2_API_KEY=your_llm_api_key\nPROJECT_2_MODEL_NAME=your_llm_model_name\n\n# Default (can be Azure or any other provider)\nDEFAULT_PROVIDER=azure\nDEFAULT_ENDPOINT=your_default_azure_endpoint\nDEFAULT_API_KEY=your_default_azure_api_key\nDEFAULT_DEPLOYMENT_NAME=your_default_azure_deployment_name\n```\n\n**Example Configuration (JSON):**\n\n```json\n{\n  "project_deployments": {\n    "project_1_id": {\n      "provider": "azure",\n      "endpoint": "your_azure_endpoint_for_project_1",\n      "api_key": "your_azure_api_key_for_project_1",\n      "deployment_name": "your_azure_deployment_name_for_project_1"\n    },\n    "project_2_id": {\n      "provider": "openai_compatible",\n      "endpoint": "http://your-llm-server:8000/v1",\n      "api_key": "your_llm_api_key",\n      "model_name": "your_llm_model_name"\n    }\n  },\n  "default_deployment": {\n    "provider": "azure",\n    "endpoint": "your_default_azure_endpoint",\n    "api_key": "your_default_azure_api_key",\n    "deployment_name": "your_default_azure_deployment_name"\n  }\n}\n```\n\n**2. Enhanced `AIAssistant`:**\n\n*   **Provider-Specific Logic:**  Modify the `AIAssistant` class to handle different providers. You might need to introduce a "provider" field (e.g., "azure," "openai\\_compatible") to determine how to construct the API request.\n*   **Client Library Handling:** If you are using the official OpenAI Python client library, you\'ll likely need to create separate client instances for Azure OpenAI and other providers, as the configuration (endpoint, API key) will be different. You might consider creating a factory method within `AIAssistant` to create the appropriate client based on the provider.\n*   **Unified Interface:**  Despite the differences in how API calls are made, ensure that the `get_ai_response` method still provides a consistent interface, returning the AI\'s response in the same format regardless of the underlying provider.\n\n**Example Conceptual Code (Phase 2 - Python):**\n\n```python\nimport openai\n\nclass AIAssistant:\n    def __init__(self, deployment_config=None):\n        self.deployment_config = deployment_config or {}\n        self.default_deployment = self.deployment_config.get("default_deployment", {})\n\n    def _get_client(self, deployment):\n        provider = deployment.get("provider")\n        api_key = deployment.get("api_key")\n        endpoint = deployment.get("endpoint")\n\n        if provider == "azure":\n            # Configure for Azure OpenAI\n            openai.api_type = "azure"\n            openai.api_key = api_key\n            openai.api_base = endpoint\n            openai.api_version = "2023-05-15" # Or your desired API version\n            return openai\n        elif provider == "openai_compatible":\n            # Configure for other OpenAI-compatible providers\n            client = openai.OpenAI(\n                api_key=api_key,\n                base_url=endpoint\n            )\n            return client\n        else:\n            raise ValueError(f"Unsupported provider: {provider}")\n\n    def get_ai_response(self, message, conversation_history, project_id=None, language_model=\'gpt-3.5-turbo\') -> str:\n        deployment = self.default_deployment\n        if project_id:\n            project_deployment = self.deployment_config.get("project_deployments", {}).get(project_id)\n            if project_deployment:\n                deployment = project_deployment\n\n        client = self._get_client(deployment)\n\n        # Construct the messages payload for the API call\n        messages = [{"role": "system", "content": "You are a helpful assistant."}]  # Customize as needed\n        for msg in conversation_history:\n            messages.append({"role": msg["role"], "content": msg["content"]})\n        messages.append({"role": "user", "content": message})\n\n        # Use the appropriate client and parameters for the API call\n        if deployment.get("provider") == "azure":\n            # Call Azure OpenAI using deployment_name\n            response = client.chat.completions.create(\n                model=deployment.get("deployment_name"),\n                messages=messages,\n                # ... other parameters like temperature, max_tokens, etc.\n            )\n        else:\n            # Call other OpenAI-compatible providers using model_name\n            response = client.chat.completions.create(\n                model=deployment.get("model_name"),\n                messages=messages,\n                # ... other parameters\n            )\n\n        return response.choices[0].message.content\n```\n\n**3. ProjectManager Integration:**\n\n*   When creating or updating a project, you\'ll now need to allow the user to select a "provider" in addition to the deployment/model details.\n*   Store the provider information along with the project data.\n\n**Key Considerations:**\n\n*   **Error Handling:** Implement robust error handling to catch issues like invalid API keys, incorrect endpoints, unsupported models, and API rate limiting.\n*   **Abstraction:** Design your `AIAssistant` class in a way that abstracts away the differences between providers as much as possible. This will make it easier to add support for more providers in the future.\n*   **Testing:** Thoroughly test your implementation with different providers to ensure that your application works correctly with each one.\n*   **User Interface:** Consider how you want to expose the choice of providers and models/deployments in the user interface. You might need to add dropdowns or other UI elements to allow users to select the desired provider and model/deployment for each project.\n\nBy incorporating these changes, your Phase 2 AI enhancements will be even more powerful, allowing your chat application to leverage a wide range of OpenAI-compatible LLMs, giving you and your users greater flexibility and choice.\n\n```'}], 'model': 'o1-preview', 'max_completion_tokens': 32000, 'response_format': {'type': 'text'}, 'temperature': 1}}
2024-12-30 02:35:20,059 - DEBUG - Sending HTTP Request: POST https://openai-hp.openai.azure.com/openai/deployments/o1-preview/chat/completions?api-version=2024-12-01-preview
2024-12-30 02:35:20,060 - DEBUG - close.started
2024-12-30 02:35:20,060 - DEBUG - close.complete
2024-12-30 02:35:20,060 - DEBUG - connect_tcp.started host='openai-hp.openai.azure.com' port=443 local_address=None timeout=5.0 socket_options=None
2024-12-30 02:35:20,155 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001BDE0E3D590>
2024-12-30 02:35:20,155 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x000001BDE0D527B0> server_hostname='openai-hp.openai.azure.com' timeout=5.0
2024-12-30 02:35:20,237 - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001BDE0E25BA0>
2024-12-30 02:35:20,238 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2024-12-30 02:35:20,239 - DEBUG - send_request_headers.complete
2024-12-30 02:35:20,239 - DEBUG - send_request_body.started request=<Request [b'POST']>
2024-12-30 02:35:20,240 - DEBUG - send_request_body.complete
2024-12-30 02:35:20,240 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2024-12-30 02:35:22,165 - INFO - 127.0.0.1 - - [30/Dec/2024 02:35:22] "GET /chat_interface HTTP/1.1" 200 -
2024-12-30 02:35:24,107 - INFO - 127.0.0.1 - - [30/Dec/2024 02:35:24] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-30 02:35:28,474 - INFO - 127.0.0.1 - - [30/Dec/2024 02:35:28] "GET /chat_interface HTTP/1.1" 200 -
2024-12-30 02:35:28,783 - INFO - 127.0.0.1 - - [30/Dec/2024 02:35:28] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-30 02:35:32,742 - INFO - 127.0.0.1 - - [30/Dec/2024 02:35:32] "[32mGET / HTTP/1.1[0m" 302 -
2024-12-30 02:35:32,748 - INFO - 127.0.0.1 - - [30/Dec/2024 02:35:32] "GET /chat_interface HTTP/1.1" 200 -
2024-12-30 02:35:33,008 - INFO - 127.0.0.1 - - [30/Dec/2024 02:35:33] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-30 02:35:43,262 - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-12-30 02:35:43,263 - INFO - [33mPress CTRL+C to quit[0m
2024-12-30 02:35:43,265 - INFO -  * Restarting with stat
2024-12-30 02:35:45,682 - WARNING -  * Debugger is active!
2024-12-30 02:35:45,696 - INFO -  * Debugger PIN: 122-746-019
2024-12-30 02:35:46,873 - INFO - 127.0.0.1 - - [30/Dec/2024 02:35:46] "[32mGET /chat_interface HTTP/1.1[0m" 302 -
2024-12-30 02:35:46,885 - INFO - 127.0.0.1 - - [30/Dec/2024 02:35:46] "GET /login?next=/chat_interface HTTP/1.1" 200 -
2024-12-30 02:35:51,464 - INFO - 127.0.0.1 - - [30/Dec/2024 02:35:51] "[32mPOST /login HTTP/1.1[0m" 302 -
2024-12-30 02:35:51,632 - DEBUG - Added message to chat 33e9c53a-2cf0-4e68-aee8-4b8995514936: user: Please format your responses in Markdown....
2024-12-30 02:35:51,638 - INFO - 127.0.0.1 - - [30/Dec/2024 02:35:51] "GET /chat_interface HTTP/1.1" 200 -
2024-12-30 02:35:52,078 - INFO - 127.0.0.1 - - [30/Dec/2024 02:35:52] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-30 02:35:55,874 - DEBUG - Received chat message
2024-12-30 02:35:55,875 - DEBUG - Chat ID: 33e9c53a-2cf0-4e68-aee8-4b8995514936
2024-12-30 02:35:55,875 - INFO - Processing user message: Test
2024-12-30 02:35:55,876 - DEBUG - Full message data: {'chat_id': '33e9c53a-2cf0-4e68-aee8-4b8995514936', 'message': 'Test'}
2024-12-30 02:35:55,876 - DEBUG - Sending request with 2 messages
2024-12-30 02:35:55,876 - DEBUG - Using deployment: o1-preview
2024-12-30 02:35:55,877 - DEBUG - Payload being sent to API: [{'role': 'user', 'content': 'Please format your responses in Markdown.'}, {'role': 'user', 'content': 'Test'}]
2024-12-30 02:35:55,877 - DEBUG - Exact file content included in payload: []
2024-12-30 02:35:55,877 - DEBUG - API Endpoint: https://openai-hp.openai.azure.com/openai/
2024-12-30 02:35:55,877 - DEBUG - Model Deployment Name: o1-preview
2024-12-30 02:35:55,878 - DEBUG - Message 0: {'role': 'user', 'content': 'Please format your responses in Markdown.'}
2024-12-30 02:35:55,878 - DEBUG - Message 1: {'role': 'user', 'content': 'Test'}
2024-12-30 02:35:55,884 - DEBUG - Request options: {'method': 'post', 'url': '/deployments/o1-preview/chat/completions', 'headers': {'api-key': '<redacted>'}, 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': 'Please format your responses in Markdown.'}, {'role': 'user', 'content': 'Test'}], 'model': 'o1-preview', 'max_completion_tokens': 32000, 'response_format': {'type': 'text'}, 'temperature': 1}}
2024-12-30 02:35:55,993 - DEBUG - Sending HTTP Request: POST https://openai-hp.openai.azure.com/openai/deployments/o1-preview/chat/completions?api-version=2024-12-01-preview
2024-12-30 02:35:55,994 - DEBUG - connect_tcp.started host='openai-hp.openai.azure.com' port=443 local_address=None timeout=5.0 socket_options=None
2024-12-30 02:35:56,130 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000026C4E42AE40>
2024-12-30 02:35:56,130 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x0000026C4E3727B0> server_hostname='openai-hp.openai.azure.com' timeout=5.0
2024-12-30 02:35:56,227 - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000026C4E45C550>
2024-12-30 02:35:56,228 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2024-12-30 02:35:56,228 - DEBUG - send_request_headers.complete
2024-12-30 02:35:56,228 - DEBUG - send_request_body.started request=<Request [b'POST']>
2024-12-30 02:35:56,229 - DEBUG - send_request_body.complete
2024-12-30 02:35:56,229 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2024-12-30 02:36:08,459 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Length', b'590'), (b'Content-Type', b'application/json'), (b'apim-request-id', b'12d50b10-216b-460e-b2ab-3eb195822225'), (b'Strict-Transport-Security', b'max-age=31536000; includeSubDomains; preload'), (b'x-content-type-options', b'nosniff'), (b'x-ratelimit-remaining-tokens', b'2336000'), (b'x-accel-buffering', b'no'), (b'x-ms-rai-invoked', b'true'), (b'x-request-id', b'2b3bd23f-bb3b-45ca-a4e9-55ca0a6134b3'), (b'x-ms-region', b'East US 2'), (b'azureml-model-session', b'd085-20241219205556'), (b'x-ratelimit-remaining-requests', b'398'), (b'x-envoy-upstream-service-time', b'12139'), (b'x-ms-client-request-id', b'12d50b10-216b-460e-b2ab-3eb195822225'), (b'Date', b'Mon, 30 Dec 2024 08:36:06 GMT')])
2024-12-30 02:36:08,460 - INFO - HTTP Request: POST https://openai-hp.openai.azure.com/openai/deployments/o1-preview/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2024-12-30 02:36:08,460 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2024-12-30 02:36:08,461 - DEBUG - receive_response_body.complete
2024-12-30 02:36:08,461 - DEBUG - response_closed.started
2024-12-30 02:36:08,461 - DEBUG - response_closed.complete
2024-12-30 02:36:08,461 - DEBUG - HTTP Response: POST https://openai-hp.openai.azure.com/openai/deployments/o1-preview/chat/completions?api-version=2024-12-01-preview "200 OK" Headers({'content-length': '590', 'content-type': 'application/json', 'apim-request-id': '12d50b10-216b-460e-b2ab-3eb195822225', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'x-content-type-options': 'nosniff', 'x-ratelimit-remaining-tokens': '2336000', 'x-accel-buffering': 'no', 'x-ms-rai-invoked': 'true', 'x-request-id': '2b3bd23f-bb3b-45ca-a4e9-55ca0a6134b3', 'x-ms-region': 'East US 2', 'azureml-model-session': 'd085-20241219205556', 'x-ratelimit-remaining-requests': '398', 'x-envoy-upstream-service-time': '12139', 'x-ms-client-request-id': '12d50b10-216b-460e-b2ab-3eb195822225', 'date': 'Mon, 30 Dec 2024 08:36:06 GMT'})
2024-12-30 02:36:08,462 - DEBUG - request_id: 2b3bd23f-bb3b-45ca-a4e9-55ca0a6134b3
2024-12-30 02:36:08,465 - INFO - API response received successfully
2024-12-30 02:36:08,465 - DEBUG - Response content: **Test received!**

This response is formatted in Markdown.
2024-12-30 02:36:08,466 - DEBUG - Token usage - Prompt: 27, Completion: 1248, Total: 1275
2024-12-30 02:36:08,466 - DEBUG - Response finish reason: stop
2024-12-30 02:36:08,468 - INFO - Received response: <p><strong>Test received!</strong></p>

<p>This response is formatted in Markdown.</p>
<div class="shortcuts"><button onclick="copyToClipboard()">Copy</button><button onclick="retryMessage()">Retry</button></div>
2024-12-30 02:36:08,469 - DEBUG - Response being sent to chat interface: <p><strong>Test received!</strong></p>

<p>This response is formatted in Markdown.</p>
<div class="shortcuts"><button onclick="copyToClipboard()">Copy</button><button onclick="retryMessage()">Retry</button></div>
2024-12-30 02:36:08,469 - DEBUG - Raw API response: ChatCompletion(id='chatcmpl-Ak5Xf5mnRRMxk8pozSsRSkgV25OrF', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='**Test received!**\n\nThis response is formatted in Markdown.', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None), content_filter_results={})], created=1735547755, model='o1-preview-2024-09-12', object='chat.completion', service_tier=None, system_fingerprint='fp_772cb6e46d', usage=CompletionUsage(completion_tokens=1248, prompt_tokens=27, total_tokens=1275, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=1216, rejected_prediction_tokens=None), prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=0)), prompt_filter_results=[{'prompt_index': 0, 'content_filter_results': {}}])
2024-12-30 02:36:08,470 - DEBUG - Usage: CompletionUsage(completion_tokens=1248, prompt_tokens=27, total_tokens=1275, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=1216, rejected_prediction_tokens=None), prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=0))
2024-12-30 02:36:08,470 - DEBUG - Cleared context for chat 33e9c53a-2cf0-4e68-aee8-4b8995514936
2024-12-30 02:36:08,470 - DEBUG - Added message to chat 33e9c53a-2cf0-4e68-aee8-4b8995514936: user: Please format your responses in Markdown....
2024-12-30 02:36:08,470 - DEBUG - Added message to chat 33e9c53a-2cf0-4e68-aee8-4b8995514936: user: Test...
2024-12-30 02:36:08,471 - DEBUG - Added message to chat 33e9c53a-2cf0-4e68-aee8-4b8995514936: assistant: <p><strong>Test received!</strong></p>

<p>This re...
2024-12-30 02:36:08,471 - INFO - 127.0.0.1 - - [30/Dec/2024 02:36:08] "POST /chat HTTP/1.1" 200 -
2024-12-30 02:43:14,954 - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-12-30 02:43:14,955 - INFO - [33mPress CTRL+C to quit[0m
2024-12-30 02:43:14,960 - INFO -  * Restarting with stat
2024-12-30 02:43:17,766 - WARNING -  * Debugger is active!
2024-12-30 02:43:17,779 - INFO -  * Debugger PIN: 122-746-019
2024-12-30 02:43:24,801 - INFO - 127.0.0.1 - - [30/Dec/2024 02:43:24] "[32mGET /chat_interface HTTP/1.1[0m" 302 -
2024-12-30 02:43:24,811 - INFO - 127.0.0.1 - - [30/Dec/2024 02:43:24] "GET /login?next=/chat_interface HTTP/1.1" 200 -
2024-12-30 02:43:30,186 - INFO - 127.0.0.1 - - [30/Dec/2024 02:43:30] "[32mPOST /login HTTP/1.1[0m" 302 -
2024-12-30 02:43:30,373 - DEBUG - Added message to chat 2d0bf211-3ba2-4b49-ae04-6781ede4ab24: user: Please format your responses in Markdown....
2024-12-30 02:43:30,404 - INFO - 127.0.0.1 - - [30/Dec/2024 02:43:30] "GET /chat_interface HTTP/1.1" 200 -
2024-12-30 02:43:30,835 - INFO - 127.0.0.1 - - [30/Dec/2024 02:43:30] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-30 02:43:30,998 - INFO - 127.0.0.1 - - [30/Dec/2024 02:43:30] "[33mGET /conversations HTTP/1.1[0m" 404 -
2024-12-30 02:43:42,794 - INFO - 127.0.0.1 - - [30/Dec/2024 02:43:42] "[33mGET /new-chat HTTP/1.1[0m" 404 -
2024-12-30 02:43:49,414 - INFO - 127.0.0.1 - - [30/Dec/2024 02:43:49] "GET /chat_interface HTTP/1.1" 200 -
2024-12-30 02:44:00,313 - INFO - 127.0.0.1 - - [30/Dec/2024 02:44:00] "[33mPOST /chat/2d0bf211-3ba2-4b49-ae04-6781ede4ab24/context HTTP/1.1[0m" 404 -
2024-12-30 02:44:04,396 - DEBUG - Received chat message
2024-12-30 02:44:04,397 - DEBUG - Chat ID: 2d0bf211-3ba2-4b49-ae04-6781ede4ab24
2024-12-30 02:44:04,397 - INFO - Processing user message: Test
2024-12-30 02:44:04,397 - DEBUG - Full message data: {'chat_id': '2d0bf211-3ba2-4b49-ae04-6781ede4ab24', 'message': 'Test'}
2024-12-30 02:44:04,398 - DEBUG - Sending request with 2 messages
2024-12-30 02:44:04,398 - DEBUG - Using deployment: o1-preview
2024-12-30 02:44:04,398 - DEBUG - Payload being sent to API: [{'role': 'user', 'content': 'Please format your responses in Markdown.'}, {'role': 'user', 'content': 'Test'}]
2024-12-30 02:44:04,399 - DEBUG - Exact file content included in payload: []
2024-12-30 02:44:04,399 - DEBUG - API Endpoint: https://openai-hp.openai.azure.com/openai/
2024-12-30 02:44:04,399 - DEBUG - Model Deployment Name: o1-preview
2024-12-30 02:44:04,399 - DEBUG - Message 0: {'role': 'user', 'content': 'Please format your responses in Markdown.'}
2024-12-30 02:44:04,399 - DEBUG - Message 1: {'role': 'user', 'content': 'Test'}
2024-12-30 02:44:04,403 - DEBUG - Request options: {'method': 'post', 'url': '/deployments/o1-preview/chat/completions', 'headers': {'api-key': '<redacted>'}, 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': 'Please format your responses in Markdown.'}, {'role': 'user', 'content': 'Test'}], 'model': 'o1-preview', 'max_completion_tokens': 32000, 'response_format': {'type': 'text'}, 'temperature': 1}}
2024-12-30 02:44:04,487 - DEBUG - Sending HTTP Request: POST https://openai-hp.openai.azure.com/openai/deployments/o1-preview/chat/completions?api-version=2024-12-01-preview
2024-12-30 02:44:04,487 - DEBUG - connect_tcp.started host='openai-hp.openai.azure.com' port=443 local_address=None timeout=5.0 socket_options=None
2024-12-30 02:44:04,624 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001D52776AF90>
2024-12-30 02:44:04,624 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x000001D5276B27B0> server_hostname='openai-hp.openai.azure.com' timeout=5.0
2024-12-30 02:44:04,710 - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001D52779D450>
2024-12-30 02:44:04,711 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2024-12-30 02:44:04,712 - DEBUG - send_request_headers.complete
2024-12-30 02:44:04,712 - DEBUG - send_request_body.started request=<Request [b'POST']>
2024-12-30 02:44:04,712 - DEBUG - send_request_body.complete
2024-12-30 02:44:04,713 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2024-12-30 02:44:09,526 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Length', b'620'), (b'Content-Type', b'application/json'), (b'apim-request-id', b'90c26a7f-8eea-47c8-b15c-dd063493698f'), (b'Strict-Transport-Security', b'max-age=31536000; includeSubDomains; preload'), (b'x-content-type-options', b'nosniff'), (b'x-ratelimit-remaining-tokens', b'2368000'), (b'x-accel-buffering', b'no'), (b'x-ms-rai-invoked', b'true'), (b'x-request-id', b'677ef4ce-378f-44d5-aa63-407eadf6d31b'), (b'x-ms-region', b'East US 2'), (b'azureml-model-session', b'd083-20241219180804'), (b'x-ratelimit-remaining-requests', b'399'), (b'x-envoy-upstream-service-time', b'4732'), (b'x-ms-client-request-id', b'90c26a7f-8eea-47c8-b15c-dd063493698f'), (b'Date', b'Mon, 30 Dec 2024 08:44:07 GMT')])
2024-12-30 02:44:09,527 - INFO - HTTP Request: POST https://openai-hp.openai.azure.com/openai/deployments/o1-preview/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2024-12-30 02:44:09,527 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2024-12-30 02:44:09,528 - DEBUG - receive_response_body.complete
2024-12-30 02:44:09,528 - DEBUG - response_closed.started
2024-12-30 02:44:09,528 - DEBUG - response_closed.complete
2024-12-30 02:44:09,529 - DEBUG - HTTP Response: POST https://openai-hp.openai.azure.com/openai/deployments/o1-preview/chat/completions?api-version=2024-12-01-preview "200 OK" Headers({'content-length': '620', 'content-type': 'application/json', 'apim-request-id': '90c26a7f-8eea-47c8-b15c-dd063493698f', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'x-content-type-options': 'nosniff', 'x-ratelimit-remaining-tokens': '2368000', 'x-accel-buffering': 'no', 'x-ms-rai-invoked': 'true', 'x-request-id': '677ef4ce-378f-44d5-aa63-407eadf6d31b', 'x-ms-region': 'East US 2', 'azureml-model-session': 'd083-20241219180804', 'x-ratelimit-remaining-requests': '399', 'x-envoy-upstream-service-time': '4732', 'x-ms-client-request-id': '90c26a7f-8eea-47c8-b15c-dd063493698f', 'date': 'Mon, 30 Dec 2024 08:44:07 GMT'})
2024-12-30 02:44:09,529 - DEBUG - request_id: 677ef4ce-378f-44d5-aa63-407eadf6d31b
2024-12-30 02:44:09,532 - INFO - API response received successfully
2024-12-30 02:44:09,618 - DEBUG - Token usage - Prompt: 27, Completion: 610, Total: 637
2024-12-30 02:44:09,618 - DEBUG - Response finish reason: stop
2024-12-30 02:44:09,639 - DEBUG - Usage: CompletionUsage(completion_tokens=610, prompt_tokens=27, total_tokens=637, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=576, rejected_prediction_tokens=None), prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=0))
2024-12-30 02:44:09,639 - DEBUG - Cleared context for chat 2d0bf211-3ba2-4b49-ae04-6781ede4ab24
2024-12-30 02:44:09,640 - DEBUG - Added message to chat 2d0bf211-3ba2-4b49-ae04-6781ede4ab24: user: Please format your responses in Markdown....
2024-12-30 02:44:09,640 - DEBUG - Added message to chat 2d0bf211-3ba2-4b49-ae04-6781ede4ab24: user: Test...
2024-12-30 02:44:09,656 - INFO - 127.0.0.1 - - [30/Dec/2024 02:44:09] "POST /chat HTTP/1.1" 200 -
2024-12-30 02:44:27,193 - DEBUG - Received chat message
2024-12-30 02:44:27,194 - DEBUG - Chat ID: 2d0bf211-3ba2-4b49-ae04-6781ede4ab24
2024-12-30 02:44:27,194 - INFO - Processing user message: Test
2024-12-30 02:44:27,195 - DEBUG - Full message data: {'chat_id': '2d0bf211-3ba2-4b49-ae04-6781ede4ab24', 'message': 'Test'}
2024-12-30 02:44:27,195 - DEBUG - Sending request with 3 messages
2024-12-30 02:44:27,195 - DEBUG - Using deployment: o1-preview
2024-12-30 02:44:27,196 - DEBUG - Payload being sent to API: [{'role': 'user', 'content': 'Please format your responses in Markdown.'}, {'role': 'user', 'content': 'Test'}, {'role': 'user', 'content': 'Test'}]
2024-12-30 02:44:27,197 - DEBUG - Exact file content included in payload: []
2024-12-30 02:44:27,197 - DEBUG - API Endpoint: https://openai-hp.openai.azure.com/openai/
2024-12-30 02:44:27,199 - DEBUG - Model Deployment Name: o1-preview
2024-12-30 02:44:27,199 - DEBUG - Message 0: {'role': 'user', 'content': 'Please format your responses in Markdown.'}
2024-12-30 02:44:27,199 - DEBUG - Message 1: {'role': 'user', 'content': 'Test'}
2024-12-30 02:44:27,200 - DEBUG - Message 2: {'role': 'user', 'content': 'Test'}
2024-12-30 02:44:27,207 - DEBUG - Request options: {'method': 'post', 'url': '/deployments/o1-preview/chat/completions', 'headers': {'api-key': '<redacted>'}, 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': 'Please format your responses in Markdown.'}, {'role': 'user', 'content': 'Test'}, {'role': 'user', 'content': 'Test'}], 'model': 'o1-preview', 'max_completion_tokens': 32000, 'response_format': {'type': 'text'}, 'temperature': 1}}
2024-12-30 02:44:27,208 - DEBUG - Sending HTTP Request: POST https://openai-hp.openai.azure.com/openai/deployments/o1-preview/chat/completions?api-version=2024-12-01-preview
2024-12-30 02:44:27,208 - DEBUG - close.started
2024-12-30 02:44:27,209 - DEBUG - close.complete
2024-12-30 02:44:27,209 - DEBUG - connect_tcp.started host='openai-hp.openai.azure.com' port=443 local_address=None timeout=5.0 socket_options=None
2024-12-30 02:44:27,267 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001D52779C050>
2024-12-30 02:44:27,268 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x000001D5276B27B0> server_hostname='openai-hp.openai.azure.com' timeout=5.0
2024-12-30 02:44:27,349 - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001D527786650>
2024-12-30 02:44:27,349 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2024-12-30 02:44:27,350 - DEBUG - send_request_headers.complete
2024-12-30 02:44:27,350 - DEBUG - send_request_body.started request=<Request [b'POST']>
2024-12-30 02:44:27,350 - DEBUG - send_request_body.complete
2024-12-30 02:44:27,350 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2024-12-30 02:44:44,810 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Length', b'583'), (b'Content-Type', b'application/json'), (b'apim-request-id', b'3721a14f-deba-4f4f-83d4-88a5000a7435'), (b'Strict-Transport-Security', b'max-age=31536000; includeSubDomains; preload'), (b'x-content-type-options', b'nosniff'), (b'x-ratelimit-remaining-tokens', b'2336000'), (b'x-accel-buffering', b'no'), (b'x-ms-rai-invoked', b'true'), (b'x-request-id', b'7248fa52-3ea6-4a36-b4dd-ab1ecd3c6b8b'), (b'x-ms-region', b'East US 2'), (b'azureml-model-session', b'd084-20241219190247'), (b'x-ratelimit-remaining-requests', b'398'), (b'x-envoy-upstream-service-time', b'17380'), (b'x-ms-client-request-id', b'3721a14f-deba-4f4f-83d4-88a5000a7435'), (b'Date', b'Mon, 30 Dec 2024 08:44:43 GMT')])
2024-12-30 02:44:44,810 - INFO - HTTP Request: POST https://openai-hp.openai.azure.com/openai/deployments/o1-preview/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2024-12-30 02:44:44,811 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2024-12-30 02:44:44,811 - DEBUG - receive_response_body.complete
2024-12-30 02:44:44,811 - DEBUG - response_closed.started
2024-12-30 02:44:44,812 - DEBUG - response_closed.complete
2024-12-30 02:44:44,812 - DEBUG - HTTP Response: POST https://openai-hp.openai.azure.com/openai/deployments/o1-preview/chat/completions?api-version=2024-12-01-preview "200 OK" Headers({'content-length': '583', 'content-type': 'application/json', 'apim-request-id': '3721a14f-deba-4f4f-83d4-88a5000a7435', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'x-content-type-options': 'nosniff', 'x-ratelimit-remaining-tokens': '2336000', 'x-accel-buffering': 'no', 'x-ms-rai-invoked': 'true', 'x-request-id': '7248fa52-3ea6-4a36-b4dd-ab1ecd3c6b8b', 'x-ms-region': 'East US 2', 'azureml-model-session': 'd084-20241219190247', 'x-ratelimit-remaining-requests': '398', 'x-envoy-upstream-service-time': '17380', 'x-ms-client-request-id': '3721a14f-deba-4f4f-83d4-88a5000a7435', 'date': 'Mon, 30 Dec 2024 08:44:43 GMT'})
2024-12-30 02:44:44,812 - DEBUG - request_id: 7248fa52-3ea6-4a36-b4dd-ab1ecd3c6b8b
2024-12-30 02:44:44,813 - INFO - API response received successfully
2024-12-30 02:44:44,814 - DEBUG - Response content: ### Test

This is a test message formatted in Markdown.
2024-12-30 02:44:44,814 - DEBUG - Token usage - Prompt: 40, Completion: 856, Total: 896
2024-12-30 02:44:44,814 - DEBUG - Response finish reason: stop
2024-12-30 02:44:44,816 - INFO - Received response: <h3>Test</h3>

<p>This is a test message formatted in Markdown.</p>
<div class="shortcuts"><button onclick="copyToClipboard()">Copy</button><button onclick="retryMessage()">Retry</button></div>
2024-12-30 02:44:44,816 - DEBUG - Response being sent to chat interface: <h3>Test</h3>

<p>This is a test message formatted in Markdown.</p>
<div class="shortcuts"><button onclick="copyToClipboard()">Copy</button><button onclick="retryMessage()">Retry</button></div>
2024-12-30 02:44:44,817 - DEBUG - Raw API response: ChatCompletion(id='chatcmpl-Ak5fuUPTNW9on5lSwOvGk6FN26TJb', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='### Test\n\nThis is a test message formatted in Markdown.', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None), content_filter_results={})], created=1735548266, model='o1-preview-2024-09-12', object='chat.completion', service_tier=None, system_fingerprint='fp_772cb6e46d', usage=CompletionUsage(completion_tokens=856, prompt_tokens=40, total_tokens=896, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=832, rejected_prediction_tokens=None), prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=0)), prompt_filter_results=[{'prompt_index': 0, 'content_filter_results': {}}])
2024-12-30 02:44:44,817 - DEBUG - Usage: CompletionUsage(completion_tokens=856, prompt_tokens=40, total_tokens=896, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=832, rejected_prediction_tokens=None), prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=0))
2024-12-30 02:44:44,817 - DEBUG - Cleared context for chat 2d0bf211-3ba2-4b49-ae04-6781ede4ab24
2024-12-30 02:44:44,817 - DEBUG - Added message to chat 2d0bf211-3ba2-4b49-ae04-6781ede4ab24: user: Please format your responses in Markdown....
2024-12-30 02:44:44,818 - DEBUG - Added message to chat 2d0bf211-3ba2-4b49-ae04-6781ede4ab24: user: Test...
2024-12-30 02:44:44,823 - DEBUG - Added message to chat 2d0bf211-3ba2-4b49-ae04-6781ede4ab24: user: Test...
2024-12-30 02:44:44,824 - DEBUG - Added message to chat 2d0bf211-3ba2-4b49-ae04-6781ede4ab24: assistant: <h3>Test</h3>

<p>This is a test message formatted...
2024-12-30 02:44:44,824 - INFO - 127.0.0.1 - - [30/Dec/2024 02:44:44] "POST /chat HTTP/1.1" 200 -
2024-12-30 02:44:50,492 - INFO - 127.0.0.1 - - [30/Dec/2024 02:44:50] "[33mPOST /chat/2d0bf211-3ba2-4b49-ae04-6781ede4ab24/context HTTP/1.1[0m" 404 -
2024-12-30 02:44:55,746 - DEBUG - Received chat message
2024-12-30 02:44:55,746 - DEBUG - Chat ID: 2d0bf211-3ba2-4b49-ae04-6781ede4ab24
2024-12-30 02:44:55,746 - INFO - Processing user message: Can you?
2024-12-30 02:44:55,747 - DEBUG - Full message data: {'chat_id': '2d0bf211-3ba2-4b49-ae04-6781ede4ab24', 'message': 'Can you?'}
2024-12-30 02:44:55,747 - DEBUG - Sending request with 4 messages
2024-12-30 02:44:55,747 - DEBUG - Using deployment: o1-preview
2024-12-30 02:44:55,747 - DEBUG - Payload being sent to API: [{'role': 'user', 'content': 'Please format your responses in Markdown.'}, {'role': 'user', 'content': 'Test'}, {'role': 'user', 'content': 'Test'}, {'role': 'user', 'content': 'Can you?'}]
2024-12-30 02:44:55,748 - DEBUG - Exact file content included in payload: []
2024-12-30 02:44:55,748 - DEBUG - API Endpoint: https://openai-hp.openai.azure.com/openai/
2024-12-30 02:44:55,748 - DEBUG - Model Deployment Name: o1-preview
2024-12-30 02:44:55,748 - DEBUG - Message 0: {'role': 'user', 'content': 'Please format your responses in Markdown.'}
2024-12-30 02:44:55,749 - DEBUG - Message 1: {'role': 'user', 'content': 'Test'}
2024-12-30 02:44:55,749 - DEBUG - Message 2: {'role': 'user', 'content': 'Test'}
2024-12-30 02:44:55,749 - DEBUG - Message 3: {'role': 'user', 'content': 'Can you?'}
2024-12-30 02:44:55,755 - DEBUG - Request options: {'method': 'post', 'url': '/deployments/o1-preview/chat/completions', 'headers': {'api-key': '<redacted>'}, 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': 'Please format your responses in Markdown.'}, {'role': 'user', 'content': 'Test'}, {'role': 'user', 'content': 'Test'}, {'role': 'user', 'content': 'Can you?'}], 'model': 'o1-preview', 'max_completion_tokens': 32000, 'response_format': {'type': 'text'}, 'temperature': 1}}
2024-12-30 02:44:55,756 - DEBUG - Sending HTTP Request: POST https://openai-hp.openai.azure.com/openai/deployments/o1-preview/chat/completions?api-version=2024-12-01-preview
2024-12-30 02:44:55,756 - DEBUG - close.started
2024-12-30 02:44:55,757 - DEBUG - close.complete
2024-12-30 02:44:55,757 - DEBUG - connect_tcp.started host='openai-hp.openai.azure.com' port=443 local_address=None timeout=5.0 socket_options=None
2024-12-30 02:44:55,835 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001D527785CD0>
2024-12-30 02:44:55,836 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x000001D5276B27B0> server_hostname='openai-hp.openai.azure.com' timeout=5.0
2024-12-30 02:44:55,909 - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001D52778DC70>
2024-12-30 02:44:55,910 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2024-12-30 02:44:55,910 - DEBUG - send_request_headers.complete
2024-12-30 02:44:55,910 - DEBUG - send_request_body.started request=<Request [b'POST']>
2024-12-30 02:44:55,911 - DEBUG - send_request_body.complete
2024-12-30 02:44:55,911 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2024-12-30 02:45:05,293 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Length', b'725'), (b'Content-Type', b'application/json'), (b'apim-request-id', b'fef881f9-5407-485e-a884-8dd6d4d24fb8'), (b'Strict-Transport-Security', b'max-age=31536000; includeSubDomains; preload'), (b'x-content-type-options', b'nosniff'), (b'x-ratelimit-remaining-tokens', b'2304000'), (b'x-accel-buffering', b'no'), (b'x-ms-rai-invoked', b'true'), (b'x-request-id', b'5f6d8a19-1efc-4800-bce0-7cf064a442cf'), (b'x-ms-region', b'East US 2'), (b'azureml-model-session', b'd083-20241219180804'), (b'x-ratelimit-remaining-requests', b'397'), (b'x-envoy-upstream-service-time', b'9312'), (b'x-ms-client-request-id', b'fef881f9-5407-485e-a884-8dd6d4d24fb8'), (b'Date', b'Mon, 30 Dec 2024 08:45:03 GMT')])
2024-12-30 02:45:05,294 - INFO - HTTP Request: POST https://openai-hp.openai.azure.com/openai/deployments/o1-preview/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2024-12-30 02:45:05,294 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2024-12-30 02:45:05,295 - DEBUG - receive_response_body.complete
2024-12-30 02:45:05,295 - DEBUG - response_closed.started
2024-12-30 02:45:05,295 - DEBUG - response_closed.complete
2024-12-30 02:45:05,295 - DEBUG - HTTP Response: POST https://openai-hp.openai.azure.com/openai/deployments/o1-preview/chat/completions?api-version=2024-12-01-preview "200 OK" Headers({'content-length': '725', 'content-type': 'application/json', 'apim-request-id': 'fef881f9-5407-485e-a884-8dd6d4d24fb8', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'x-content-type-options': 'nosniff', 'x-ratelimit-remaining-tokens': '2304000', 'x-accel-buffering': 'no', 'x-ms-rai-invoked': 'true', 'x-request-id': '5f6d8a19-1efc-4800-bce0-7cf064a442cf', 'x-ms-region': 'East US 2', 'azureml-model-session': 'd083-20241219180804', 'x-ratelimit-remaining-requests': '397', 'x-envoy-upstream-service-time': '9312', 'x-ms-client-request-id': 'fef881f9-5407-485e-a884-8dd6d4d24fb8', 'date': 'Mon, 30 Dec 2024 08:45:03 GMT'})
2024-12-30 02:45:05,296 - DEBUG - request_id: 5f6d8a19-1efc-4800-bce0-7cf064a442cf
2024-12-30 02:45:05,296 - INFO - API response received successfully
2024-12-30 02:45:05,297 - DEBUG - Response content: Yes, I can format my responses in Markdown.

# Heading Level 1

## Heading Level 2

**Bold Text**

*Italic Text*

- List Item 1
- List Item 2

[OpenAI Website](https://www.openai.com)
2024-12-30 02:45:05,297 - DEBUG - Token usage - Prompt: 54, Completion: 1218, Total: 1272
2024-12-30 02:45:05,297 - DEBUG - Response finish reason: stop
2024-12-30 02:45:05,304 - INFO - Received response: <p>Yes, I can format my responses in Markdown.</p>

<h1>Heading Level 1</h1>

<h2>Heading Level 2</h2>

<p><strong>Bold Text</strong></p>

<p><em>Italic Text</em></p>

<ul>
<li>List Item 1</li>
<li>List Item 2</li>
</ul>

<p><a href="https://www.openai.com">OpenAI Website</a></p>
<div class="shortcuts"><button onclick="copyToClipboard()">Copy</button><button onclick="retryMessage()">Retry</button></div>
2024-12-30 02:45:05,304 - DEBUG - Response being sent to chat interface: <p>Yes, I can format my responses in Markdown.</p>

<h1>Heading Level 1</h1>

<h2>Heading Level 2</h2>

<p><strong>Bold Text</strong></p>

<p><em>Italic Text</em></p>

<ul>
<li>List Item 1</li>
<li>List Item 2</li>
</ul>

<p><a href="https://www.openai.com">OpenAI Website</a></p>
<div class="shortcuts"><button onclick="copyToClipboard()">Copy</button><button onclick="retryMessage()">Retry</button></div>
2024-12-30 02:45:05,305 - DEBUG - Raw API response: ChatCompletion(id='chatcmpl-Ak5gMqMKVI71QFjylJIzLGEtvV5Wl', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Yes, I can format my responses in Markdown.\n\n# Heading Level 1\n\n## Heading Level 2\n\n**Bold Text**\n\n*Italic Text*\n\n- List Item 1\n- List Item 2\n\n[OpenAI Website](https://www.openai.com)', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None), content_filter_results={})], created=1735548294, model='o1-preview-2024-09-12', object='chat.completion', service_tier=None, system_fingerprint='fp_772cb6e46d', usage=CompletionUsage(completion_tokens=1218, prompt_tokens=54, total_tokens=1272, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=1152, rejected_prediction_tokens=None), prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=0)), prompt_filter_results=[{'prompt_index': 0, 'content_filter_results': {}}])
2024-12-30 02:45:05,305 - DEBUG - Usage: CompletionUsage(completion_tokens=1218, prompt_tokens=54, total_tokens=1272, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=1152, rejected_prediction_tokens=None), prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=0))
2024-12-30 02:45:05,306 - DEBUG - Cleared context for chat 2d0bf211-3ba2-4b49-ae04-6781ede4ab24
2024-12-30 02:45:05,306 - DEBUG - Added message to chat 2d0bf211-3ba2-4b49-ae04-6781ede4ab24: user: Please format your responses in Markdown....
2024-12-30 02:45:05,306 - DEBUG - Added message to chat 2d0bf211-3ba2-4b49-ae04-6781ede4ab24: user: Test...
2024-12-30 02:45:05,312 - DEBUG - Added message to chat 2d0bf211-3ba2-4b49-ae04-6781ede4ab24: user: Test...
2024-12-30 02:45:05,312 - DEBUG - Added message to chat 2d0bf211-3ba2-4b49-ae04-6781ede4ab24: assistant: <h3>Test</h3>

<p>This is a test message formatted...
2024-12-30 02:45:05,313 - DEBUG - Added message to chat 2d0bf211-3ba2-4b49-ae04-6781ede4ab24: user: Can you?...
2024-12-30 02:45:05,313 - DEBUG - Added message to chat 2d0bf211-3ba2-4b49-ae04-6781ede4ab24: assistant: <p>Yes, I can format my responses in Markdown.</p>...
2024-12-30 02:45:05,314 - INFO - 127.0.0.1 - - [30/Dec/2024 02:45:05] "POST /chat HTTP/1.1" 200 -
2024-12-30 02:46:48,229 - INFO -  * Detected change in 'C:\\Users\\htper\\OneDrive\\Folder\\chats\\chatapp\\routes\\chat_routes.py', reloading
2024-12-30 02:46:48,343 - INFO -  * Restarting with stat
2024-12-30 02:46:51,416 - WARNING -  * Debugger is active!
2024-12-30 02:46:51,426 - INFO -  * Debugger PIN: 122-746-019
2024-12-30 02:53:12,492 - INFO -  * Detected change in 'C:\\Users\\htper\\OneDrive\\Folder\\chats\\chatapp\\database.py', reloading
2024-12-30 02:53:12,660 - INFO -  * Restarting with stat
2024-12-30 02:53:15,529 - WARNING -  * Debugger is active!
2024-12-30 02:53:15,540 - INFO -  * Debugger PIN: 122-746-019
2024-12-30 02:53:34,278 - INFO -  * Detected change in 'C:\\Users\\htper\\OneDrive\\Folder\\chats\\chatapp\\app.py', reloading
2024-12-30 02:53:34,461 - INFO -  * Restarting with stat
2024-12-30 02:53:36,964 - WARNING -  * Debugger is active!
2024-12-30 02:53:36,974 - INFO -  * Debugger PIN: 122-746-019
2024-12-30 02:53:39,607 - INFO -  * Detected change in 'C:\\Users\\htper\\OneDrive\\Folder\\chats\\chatapp\\app.py', reloading
2024-12-30 02:53:39,762 - INFO -  * Restarting with stat
2024-12-30 02:53:43,762 - WARNING -  * Debugger is active!
2024-12-30 02:53:43,774 - INFO -  * Debugger PIN: 122-746-019
2024-12-30 02:53:47,685 - INFO -  * Detected change in 'C:\\Users\\htper\\OneDrive\\Folder\\chats\\chatapp\\app.py', reloading
2024-12-30 02:53:47,843 - INFO -  * Restarting with stat
2024-12-30 03:06:17,977 - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-12-30 03:06:17,978 - INFO - [33mPress CTRL+C to quit[0m
2024-12-30 03:08:25,855 - INFO - 127.0.0.1 - - [30/Dec/2024 03:08:25] "[32mGET /chat_interface HTTP/1.1[0m" 302 -
2024-12-30 03:08:25,865 - INFO - 127.0.0.1 - - [30/Dec/2024 03:08:25] "GET /login?next=/chat_interface HTTP/1.1" 200 -
2024-12-30 03:08:31,615 - INFO - 127.0.0.1 - - [30/Dec/2024 03:08:31] "POST /login HTTP/1.1" 200 -
2024-12-30 03:08:38,186 - INFO - 127.0.0.1 - - [30/Dec/2024 03:08:38] "[32mPOST /login HTTP/1.1[0m" 302 -
2024-12-30 03:08:38,392 - DEBUG - Added message to chat 0c3e78c9-88bc-4216-8710-dcc748ddc832: user: Please format your responses in Markdown....
2024-12-30 03:08:38,424 - INFO - 127.0.0.1 - - [30/Dec/2024 03:08:38] "GET /chat_interface HTTP/1.1" 200 -
2024-12-30 03:08:38,852 - INFO - 127.0.0.1 - - [30/Dec/2024 03:08:38] "[36mGET /static/css/style.css HTTP/1.1[0m" 304 -
2024-12-30 03:08:39,019 - ERROR - Exception on /models [GET]
Traceback (most recent call last):
  File "C:\Users\htper\OneDrive\Folder\chats\chatapp\.venv\Lib\site-packages\flask\app.py", line 1511, in wsgi_app
    response = self.full_dispatch_request()
  File "C:\Users\htper\OneDrive\Folder\chats\chatapp\.venv\Lib\site-packages\flask\app.py", line 919, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File "C:\Users\htper\OneDrive\Folder\chats\chatapp\.venv\Lib\site-packages\flask\app.py", line 917, in full_dispatch_request
    rv = self.dispatch_request()
  File "C:\Users\htper\OneDrive\Folder\chats\chatapp\.venv\Lib\site-packages\flask\app.py", line 902, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "C:\Users\htper\OneDrive\Folder\chats\chatapp\.venv\Lib\site-packages\flask_login\utils.py", line 290, in decorated_view
    return current_app.ensure_sync(func)(*args, **kwargs)
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "C:\Users\htper\OneDrive\Folder\chats\chatapp\app.py", line 108, in list_models
    models = get_models()
  File "C:\Users\htper\OneDrive\Folder\chats\chatapp\database.py", line 41, in get_models
    return db.execute(
           ~~~~~~~~~~^
        "SELECT id, name, description, model_type, api_endpoint, temperature, max_tokens, is_default FROM models"
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ).fetchall()
    ^
sqlite3.OperationalError: no such column: model_type
2024-12-30 03:08:39,023 - INFO - 127.0.0.1 - - [30/Dec/2024 03:08:39] "[35m[1mGET /models HTTP/1.1[0m" 500 -
2024-12-30 03:08:39,175 - INFO - 127.0.0.1 - - [30/Dec/2024 03:08:39] "[33mGET /conversations HTTP/1.1[0m" 404 -
2024-12-30 03:11:45,832 - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-12-30 03:11:45,833 - INFO - [33mPress CTRL+C to quit[0m
